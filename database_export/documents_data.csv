id,title,content,text_content,quantum_score,created_at,updated_at,document_type,source,metadata,author_organization,publish_date,content_preview,ai_cybersecurity_score,quantum_cybersecurity_score,ai_ethics_score,quantum_ethics_score,detected_region,region_confidence,region_reasoning,topic,url_valid,url_status,url_checked,source_redirect
10,NIST Special Publication 800-63-3 Digital Identity Guidelines," 
NIST Special Publication 800-63-3 
 
Digital Identity Guidelines 
 
Paul A. Grassi 
Michael E. Garcia 
James L. Fenton 
 
 
 
 
 
 
 
This publication is available free of charge from: 
https://doi.org/10.6028/NIST.SP.800-63-3 
 

  
NIST Special Publication 800-63-3 
Digital Identity Guidelines 
 
 
Paul A. Grassi 
Michael E. Garcia 
Applied Cybersecurity Division 
Information Technology Laboratory 
James L. Fenton 
Altmode Networks 
Los Altos, Calif. 
 
 
 
 
 
 
 
This publication is availa..."," 
NIST Special Publication 800-63-3 
 
Digital Identity Guidelines 
 
Paul A. Grassi 
Michael E. Garcia 
James L. Fenton 
 
 
 
 
 
 
 
This publication is available free of charge from: 
https://doi.org/10.6028/NIST.SP.800-63-3 
 

  
NIST Special Publication 800-63-3 
Digital Identity Guidelines 
 
 
Paul A. Grassi 
Michael E. Garcia 
Applied Cybersecurity Division 
Information Technology Laboratory 
James L. Fenton 
Altmode Networks 
Los Altos, Calif. 
 
 
 
 
 
 
 
This publication is available free of charge from: 
https://doi.org/10.6028/NIST.SP.800-63-3 
 
 
 
 
June 2017 
INCLUDES UPDATES AS OF 03-02-2020; PAGE X 
 
 
 
 
 
 
 
U.S. Department of Commerce 
Wilbur L. Ross, Jr., Secretary 
 
National Institute of Standards and Technology  
Kent Rochford, Acting NIST Director and Under Secretary of Commerce for Standards and Technology 
 
  
Authority 
This publication has been developed by NIST in accordance with its statutory responsibilities under the 
Federal Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law 
(P.L.) 113-283. NIST is responsible for developing information security standards and guidelines, including 
minimum requirements for federal information systems, but such standards and guidelines shall not apply 
to national security systems without the express approval of appropriate federal officials exercising policy 
authority over such systems. This guideline is consistent with the requirements of the Office of Management 
and Budget (OMB) Circular A-130. 
Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and 
binding on f ederal agencies by the Secretary of Commerce under statutory authority. Nor should these 
guidelines be interpreted as altering or superseding the existing authorities of the Secretary of Commerce, 
Director of the OMB, or any other f ederal official. This publication may be used by nongovernmental 
organizations on a voluntary basis and is not subject to copyright in the United States. Attribution would, 
however, be appreciated by NIST.   
National Institute of Standards and Technology Special Publication 800-63-3 
Natl. Inst. Stand. Technol. Spec. Publ. 800-63-3, 75 pages (June 2017) 
CODEN: NSPUE2 
This publication is available free of charge from: 
https://doi.org/10.6028/NIST.SP.800-63-3 
Certain commercial entities, equipment, or materials may be identified in this document in order to describe an 
experimental procedure or concept adequately. Such identification is not intended to imply recommendation or 
endorsement by NIST, nor is it inte nded to imply that the entities, materials, or equipment are necessarily the best 
available for the purpose.  
There may be references in this publication to other publications currently under development by NIST in accordance 
with its assigned statutory responsibilities. The information in this publication, including concepts and methodologies, 
may be used by f ederal agencies even before the completion of such companion publications. Thus, until each 
publication is completed, current requirements, guideline s, and procedures, where they exist, remain operative. For 
planning and transition purposes, f ederal agencies may wish to closely follow the development of these new 
publications by NIST.   
Organizations are encouraged to review all draft publications during public comment periods and provide feedback to 
NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at 
http://csrc.nist.gov/publications. 
Comments on this publication may be submitted to: 
National Institute of Standards and Technology 
Attn: Applied Cybersecurity Division, Information Technology Laboratory 
100 Bureau Drive (Mail Stop 2000) Gaithersburg, MD 20899-2000 
Email: dig-comments@nist.gov 
All comments are subject to release under the Freedom of Information Act (FOIA).  
",0.00,2025-06-10 14:51:01.062945,2025-06-14 03:11:28.682084,Standard,https://doi.org/10.6028/NIST.SP.800-63-3,,NIST,2017-06-01,This standard defines standards for emerging technology governance governance and implementation. The document focuses on NIST implementations and requirements.,,,,,US,0.9,NIST is a US federal agency,AI,True,Valid (redirected to https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf),2025-06-14 03:27:53.429414,https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-63-3.pdf
12,DHS CISA and UK NCSC Release Joint Guidelines for Secure AI System Development,"DHS CISA and UK NCSC Release Joint Guidelines for Secure AI System Development
WASHINGTON – Taking a significant step forward in addressing the intersection of artificial intelligence (AI) and cybersecurity, the U.S. Department of Homeland Security’s (DHS) Cybersecurity and Infrastructure Security Agency (CISA) and the United Kingdom’s National Cyber Security Centre (NCSC) today jointly released Guidelines for Secure AI System Development to help developers of any systems that use AI make inform...","DHS CISA and UK NCSC Release Joint Guidelines for Secure AI System Development
WASHINGTON – Taking a significant step forward in addressing the intersection of artificial intelligence (AI) and cybersecurity, the U.S. Department of Homeland Security’s (DHS) Cybersecurity and Infrastructure Security Agency (CISA) and the United Kingdom’s National Cyber Security Centre (NCSC) today jointly released Guidelines for Secure AI System Development to help developers of any systems that use AI make informed cybersecurity decisions at every stage of the development process. The guidelines were formulated in cooperation with 21 other agencies and ministries from across the world – including all members of the Group of 7 major industrial economies -- and are the first of their kind to be agreed to globally.
“We are at an inflection point in the development of artificial intelligence, which may well be the most consequential technology of our time. Cybersecurity is key to building AI systems that are safe, secure, and trustworthy,” said Secretary of Homeland Security Alejandro N. Mayorkas. “The guidelines jointly issued today by CISA, NCSC, and our other international partners, provide a commonsense path to designing, developing, deploying, and operating AI with cybersecurity at its core. By integrating ‘secure by design’ principles, these guidelines represent an historic agreement that developers must invest in, protecting customers at each step of a system’s design and development. Through global action like these guidelines, we can lead the world in harnessing the benefits while addressing the potential harms of this pioneering technology.”
The guidelines provide essential recommendations for AI system development and emphasize the importance of adhering to Secure by Design principles that CISA has long championed.
“The release of the Guidelines for Secure AI System Development marks a key milestone in our collective commitment—by governments across the world—to ensure the development and deployment of artificial intelligence capabilities that are secure by design,” said CISA Director Jen Easterly. “As nations and organizations embrace the transformative power of AI, this international collaboration, led by CISA and NCSC, underscores the global dedication to fostering transparency, accountability, and secure practices. The domestic and international unity in advancing secure by design principles and cultivating a resilient foundation for the safe development of AI systems worldwide could not come at a more important time in our shared technology revolution. This joint effort reaffirms our mission to protect critical infrastructure and reinforces the importance of international partnership in securing our digital future.”
The guidelines are broken down into four key areas within the AI system development lifecycle: secure design, secure development, secure deployment, and secure operation and maintenance. Each section highlights considerations and mitigations that will help reduce the cybersecurity risk to an organizational AI system development process.
“We know that AI is developing at a phenomenal pace and there is a need for concerted international action, across governments and industry, to keep up,” said NCSC CEO Lindy Cameron. “These Guidelines mark a significant step in shaping a truly global, common understanding of the cyber risks and mitigation strategies around AI to ensure that security is not a postscript to development but a core requirement throughout. I’m proud that the NCSC is leading crucial efforts to raise the AI cyber security bar: a more secure global cyber space will help us all to safely and confidently realize this technology’s wonderful opportunities.”
“I believe the UK is an international standard bearer on the safe use of AI,” said UK Secretary of State for Science, Innovation and Technology Michelle Donelan. “The NCSC’s publication of these new guidelines will put cyber security at the heart of AI development at every stage so protecting against risk is considered throughout.”
These guidelines are the latest effort across the U.S.’s body of work supporting safe and secure AI technology development and deployment. In October, President Biden issued an Executive Order that directed DHS to promote the adoption of AI safety standards globally, protect U.S. networks and critical infrastructure, reduce the risks that AI can be used to create weapons of mass destruction, combat AI-related intellectual property theft, and help the United States attract and retain skilled talent, among other missions.
Earlier this month, CISA released its Roadmap for Artificial Intelligence, a whole-of-agency plan aligned with national strategy to address our efforts to promote the beneficial uses of AI to enhance cybersecurity capabilities, ensure AI systems are protected from cyber-based threats, and deter the malicious use of AI capabilities to threaten the critical infrastructure Americans rely on every day. Learn more about CISA’s AI work.
###
About CISA
As the nation’s cyber defense agency and national coordinator for critical infrastructure security, the Cybersecurity and Infrastructure Security Agency leads the national effort to understand, manage, and reduce risk to the digital and physical infrastructure Americans rely on every hour of every day.
Visit CISA.gov for more information and follow us on Twitter, Facebook, LinkedIn, Instagram.",0.00,2025-06-10 15:12:26.401959,2025-06-14 03:11:28.682084,Guideline,https://www.cisa.gov/sites/default/files/2025-01/JCDC%20AI%20Playbook_1.pdf,,CISA,2025-01-14,This guideline addresses emerging technology governance governance and implementation. It provides national guidance for CISA and related organizations.,19,,36,,US,0.85,DHS CISA is a US government agency,AI,True,valid,2025-06-14 03:56:01.906063,https://www.cisa.gov/sites/default/files/2025-01/JCDC%20AI%20Playbook_1.pdf
13,Joint Guidance on Deploying AI Systems Securely,"Joint Guidance on Deploying AI Systems Securely
Today, the National Security Agency’s Artificial Intelligence Security Center (NSA AISC) published the joint Cybersecurity Information Sheet Deploying AI Systems Securely in collaboration with CISA, the Federal Bureau of Investigation (FBI), the Australian Signals Directorate’s Australian Cyber Security Centre (ASD ACSC), the Canadian Centre for Cyber Security (CCCS), the New Zealand National Cyber Security Centre (NCSC-NZ), and the United Kingdom’...","Joint Guidance on Deploying AI Systems Securely
Today, the National Security Agency’s Artificial Intelligence Security Center (NSA AISC) published the joint Cybersecurity Information Sheet Deploying AI Systems Securely in collaboration with CISA, the Federal Bureau of Investigation (FBI), the Australian Signals Directorate’s Australian Cyber Security Centre (ASD ACSC), the Canadian Centre for Cyber Security (CCCS), the New Zealand National Cyber Security Centre (NCSC-NZ), and the United Kingdom’s National Cyber Security Centre (NCSC-UK).
The guidance provides best practices for deploying and operating externally developed artificial intelligence (AI) systems and aims to:
- Improve the confidentiality, integrity, and availability of AI systems.
- Ensure there are appropriate mitigations for known vulnerabilities in AI systems.
- Provide methodologies and controls to protect, detect, and respond to malicious activity against AI systems and related data and services.
CISA encourages organizations deploying and operating externally developed AI systems to review and apply this guidance as applicable. CISA also encourages organizations to review previously published joint guidance on securing AI systems: Guidelines for secure AI system development and Engaging with Artificial Intelligence. For more CISA information and guidance on securing AI systems, see cisa.gov/ai.
This product is provided subject to this Notification and this Privacy & Use policy.",0.00,2025-06-10 15:12:47.988802,2025-06-14 02:22:15.977106,Guideline,https://www.cisa.gov/news-events/alerts/2024/04/15/joint-guidance-deploying-ai-systems-securely,,NSA,,This guideline addresses ai governance and implementation. It provides national guidance for CISA and related organizations.,19,,42,,US,0.88,NSA is a US government agency,AI,True,Valid,2025-06-14 03:38:52.926567,
14,AI Red Teaming: Applying Software TEVV for AI Evaluations,"AI Red Teaming: Applying Software TEVV for AI Evaluations
As the National Coordinator for critical infrastructure security and resilience, CISA is responsible for facilitating a Secure by Design approach to AI-based software across the digital ecosystem and helping protect critical infrastructure from malicious uses of AI. To effectively mitigate against critical failures, physical attacks, and cyberattacks, AI software developers must prioritize conducting rigorous safety and security testing t...","AI Red Teaming: Applying Software TEVV for AI Evaluations
As the National Coordinator for critical infrastructure security and resilience, CISA is responsible for facilitating a Secure by Design approach to AI-based software across the digital ecosystem and helping protect critical infrastructure from malicious uses of AI. To effectively mitigate against critical failures, physical attacks, and cyberattacks, AI software developers must prioritize conducting rigorous safety and security testing to understand how an AI system can fail or be exploited.
AI red teaming is a foundational component of the safety and security evaluations process.
This blogpost demonstrates that AI red teaming must fit into the existing framework for AI Testing, Evaluation, Validation and Verification (TEVV). Additionally, the post explains how and why AI TEVV must fit into software TEVV, ensuring AI systems are fit for purpose. While there are differences in the specific software tools used, AI TEVV—despite common misconceptions—must be treated under software TEVV from a strategic and operational perspective.
This assertion is grounded in the fact that TEVV has been used for more than four decades to improve the safety and security of software.1 Experts working on AI evaluations should avoid reinventing the wheel and build upon lessons the software security community has learned through developing and improving guidance and requirements.
Framing AI Red Teaming in the Context of TEVV
AI red teaming is the third-party safety and security evaluation of AI systems; AI red teaming is a subset of AI Testing, Evaluation, Verification and Validation (TEVV).
AI TEVV, a broader risk-based approach for the external testing of AI systems, has been developed and operationalized by our interagency partners at the National Institute of Standards (NIST) through programs like Assessing Risks and Impacts of AI (ARIA) and the NIST GenAI Challenge.
Because AI systems are a type of software system, approaches for AI TEVV must fundamentally be a sub-component of the more established software TEVV.2 The TEVV framework is commonly used to test software reliability and help ensure that software is fit for purpose. TEVV can broadly be deconstructed into three components: software system test and evaluation process, software verification, and software validation.
Software TEVV Can be Used for AI Evaluations
A common misconception surrounding AI evaluation methods is that the established software TEVV framework is not, or cannot be, adapted to account for the evaluation of AI systems.
However, while there are tactical implementation and technical details that differ between AI TEVV and software TEVV, the two processes—from a strategic and operational vantage point—are quite similar. There are three truths about all software systems that illuminate this assertion:
1. Software systems have always had safety risks
One pervasive example that fuels the misconception that AI and software TEVV are dissimilar is the narrative that AI evaluations are unique because of the need to mitigate risk posed by potential security vulnerabilities and safety violations.3 While true, many software developers have long had to consider both the security and safety dimensions within traditional software systems.
For example, the Food and Drug Administration (FDA) approves medical devices for use within the United States. Since the 1980s, some medical devices have had a software component, a trend that is increasingly common today. In 1986, software flaws in a cancer radiotherapy device, the Therac-25, led to several deaths.4 The software flaw was a race condition, a type of error where operations are not executed in the correct order, resulting in unexpected outcomes. They occur often unpredictably and arise due to complex interactions between components and data. Race conditions are often hard to reproduce and identifying which single lines of code require modification to fix the flaw can be challenging.
While the FDA’s device approval process has since been updated, the Therac-25 incident demonstrates how traditional software can have fatal consequences for human safety. Medical devices are not uniquely susceptible to software safety risks; many other critical infrastructure sectors also employ safety critical software. Examples include aerospace, water/wastewater, and transportation, among many others. AI systems, as one type of software system, should similarly be evaluated for safety concerns, cybersecurity vulnerabilities, and in particular cybersecurity issues that could be exploited to cause safety issues.
2. Software systems require validity and reliability testing
Another common misconception is that AI systems must distinctively be tested for validity and reliability, preventing the deployment of AI systems which are inaccurate, unreliable, or poorly generalized to data.
However, mitigating against validity and reliability concerns while also ensuring the robustness of software against novel situations and inputs is common to both software and AI. For example, modern road vehicle braking systems often heavily rely on software to work effectively. Automated braking software interprets data from sensors and assists when a driver may not react a hazard in time.
This safety-critical software must demonstrate robustness to a variety of events and conditions, like unexpected pedestrians, slick roads, or the driver following too closely behind another car. Designers of all safety-critical software systems, whether they include an AI-element or not, must consider a range of factors including the dynamics of the system, the probability of certain external events, and the desired degree of “safety margin” given the impact of the system losing control. Additionally, rigorous testing is applied to ensure that the real system reflects the intended design assumptions.
While AI systems can be more complex than this simple example, many of the concepts and techniques from evaluating and modeling software robustness against unexpected inputs are akin to those used in traditional software evaluations.
3. Software systems are fundamentally probabilistic
Finally, many point to how AI systems, constructed with probabilities and commonly created with intentional variance to avoid producing repetitive results, often need multiple trials to discover improper behavior. This concern surrounding variability also extends to how outputs from AI systems may differ entirely even with small changes to configuration details or training data.
However, traditional software systems are also inherently unpredictable and can exhibit wildly different behavior based on small changes in input if appropriate safeguards are not implemented. Broadly, one cannot prove any non-trivial properties of any computer program in general ahead of time (i.e., Rice’s Theorem).
However, more operationally relevant are security vulnerabilities where a change to one or a few bytes in the input from the network can lead to total control of the machine by a threat actor; this happened with the popular web server NGINX in 2021 (CVE-2021-23017). Some classes of vulnerabilities, like race conditions, are not deterministic in any software system. Software engineers may also intentionally create controlled randomness in computer processes; this is a core function of cryptography.
Characteristics that may seem more prominent or concerning in AI systems (safety concerns, testing for validity and reliability, their probabilistic nature) have always been present in traditional software systems. As such, the well-established software TEVV methodology is a perfectly valid approach from which to conduct AI evaluations. Yes, there are differences with AI that require some adaptation, but none so large as to warrant a drastically different approach.
CISA's Role in AI TEVV
As the AI evaluations field continues to mature, there is an array of diverse stakeholders working to advance the science and practice of AI red teaming. This includes developing novel methodologies, creating tools that are interoperable across models or platforms, and improving capabilities to conduct AI red teaming at scale.
Serving both as National Coordinator and an operational lead for federal cybersecurity, CISA focuses on contributing to AI red teaming efforts that primarily support security evaluations for Federal and non-Federal entities; this work is organized into three broad workstreams.
First, CISA remains steadfast in ensuring that our work on AI pre-deployment testing supplements efforts in industry, academia, and government. CISA is a founding member of the recently announced Testing Risks of AI for National Security (TRAINS) Taskforce, which will include testing of advanced AI models across national security and public safety domains. Led by the NIST AI Safety Institute, CISA will contribute expertise both by helping build new AI evaluation methods and benchmarks that integrate with security testing processes, as well as providing subject matter expertise on cybersecurity testing. For much of this work, CISA will rely upon Vulnerability Management within CISA’s Cybersecurity Division, which offers CISA security evaluation services such as Cyber Hygiene and Risk and Vulnerability Assessments.
Second, CISA continues to provide technical assistance and risk management support to Federal and non-Federal partners, specifically supporting AI security technical post-deployment testing. This includes varied forms of testing, such as penetration testing, vulnerability scanning, and configuration testing. CISA also often works independently to detect and identify security vulnerabilities impacting critical infrastructure systems and devices. CISA has already begun to receive requests from partners to conduct penetration and technical security testing on Large Language Models (LLMs) and expects demand for these services to grow as partners increasingly adopt AI tools.
Third, CISA collaborates with NIST on the development of standards for AI security testing. CISA provides operational cybersecurity expertise to help make standards practicable. Additionally, CISA builds on NIST standards to provide high-quality services and advice to our partners. CISA security evaluation services, such as red teaming, include AI systems in the scope of those security assessment services. CISA also provides operational guidance for securing software systems, such as the cross-sector cybersecurity performance goals, and priority security practices for AI systems, such as the Secure by Design pledge.
New, but the Same
By treating AI TEVV as a subset of traditional software TEVV, the AI evaluations community benefits from using and building upon decades of proven and tested approaches towards assuring software is fit for purpose. Additionally, by streamlining processes, enterprises can avoid standing up parallel testing processes to accomplish similar ends, saving time and resources.
Most notably, with the knowledge that software and AI TEVV must be treated similarly to software TEVV from a strategic and operational perspective, the digital ecosystem can instead channel effort at the tactical level, developing novel tools, applications, and benchmarks to robustly execute AI TEVV.
1 The US Department of Defense publications in the Rainbow Series in the 1980s represent major development in the history of cybersecurity guidance and requirements. See NSA/NCSC Rainbow Series.
2 Some organizations approach software TEVV as a part of a broader product quality management regime, as defined by the processes and practices necessary for conformance with the ISO 9000 series of publications. For example, NIST SP 800-160r1 applies ISO 9000 definitions for validation and verification directly to the topic of engineering trustworthy, secure information systems. However, NIST SP 800-160r1 refers to a different ISO standard (29119-2:2021 – Software and systems engineering – Software testing) to define testing. There are several other documents referenced by SP 800-160r1 for evaluation criteria and processes. While SP 800-160r1 does not title itself a TEVV process manual, it does in fact define and inter-relate the processes for software testing, evaluation, validation, and verification.
3 NIST defines “security” as resistance to intentional, unauthorized act(s) designed to cause harm or damage to a system; “safety” is a property of a system such that it does not, under defined conditions, lead to a state in which human life, health, property, or the environment is endangered. See: The Language of Trustworthy AI: An In-Depth Glossary of Terms.
4 Leveson, Nancy G., and Clark S. Turner. ""An investigation of the Therac-25 accidents."" Computer 26.7 (1993): 18-41.c",0.00,2025-06-10 15:13:09.751663,2025-06-14 02:23:18.757749,Policy,https://www.cisa.gov/news-events/news/ai-red-teaming-applying-software-tevv-ai-evaluations,,Unknown,,This policy addresses emerging technology governance by prioritize conducting rigorous safety and security testing t. It provides national guidance for CISA and related organizations.,23,,38,,US,0.82,US government publication,AI,True,Valid,2025-06-14 03:38:51.259066,
15,AI Cybersecurity Collaboration Playbook,"AI Cybersecurity Collaboration Playbook
The AI Cybersecurity Collaboration Playbook provides guidance to organizations across the AI community –including AI providers, developers, and adopters – for sharing AI-related cybersecurity information voluntarily with the Cybersecurity and Infrastructure Security Agency (CISA) and other partners through the Joint Cyber Defense Collaborative (JCDC). While focused on strengthening collaboration within JCDC, the playbook also identifies actionable informat...","AI Cybersecurity Collaboration Playbook
The AI Cybersecurity Collaboration Playbook provides guidance to organizations across the AI community –including AI providers, developers, and adopters – for sharing AI-related cybersecurity information voluntarily with the Cybersecurity and Infrastructure Security Agency (CISA) and other partners through the Joint Cyber Defense Collaborative (JCDC). While focused on strengthening collaboration within JCDC, the playbook also identifies actionable information sharing categories applicable to broader critical infrastructure stakeholders and other sharing mechanisms. CISA encourages organizations to adopt the playbook’s guidance to enhance their own information-sharing practices, contributing to a unified approach to AI-related cybersecurity threats across critical infrastructure.
This playbook aims to:
- Facilitate collaboration between federal agencies, private industry, international partners, and other stakeholders to raise awareness of AI cybersecurity risks and improve the resilience of AI systems.
- Guide JCDC partners on how to voluntarily share information related to cybersecurity incidents and vulnerabilities associated with AI systems.
- Delineate information sharing protections and mechanisms.
- Outline CISA’s actions upon receiving shared information to strengthen collective defense.
AI safety topics, such as risks to human life, health, property, or the environment, are outside the intended scope of the JCDC AI Cybersecurity Collaboration Playbook. Stakeholders should address any risks or threats involving human life, health, property, or the environment in a timely and appropriate manner, in accordance with their own applicable process or procedures for such events. Similarly, issues related to AI fairness and ethics are also outside the scope of this playbook. This playbook does not create policies, impose requirements, mandate actions, or override existing legal or regulatory obligations. All actions taken under this playbook are voluntary. This playbook will undergo periodic updates, evolving to address these challenges through active collaboration among government, industry, and international partners.
Please share your thoughts with us via our anonymous product survey; we welcome your feedback.",0.00,2025-06-10 15:13:25.437859,2025-06-14 03:11:28.682084,Guideline,https://www.cisa.gov/sites/default/files/2025-01/JCDC%20AI%20Playbook_1.pdf,,CISA,,This guideline addresses emerging technology governance by guidance to organizations across the ai community –including. The document focuses on CISA implementations and requirements.,19,,34,,US,0.85,DHS CISA is a US government agency,AI,True,valid,2025-06-14 03:56:01.906063,https://www.cisa.gov/sites/default/files/2025-01/JCDC%20AI%20Playbook_1.pdf
16,ITI's AI Security Policy Principles," www.itic.org
1
ITI's AI Security 
Policy Principles
October 2024
 www.itic.orgPromoting Innovation Worldwide
2
As with many technologies that came before 
it, Artificial Intelligence (AI) intersects with 
cybersecurity in two ways: 
Introduction
I. AI is increasingly a target of cyber-attacks – both  
traditional attacks and attacks unique to AI and 
II. AI is being used to improve cybersecurity  
and will continue to have significant impact  
in the field. 
As AI becomes more integrated into o..."," www.itic.org
1
ITI's AI Security 
Policy Principles
October 2024
 www.itic.orgPromoting Innovation Worldwide
2
As with many technologies that came before 
it, Artificial Intelligence (AI) intersects with 
cybersecurity in two ways: 
Introduction
I. AI is increasingly a target of cyber-attacks – both  
traditional attacks and attacks unique to AI and 
II. AI is being used to improve cybersecurity  
and will continue to have significant impact  
in the field. 
As AI becomes more integrated into our daily lives, governments worldwide 
are increasingly focused on its cybersecurity implications. Their interest is 
twofold: using policy tools to enhance AI’s resilience against cyber-attacks, 
and leveraging AI to bolster overall cybersecurity for citizens, businesses,  
and economies.
The purpose of this paper is to build upon our 2021 Global AI Policy 
Recommendations, providing more in-depth suggestions to policymakers 
that specifically address how to improve the cybersecurity of AI models and 
systems. 1  Our recommendations are drawn from the technical expertise of 
our members and are based upon sound best practices that have long driven 
advances in cybersecurity globally.
Leveraging AI to Improve Cybersecurity
 www.itic.orgPromoting Innovation Worldwide 3
While malicious actors can leverage AI to launch more efficient cyber attacks, AI can also significantly 
bolster pr
oactive cybersecurity measures and is already transforming the fields of cybersecurity, data 
security
, and threat intelligence. As cyber threats are continually evolving, and there is a growing 
demand for cyber professionals, AI can address some limitations of traditional cyber defense methods. 
The ability to analyze vast amounts of data, identify patterns, and predict potential security breaches 
makes AI particularly useful in the detection, prevention, and response to cyber attacks. Leveraging AI 
does not render traditional cyber methods obsolete but rather can enable more proactive defenses 
against evolving cyber threats in real-time. AI can work as a force multiplier, particularly for Small and 
Medium Size Enterprises (SMEs) and security operations center (SOC) analysts, to improve their 
cybersecurity posture. AI can contribute to:
Advanced threat detection. AI algorithms can identify anomalies and behavior that may 
indicate a cyber-attack at much earlier phases of the cybersecurity kill chain. Many 
traditional cyber threat detection methods recognize ""signatures,"" which is not effective 
against never-before-seen threats. AI tools rely on patterns as opposed to defined 
signatures which can make them more effective against novel or zero-day cyber threats.
Automated incident response. AI systems can continuously monitor user activities to 
establish a baseline of normal behavior and more easily identify anomalous behavior.  
They can also enable organizations to understand what data, business secrets, or IP is  being 
shared in AI applications. This is particularly impactful for analyzing supply chain data to 
identify potential security risks, such as vulnerabilities in third party software and hardware 
components.
Enhancing exposure management. AI can help organizations map their internet-exposed 
attack surface and analyze the breadth of vulnerability, misconfigurations, and identity data 
against current exploit activity and attack paths to better prioritize proactive defenses and 
remediation activities.
Threat intelligence. AI can more efficiently process and analyze threat intelligence 
data from diverse sources, such as threat databases, network logs, security feeds, threat 
intelligence platforms and cybersecurity reports in real-time. AI can also help to discover 
specific patterns in datasets that may otherwise be overlooked by a human analyst, which 
can in turn reveal potential correlations between seemingly unrelated threat indicators. AI 
can also perform cross-language analysis and temporal analysis, which can help to broaden 
the scope of information gathering and analysis and take historical context into account.
Policymakers should ensure that policy approaches support the use of AI for cybersecurity purposes. We 
encourage policymakers to review our Global AI Policy Recommendations (linked in footnote 1) for more 
information on how to ensure this happens.
101011
101110
101000
101111
 www.itic.org
4
Improving the Cybersecurity 
of AI Models and AI Systems2
AI models and systems are subject to traditional cybersecurity 
threats, such as data breaches, denial of service attacks, 
man-in-the-middle attacks, malware infections, supply chain 
attacks, and others.
3 In such cases, existing cybersecurity 
approaches (including, for example, existing controls, 
principles, frameworks, and laws) usually can be applied. 
However, threats unique to AI are emerging with the advent 
of these systems, like attacks leveraging adversarial AI, 
the malicious introduction of data impacting a model's 
performance, prompt injection attacks, and the extraction of 
sensitive model data. While these new threats are occurring 
less often than traditional cybersecurity threats, they may 
require novel or different cybersecurity approaches.
Policymaker interest in the security of AI systems has been steadily increasing. 
Around the globe, governments are issuing guidelines on AI security 
and seeking feedback on different proposals, intending to improve the 
cybersecurity of AI systems and models.
456  In order to enable interoperability, 
prioritize effectiveness in improving cybersecurity, and align across 
jurisdictions, ITI suggests that policymakers follow these principles when 
crafting AI security policy:
1
Leverage existing cybersecurity practices, standards, 
and controls where they are already sufficient
3
Coordinate internationally with likeminded allies and  
partners to ensure that policy approaches to AI security 
are global and interoperable
2
Ensure that any AI security policy reflects a comprehensive 
approach across the AI life cycle and value chain
4
Utilize public-private partnerships to achieve 
AI security outcomes
5
Ensure adequate government support for AI research & 
development and for training and growing the existing 
cybersecurity workforce
 www.itic.org
5
Leverage existing cybersecurity practices, standards, and controls where 
they are already sufficient
The same cybersecurity practices, standards, and controls that apply to non-AI systems should also 
apply to AI systems, which share many of the attributes as other systems comprised of hardware, 
software, and data. While there are some unique security risks to AI systems – including as related to 
proactive security monitoring and response, which can create unique privacy challenges – the vast 
majority of the risk is the same as with other non-AI systems. Creating or diverging from existing 
practices, standards, and controls has the potential to create conflicting or duplicative requirements, 
increasing the compliance burden for companies and diverting resources from critical cybersecurity 
activities, potentially giving attackers the upper hand. Therefore, requiring AI models and systems to 
adhere to existing standards and controls should be a priority.  For example, ISO/IEC 270017, SOC28, 
and the U.S. NIST Cybersecurity Framework9 are all applicable to AI systems. Good AI cybersecurity 
requires robust baseline cybersecurity practices, so in considering how to bolster AI cybersecurity, 
policymakers should continue to encourage strong cybersecurity hygiene. 
Align new AI-specific security practices, 
standards and controls with existing 
frameworks and guidelines. AI-specific risks, 
standards, and controls can often be mapped 
to established cybersecurity frameworks 
and guidelines.
10  Mapping to established 
frameworks and guidelines can help to  
create a unified and integrated approach  
to cybersecurity, meeting the needs of  
security teams, practitioners, and security 
programs overall. 
For example, an AI-specific security control 
might be focused on encouraging the 
protection of training data, model weights, 
and input/output data flows, mapping to 
relevant controls in existing frameworks or 
standards such as the U.S. National Institute of 
Standards and Technology (NIST) Cybersecurity 
Framework (CSF), COBIT, or ISO 27001.
Collaborate with private sector stakeholders 
to develop new standards, controls, or 
practices only where there is evidence that AI-
specific risks are not addressed by an existing 
cybersecurity standard or practice. In doing so, 
governments shou ld work with existing private 
s ector groups, su ch as those conv ened by the 
NIST, including the AI Safety Institute 
Consortium 1 1  in the United Sta tes or the AI 
Verify Foundation in Singapore 1 2  to propose 
new work items in international st andards 
bodies, or other wis e work with st ak eholders to 
propose new work items in inte rnational 
s tandards bodie s  directly.
1
 www.itic.org
6
Ensure that any AI security policy reflects a comprehensive approach 
across the AI life cycle and value chain
From data collection and model development to deployment and ongoing monitoring of the  
system post-deployment, a holistic approach is critical to managing and securing complex and  
evolving AI systems.
Adopt evidence-based policies and/or 
encourage the adoption of frameworks 
that emphasize the importance of utilizing 
st
ate-of-the-art product, software, and 
network security approaches, such as secure 
development lifecycles and a secure-by-
design approach, and implementing zero trust 
architecture principles, includin
g hardware-
based security measures. Policies or 
frameworks that provide incentives for the 
private and public sectors would demonstrate a 
commitment to secure-by-design and 
encourage widespread adoption across 
industries. Such policies or frameworks should 
also ensure a balance between innovation and 
security, avoid imposing overly 
rigid and/or 
prescriptive security requiremen ts, and instead 
focus on outcomes .
Adopt policies and/or encourage the adoption 
of frameworks that incorporate robust data 
governance practices that ensure data integrity 
and provenance throughout the data lifecycle. 
This may include using curated data sets – data 
sets that have been cleaned, organized, and 
transformed – to enhance secure data 
collection, storage, and processing. Data 
poisoning 
– where malicious actor s manipulate 
training data to corrupt AI models – constitutes 
a potential risk to public and pr ivate sector 
organizations ali ke. As such, encouraging the 
adoption of robust data governanc e practices 
can help to mitigate this risk. 
Pr
omoting Innovation Worldwide
Encourage organizations to define  
AI-specifi
c security requirements early in 
the software and hardware development 
lifecycle. Such requirements might include 
considerations around data integrity and 
model transparency. Defi
ning requirements 
early can help organizations identify where 
existing security frameworks are not fit-for-
purpose, or where organizations may need to 
identify and reference additional AI-specifi
c 
security resources. 
Consider the benefits of openness in 
improving the security of AI systems. 
Policymakers sho uld recognize that for decades, 
the open-source ecosystem has ena bled a 
broad and divers e stakeholder co mmunity 
to identify and fix vulnerabilities, increase 
p erformance and resilience, and  raise the bar 
for security overall. An open AI ecosystem can 
allow for independent evaluations  of software 
by a wider commu nity of develo pers, enabling 
them to identify  vulnerabilities and test for 
s afety issues. This openness pro motes the 
adoption of robust  s ecurity practices  s ince a 
diverse pool of experts are able to effectively 
evaluate model co mponents (train ing data, 
model weights, s ource code) to mitigate risks 
that may other wis e go unnoticed, which in 
turn can lead t o improved and safer foundation 
models. These pra ctices can also  support the 
establishment of verifiable benchmarks for 
model performance  in safety and  compliance.
2
 www.itic.org
7
Coordinate internationally with likeminded allies and partners to ensure that policy 
approaches to AI security are global and interoperable
In the same way that cybersecurity is borderless, so too is AI security. Prioritizing consistent 
international cooperation will help to promote technological innovation and align strategies for 
addressing threats stemming from and related to emerging technologies, avoiding a conflicting or 
misaligned global AI security landscape and the creation of gaps that adversaries can leverage. 
Leverage and promote existing, voluntary, 
industry-led international standards, 
guidelines, and best practices for AI 
security. Technica l standards ca n h elp to 
facilitate align ment and interoperability 
across borders.  There is ongoi ng work in 
global standards  development o rganizations 
( S DOs), such as ISO/IEC JTC 1 SC  4 2 1 3  and 
SC 27 addressin g AI risk management, 
governance, and cybersecurity of AI systems. 
For example, ISO /IEC 27090 – Guidance for 
addressing security threats and failures in AI 
systems is curren tly under devel opment and 
ISO/IEC 42001 – Artificial Intelligence 
Management System includes a requirement 
to implement sec urity controls as  a part of 
an organization’s  overall AI risk  management 
activities. 1 4  To the extent t h at governments  
are undertaking efforts to develop guidelines 
or identify best practices, such a s with the 
N IST AI Risk Man agement Framework 1 5 , we 
encourage govern ments to contri but e them 
to international standards bod ies for further 
standards devel opm ent. 
Engage in multilateral fora to share best 
practices, share information about AI incidents, 
and advance a common understanding of 
threats to AI security. Engagement will help 
to foster partnerships with likeminded allies 
and partners, allowing cybersecurity experts, 
companies, and governments to more quickly 
respond to and mitigate incidents. As a part of 
this, we encourage governments to share best 
practices about AI security, leverage existing 
mechanisms to share information about AI 
security related threats across borders, and 
utilize existing efforts to track and monitor  
AI incidents.
•
F
or example, on AI security incident
reporting, the OECD has been developing
an AI incidents and hazards monitor to
collect publicly reported information about
AI security incidents, among other, broader
incidents, ideally helping to identify patterns
and trends over time.
16  
• T o the extent that other multilateral
conversations, such as those taking place in
the G7 or as a part of the work of the Network
of AI Safety Institutes
17  implicate security,
policymakers should also seek to leverage
those venues.
3
 www.itic.org
8
Utilize public-private partnerships to achieve AI security outcomes 
Public-private partnerships are critical in addressing AI security challenges. Government and industry 
often possess distinct sets of information that, when combined, provide a comprehensive picture of 
AI-related security threats. Collaboration is key to identifying potential risks, developing effective risk 
manag
ement strategies, and implementing balanced security measures. The tech industry, which has 
led in the development of AI systems and in the infrastructure of cyberspace, contributes vital expertise 
to these partnerships. By sharing information and participating in joint initiatives, the public and private 
sectors can create more a robust AI security ecosystem while avoiding unintended consequences.
F
or example, in t he United States, there are existing public private partnership ac tivities that  
government should continue to harness, such as the U.S. ICT Suppl y Chain Risk Management Task Force’s 
AI Working Group, as well as the  collaborative work that the Cybersecurity and Infrastructure Sec urity 
Agency (CISA) has done with the IT Sector Coordinating Council to develop the AI Safety and Security 
Guidelines
 for Critical Infrastructure. CISA’s Joint Cybersecurity Defense Collaborative has also begun 
specific efforts on AI security with robust industry engagements. In Europe, legislation like the NIS 2 
Directive
, as well as the Europea n Union Agency for Cybersecurity (ENISA) encourage private-public 
cooperation in the field of cybersecurity as well as the importance of building trust through public-
private partners hips.
Work with stakeholders to determine appropriate roles and responsibilities in the AI value chain for 
developers, integrators, deployers, and others in considering AI security. This will help to achieve 
a holistic approach to cybersecurity and distribute responsibility based on a clear understanding of 
which stakeholders in the value chain can most appropriately perform a specific activity. 
Ensure adequate government support for AI research & development and for training  
and growing the existing cybersecurity workforce
Prioritize government funding for R&D 
focused on understanding and mitigating 
adversarial attacks against AI models. 
Anticipating future threats both against 
AI models, and stemming from malicious 
use of AI, is crucial as the threat landscape 
is constantly evolving.  Staying ahead of 
attackers requires continually collecting  
and evaluating evidence. While the private 
sector is undertaking such R&D, government 
funding can complement industry resources 
when needed.
Consider how to incorporate AI-specific 
training, including related to coding with 
certain types of models, into existing national 
and local cyber skilling programs. This includes 
supporting the creation of new workforce 
programs as appropriate to help upskill the 
existing cyber workforce and create pathways 
for new talent. As a part of this, policymakers 
should explore how to deepen and support 
partnerships with educational institutions 
to create AI-specific training programs and 
certification incentives.
4
5
 www.itic.orgPromoting Innovation Worldwide
9
1  See our Global AI Policy Recommendations here: https://www.itic.org/documents/artificial-intelligence/ITI_GlobalAIPrinci-
ples_032321_v3.pdf .
2 An AI model is an algorithm that is trained to recognize patterns and make predictions or decisions based on input data, while an 
AI system is one or more AI models and other components that are used to solve a specific problem in real-world scenarios.  
It can be fine-tuned or undergo additional training and tooling for dedicated use cases. See our Foundation Models Guide for 
more information. 
3  We note that the ICT Supply Chain Risk Management Task Force’s AI Working Group is updating their Threat Scenarios Report 
to account for AI. The most up to date resources can be found here: https://www.cisa.gov/resources-tools/groups/ict-sup-
ply-chain-risk-management-task-force . We also highlight the threat assessment work that CISA did in producing its AI Safe-
ty and Security Guidelines for Critical Infrastructure, which is referenced in its paper: https://www.dhs.gov/sites/default/
files/2024-04/24_0426_dhs_ai-ci-safety-security-guidelines-508c.pdf . 
4  See United Kingdom’s Call for Views on the Cyber Security of AI here: https://www.gov.uk/government/calls-for-evidence/call-
for-views-on-the-cyber-security-of-ai/call-for-views-on-the-cyber-security-of-ai.
5  See the United States’ Department of Homeland Security Guidelines for Critical Infrastructure Owners and Operators here: 
https://www.dhs.gov/publication/safety-and-security-guidelines-critical-infrastructure-owners-and-operators.
6  See Singapore’s Draft AI Security Guidelines and Companion Document here: https://www.csa.gov.sg/News-Events/public-con-
sultations/public-consultation-on-securing-ai-systems.
7  See ISO 27001 here: https://www.iso.org/standard/27001.
8 More about SOC2 here: https://www.aicpa-cima.com/topic/audit-assurance/audit-and-assurance-greater-than-soc-2.
9 See U.S. NIST Cyber Framework here: https://www.nist.gov/cyberframework.
10  There are many existing frameworks that policymakers can reference, including the NIST Cybersecurity Framework (CSF) 2.0 
and the NIST Secure Software Development Framework (SSDF), the NIST SSDF Generative AI Profile, the United Kingdom’s 
Guidelines for Secure Software Development, CISA’s Secure-By-Design Principles, and Singapore’s Draft AI Security Guidelines 
(once finalized).
11  More information about the U.S. AI Safety Institute Consortium can be found here: https://www.nist.gov/aisi/artificial-intelli-
gence-safety-institute-consortium-aisic.
12  More information about the AI Verify Foundation can be found here: https://aiverifyfoundation.sg/ai-verify-foundation/.
13  More information about SC 42 can be found here: https://www.iso.org/committee/6794475.html; more information about SC 
47 can be found here: https://www.iso.org/committee/45306.html.
14  The Committee is currently reviewing the draft standard. More information is available here: https://www.iso.org/stan-
dard/56581.html#:~:text=ISO%2FIEC%20CD%2027090%20%2D%20Cybersecurity,failures%20in%20artificial%20intelli-
gence%20systems; ISO/IEC 42001 can be found here: https://www.iso.org/standard/81230.html.
15 U.S. NIST AI Risk Management Framework available here: https://www.nist.gov/itl/ai-risk-management-framework.
16  More information on the OECD’s work can be found here: https://oecd.ai/en/incidents.
17  More information about the Network of AI Safety Institutes is available here: https://www.gov.uk/government/news/global-
leaders-agree-to-launch-first-international-network-of-ai-safety-institutes-to-boost-understanding-of-ai; the entire Statement 
of Intent on AI Safety can be found here: https://www.gov.uk/government/publications/seoul-declaration-for-safe-innovative-
and-inclusive-ai-ai-seoul-summit-2024.
References
The Information Technology Industry Council (ITI) 
is the premier global advocate for technology, 
representing the world’s most innovative 
companies. We promote public policies and 
industry standards that advance competition and 
innovation worldwide.
",0.00,2025-06-10 17:58:01.189866,2025-06-13 05:57:52.921591,Policy,https://www.itic.org/documents/artificial-intelligence/ITI_AI-Security-Principles_102124_FINAL.pdf,,ITI,2024-10-01,This policy addresses emerging technology governance by continue to have significant impact in the field. It establishes international standards and requirements for implementation.,40,,59,,Unknown,0.0,,AI,True,Valid,2025-06-14 03:38:52.502318,
26,Artificial Intelligence Risk Management Framework (AI RMF 1.0),"NIST AI 100-1 Artificial Intelligence Risk Management Framework (AI RMF 1.0) NIST AI 100-1 Artificial Intelligence Risk Management Framework (AI RMF 1.0) This publication is available free of charge from: January 2023 U.S. Raimondo, Secretary National Institute of Standards and Technology Laurie E.","NIST AI 100-1
Artificial Intelligence Risk Management
Framework (AI RMF 1.0)

NIST AI 100-1
Artificial Intelligence Risk Management
Framework (AI RMF 1.0)
This publication is available free of charge from:
https://doi.org/10.6028/NIST.AI.100-1
January 2023
U.S. Department of Commerce
Gina M. Raimondo, Secretary
National Institute of Standards and Technology
Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology
Certain commercial entities, equipment, or materials may be identified in this document in order to describe 
an experimental procedure or concept adequately. Such identification is not intended to imply recommenda-
tion or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that 
the entities, materials, or equipment are necessarily the best available for the purpose. 
This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1
Update Schedule and Versions
The Artificial Intelligence Risk Management Framework (AI RMF) is intended to be a living document.
NIST will review the content and usefulness of the Framework regularly to determine if an update is appro-
priate; a review with formal input from the AI community is expected to take place no later than 2028. The
Framework will employ a two-number versioning system to track and identify major and minor changes. The
first number will represent the generation of the AI RMF and its companion documents (e.g., 1.0) and will
change only with major revisions. Minor revisions will be tracked using “.n” after the generation number
(e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including
version number, date of change, and description of change. NIST plans to update the AI RMF Playbook
frequently. Comments on the AI RMF Playbook may be sent via email to AIframework@nist.gov at any time
and will be reviewed and integrated on a semi-annual basis.
Table of Contents
Executive Summary 1
Part 1: Foundational Information 4
1 Framing Risk 4
1.1 Understanding and Addressing Risks, Impacts, and Harms 4
1.2 Challenges for AI Risk Management 5
1.2.1 Risk Measurement 5
1.2.2 Risk Tolerance 7
1.2.3 Risk Prioritization 7
1.2.4 Organizational Integration and Management of Risk 8
2 Audience 9
3 AI Risks and Trustworthiness 12
3.1 Valid and Reliable 13
3.2 Safe 14
3.3 Secure and Resilient 15
3.4 Accountable and Transparent 15
3.5 Explainable and Interpretable 16
3.6 Privacy-Enhanced 17
3.7 Fair – with Harmful Bias Managed 17
4 Effectiveness of the AI RMF 19
Part 2: Core and Profiles 20
5 AI RMF Core 20
5.1 Govern 21
5.2 Map 24
5.3 Measure 28
5.4 Manage 31
6 AI RMF Profiles 33
Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3 35
Appendix B: How AI Risks Differ from Traditional Software Risks 38
Appendix C: AI Risk Management and Human-AI Interaction 40
Appendix D: Attributes of the AI RMF 42
List of Tables
Table 1 Categories and subcategories for the GOVERN function. 22
Table 2 Categories and subcategories for the MAP function. 26
Table 3 Categories and subcategories for the MEASURE function. 29
Table 4 Categories and subcategories for the MANAGE function. 32
i
NIST AI 100-1 AI RMF 1.0
List of Figures
Fig. 1 Examples of potential harms related to AI systems. Trustworthy AI systems
and their responsible use can mitigate negative risks and contribute to bene-
fits for people, organizations, and ecosystems. 5
Fig. 2 Lifecycle and Key Dimensions of an AI System. Modified from OECD
(2022) OECD Framework for the Classification of AI systems — OECD
Digital Economy Papers. The two inner circles show AI systems’ key di-
mensions and the outer circle shows AI lifecycle stages. Ideally, risk man-
agement efforts start with the Plan and Design function in the application
context and are performed throughout the AI system lifecycle. See Figure 3
for representative AI actors. 10
Fig. 3 AI actors across AI lifecycle stages. See Appendix A for detailed descrip-
tions of AI actor tasks, including details about testing, evaluation, verifica-
tion, and validation tasks. Note that AI actors in the AI Model dimension
(Figure 2) are separated as a best practice, with those building and using the
models separated from those verifying and validating the models. 11
Fig. 4 Characteristics of trustworthy AI systems. Valid & Reliable is a necessary
condition of trustworthiness and is shown as the base for other trustworthi-
ness characteristics. Accountable & Transparent is shown as a vertical box
because it relates to all other characteristics. 12
Fig. 5 Functions organize AI risk management activities at their highest level to
govern, map, measure, and manage AI risks. Governance is designed to be
a cross-cutting function to inform and be infused throughout the other three
functions. 20
Page ii
NIST AI 100-1 AI RMF 1.0
Executive Summary
Artificial intelligence (AI) technologies have significant potential to transform society and
people’s lives – from commerce and health to transportation and cybersecurity to the envi-
ronment and our planet. AI technologies can drive inclusive economic growth and support
scientific advancements that improve the conditions of our world. AI technologies, how-
ever, also pose risks that can negatively impact individuals, groups, organizations, commu-
nities, society, the environment, and the planet. Like risks for other types of technology, AI
risks can emerge in a variety of ways and can be characterized as long- or short-term, high-
or low-probability, systemic or localized, and high- or low-impact.
The AI RMF refers to an AI system as an engineered or machine-based system that
can, for a given set of objectives, generate outputs such as predictions, recommenda-
tions, or decisions influencing real or virtual environments. AI systems are designed
to operate with varying levels of autonomy (Adapted from: OECD Recommendation
on AI:2019; ISO /IEC 22989:2022).
While there are myriad standards and best practices to help organizations mitigate the risks
of traditional software or information-based systems, the risks posed by AI systems are in
many ways unique (See Appendix B). AI systems, for example, may be trained on data that
can change over time, sometimes significantly and unexpectedly, affecting system function-
ality and trustworthiness in ways that are hard to understand. AI systems and the contexts
in which they are deployed are frequently complex, making it difficult to detect and respond
to failures when they occur. AI systems are inherently socio-technical in nature, meaning
they are influenced by societal dynamics and human behavior. AI risks – and benefits –
can emerge from the interplay of technical aspects combined with societal factors related
to how a system is used, its interactions with other AI systems, who operates it, and the
social context in which it is deployed.
These risks make AI a uniquely challenging technology to deploy and utilize both for orga-
nizations and within society. Without proper controls, AI systems can amplify, perpetuate,
or exacerbate inequitable or undesirable outcomes for individuals and communities. With
proper controls, AI systems can mitigate and manage inequitable outcomes.
AI risk management is a key component of responsible development and use of AI sys-
tems. Responsible AI practices can help align the decisions about AI system design, de-
velopment, and uses with intended aim and values. Core concepts in responsible AI em-
phasize human centricity, social responsibility, and sustainability. AI risk management can
drive responsible uses and practices by prompting organizations and their internal teams
who design, develop, and deploy AI to think more critically about context and potential
or unexpected negative and positive impacts. Understanding and managing the risks of AI
systems will help to enhance trustworthiness, and in turn, cultivate public trust.
Page 1
NIST AI 100-1 AI RMF 1.0
Social responsibility can refer to the organization’s responsibility “for the impacts
of its decisions and activities on society and the environment through transparent
and ethical behavior” ( ISO 26000:2010). Sustainability refers to the “state of the
global system, including environmental, social, and economic aspects, in which the
needs of the present are met without compromising the ability of future generations
to meet their own needs” ( ISO /IEC TR 24368:2022). Responsible AI is meant to
result in technology that is also equitable and accountable. The expectation is that
organizational practices are carried out in accord with “professional responsibility,”
defined by ISO as an approach that “aims to ensure that professionals who design,
develop, or deploy AI systems and applications or AI-based products or systems,
recognize their unique position to exert influence on people, society, and the future
of AI” (ISO /IEC TR 24368:2022).
As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283),
the goal of the AI RMF is to offer a resource to the organizations designing, developing,
deploying, or using AI systems to help manage the many risks of AI and promote trustwor-
thy and responsible development and use of AI systems. The Framework is intended to be
voluntary, rights-preserving, non-sector-specific, and use-case agnostic, providing flexibil-
ity to organizations of all sizes and in all sectors and throughout society to implement the
approaches in the Framework.
The Framework is designed to equip organizations and individuals – referred to here as
AI actors – with approaches that increase the trustworthiness of AI systems, and to help
foster the responsible design, development, deployment, and use of AI systems over time.
AI actors are defined by the Organisation for Economic Co-operation and Development
(OECD) as “those who play an active role in the AI system lifecycle, including organiza-
tions and individuals that deploy or operate AI” [OECD (2019) Artificial Intelligence in
Society—OECD iLibrary] (See Appendix A).
The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies
continue to develop, and to be operationalized by organizations in varying degrees and
capacities so society can benefit from AI while also being protected from its potential
harms.
The Framework and supporting resources will be updated, expanded, and improved based
on evolving technology, the standards landscape around the world, and AI community ex-
perience and feedback. NIST will continue to align the AI RMF and related guidance with
applicable international standards, guidelines, and practices. As the AI RMF is put into
use, additional lessons will be learned to inform future updates and additional resources.
The Framework is divided into two parts. Part 1 discusses how organizations can frame
the risks related to AI and describes the intended audience. Next, AI risks and trustworthi-
ness are analyzed, outlining the characteristics of trustworthy AI systems, which include
Page 2
NIST AI 100-1 AI RMF 1.0
valid and reliable, safe, secure and resilient, accountable and transparent, explainable and
interpretable, privacy enhanced, and fair with their harmful biases managed.
Part 2 comprises the “Core” of the Framework. It describes four specific functions to help
organizations address the risks of AI systems in practice. These functions – GOVERN ,
MAP , MEASURE , and MANAGE – are broken down further into categories and subcate-
gories. While GOVERN applies to all stages of organizations’ AI risk management pro-
cesses and procedures, the MAP , MEASURE , and MANAGE functions can be applied in AI
system-specific contexts and at specific stages of the AI lifecycle.
Additional resources related to the Framework are included in the AI RMF Playbook,
which is available via the NIST AI RMF website:
https://www.nist.gov/itl/ai-risk-management-framework.
Development of the AI RMF by NIST in collaboration with the private and public sec-
tors is directed and consistent with its broader AI efforts called for by the National AI
Initiative Act of 2020, the National Security Commission on Artificial Intelligence recom-
mendations, and the Plan for Federal Engagement in Developing Technical Standards and
Related Tools. Engagement with the AI community during this Framework’s development
– via responses to a formal Request for Information, three widely attended workshops,
public comments on a concept paper and two drafts of the Framework, discussions at mul-
tiple public forums, and many small group meetings – has informed development of the AI
RMF 1.0 as well as AI research and development and evaluation conducted by NIST and
others. Priority research and additional guidance that will enhance this Framework will be
captured in an associated AI Risk Management Framework Roadmap to which NIST and
the broader community can contribute.
Page 3
NIST AI 100-1 AI RMF 1.0
Part 1: Foundational Information
1. Framing Risk
AI risk management offers a path to minimize potential negative impacts of AI systems,
such as threats to civil liberties and rights, while also providing opportunities to maximize
positive impacts. Addressing, documenting, and managing AI risks and potential negative
impacts effectively can lead to more trustworthy AI systems.
1.1 Understanding and Addressing Risks, Impacts, and Harms
In the context of the AI RMF,risk refers to the composite measure of an event’s probability
of occurring and the magnitude or degree of the consequences of the corresponding event.
The impacts, or consequences, of AI systems can be positive, negative, or both and can
result in opportunities or threats (Adapted from: ISO 31000:2018). When considering the
negative impact of a potential event, risk is a function of 1) the negative impact, or magni-
tude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of
occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be
experienced by individuals, groups, communities, organizations, society, the environment,
and the planet.
“Risk management refers to coordinated activities to direct and control an organiza-
tion with regard to risk” (Source: ISO 31000:2018).
While risk management processes generally address negative impacts, this Framework of-
fers approaches to minimize anticipated negative impacts of AI systems and identify op-
portunities to maximize positive impacts. Effectively managing the risk of potential harms
could lead to more trustworthy AI systems and unleash potential benefits to people (individ-
uals, communities, and society), organizations, and systems/ecosystems. Risk management
can enable AI developers and users to understand impacts and account for the inherent lim-
itations and uncertainties in their models and systems, which in turn can improve overall
system performance and trustworthiness and the likelihood that AI technologies will be
used in ways that are beneficial.
The AI RMF is designed to address new risks as they emerge. This flexibility is particularly
important where impacts are not easily foreseeable and applications are evolving. While
some AI risks and benefits are well-known, it can be challenging to assess negative impacts
and the degree of harms. Figure 1 provides examples of potential harms that can be related
to AI systems.
AI risk management efforts should consider that humans may assume that AI systems work
– and work well – in all settings. For example, whether correct or not, AI systems are
often perceived as being more objective than humans or as offering greater capabilities
than general software.
Page 4
NIST AI 100-1 AI RMF 1.0
Fig. 1. Examples of potential harms related to AI systems. Trustworthy AI systems and their
responsible use can mitigate negative risks and contribute to benefits for people, organizations, and
ecosystems.
1.2 Challenges for AI Risk Management
Several challenges are described below. They should be taken into account when managing
risks in pursuit of AI trustworthiness.
1.2.1 Risk Measurement
AI risks or failures that are not well-defined or adequately understood are difficult to mea-
sure quantitatively or qualitatively. The inability to appropriately measure AI risks does not
imply that an AI system necessarily poses either a high or low risk. Some risk measurement
challenges include:
Risks related to third-party software, hardware, and data: Third-party data or systems
can accelerate research and development and facilitate technology transition. They also
may complicate risk measurement. Risk can emerge both from third-party data, software or
hardware itself and how it is used. Risk metrics or methodologies used by the organization
developing the AI system may not align with the risk metrics or methodologies uses by
the organization deploying or operating the system. Also, the organization developing
the AI system may not be transparent about the risk metrics or methodologies it used. Risk
measurement and management can be complicated by how customers use or integrate third-
party data or systems into AI products or services, particularly without sufficient internal
governance structures and technical safeguards. Regardless, all parties and AI actors should
manage risk in the AI systems they develop, deploy, or use as standalone or integrated
components.
Tracking emergent risks: Organizations’ risk management efforts will be enhanced by
identifying and tracking emergent risks and considering techniques for measuring them.
Page 5
NIST AI 100-1 AI RMF 1.0
AI system impact assessment approaches can help AI actors understand potential impacts
or harms within specific contexts.
Availability of reliable metrics: The current lack of consensus on robust and verifiable
measurement methods for risk and trustworthiness, and applicability to different AI use
cases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure
negative risk or harms include the reality that development of metrics is often an institu-
tional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In
addition, measurement approaches can be oversimplified, gamed, lack critical nuance, be-
come relied upon in unexpected ways, or fail to account for differences in affected groups
and contexts.
Approaches for measuring impacts on a population work best if they recognize that contexts
matter, that harms may affect varied groups or sub-groups differently, and that communities
or other sub-groups who may be harmed are not always direct users of a system.
Risk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI
lifecycle may yield different results than measuring risk at a later stage; some risks may
be latent at a given point in time and may increase as AI systems adapt and evolve. Fur-
thermore, different AI actors across the AI lifecycle can have different risk perspectives.
For example, an AI developer who makes AI software available, such as pre-trained mod-
els, can have a different risk perspective than an AI actor who is responsible for deploying
that pre-trained model in a specific use case. Such deployers may not recognize that their
particular uses could entail risks which differ from those perceived by the initial developer.
All involved AI actors share responsibilities for designing, developing, and deploying a
trustworthy AI system that is fit for purpose.
Risk in real-world settings: While measuring AI risks in a laboratory or a controlled
environment may yield important insights pre-deployment, these measurements may differ
from risks that emerge in operational, real-world settings.
Inscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability
can be a result of the opaque nature of AI systems (limited explainability or interpretabil-
ity), lack of transparency or documentation in AI system development or deployment, or
inherent uncertainties in AI systems.
Human baseline: Risk management of AI systems that are intended to augment or replace
human activity, for example decision making, requires some form of baseline metrics for
comparison. This is difficult to systematize since AI systems carry out different tasks – and
perform tasks differently – than humans.
Page 6
NIST AI 100-1 AI RMF 1.0
1.2.2 Risk Tolerance
While the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk
tolerance refers to the organization’s or AI actor’s (see Appendix A) readiness to bear the
risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula-
tory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that
is acceptable to organizations or society are highly contextual and application and use-case
specific. Risk tolerances can be influenced by policies and norms established by AI sys-
tem owners, organizations, industries, communities, or policy makers. Risk tolerances are
likely to change over time as AI systems, policies, and norms evolve. Different organiza-
tions may have varied risk tolerances due to their particular organizational priorities and
resource considerations.
Emerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con-
tinue to be developed and debated by businesses, governments, academia, and civil society.
To the extent that challenges for specifying AI risk tolerances remain unresolved, there may
be contexts where a risk management framework is not yet readily applicable for mitigating
negative AI risks.
The Framework is intended to be flexible and to augment existing risk practices
which should align with applicable laws, regulations, and norms. Organizations
should follow existing regulations and guidelines for risk criteria, tolerance, and
response established by organizational, domain, discipline, sector, or professional
requirements. Some sectors or industries may have established definitions of harm or
established documentation, reporting, and disclosure requirements. Within sectors,
risk management may depend on existing guidelines for specific applications and
use case settings. Where established guidelines do not exist, organizations should
define reasonable risk tolerance. Once tolerance is defined, this AI RMF can be used
to manage risks and to document risk management processes.
1.2.3 Risk Prioritization
Attempting to eliminate negative risk entirely can be counterproductive in practice because
not all incidents and failures can be eliminated. Unrealistic expectations about risk may
lead organizations to allocate resources in a manner that makes risk triage inefficient or
impractical or wastes scarce resources. A risk management culture can help organizations
recognize that not all AI risks are the same, and resources can be allocated purposefully.
Actionable risk management efforts lay out clear guidelines for assessing trustworthiness
of each AI system an organization develops or deploys. Policies and resources should be
prioritized based on the assessed risk level and potential impact of an AI system. The extent
to which an AI system may be customized or tailored to the specific context of use by the
AI deployer can be a contributing factor.
Page 7
NIST AI 100-1 AI RMF 1.0
When applying the AI RMF, risks which the organization determines to be highest for the
AI systems within a given context of use call for the most urgent prioritization and most
thorough risk management process. In cases where an AI system presents unacceptable
negative risk levels – such as where significant negative impacts are imminent, severe harms
are actually occurring, or catastrophic risks are present – development and deployment
should cease in a safe manner until risks can be sufficiently managed. If an AI system’s
development, deployment, and use cases are found to be low-risk in a specific context, that
may suggest potentially lower prioritization.
Risk prioritization may differ between AI systems that are designed or deployed to directly
interact with humans as compared to AI systems that are not. Higher initial prioritization
may be called for in settings where the AI system is trained on large datasets comprised of
sensitive or protected data such as personally identifiable information, or where the outputs
of the AI systems have direct or indirect impact on humans. AI systems designed to interact
only with computational systems and trained on non-sensitive datasets (for example, data
collected from the physical environment) may call for lower initial prioritization. Nonethe-
less, regularly assessing and prioritizing risk based on context remains important because
non-human-facing AI systems can have downstream safety or social implications.
Residual risk – defined as risk remaining after risk treatment (Source: ISO GUIDE 73) –
directly impacts end users or affected individuals and communities. Documenting residual
risks will call for the system provider to fully consider the risks of deploying the AI product
and will inform end users about potential negative impacts of interacting with the system.
1.2.4 Organizational Integration and Management of Risk
AI risks should not be considered in isolation. Different AI actors have different responsi-
bilities and awareness depending on their roles in the lifecycle. For example, organizations
developing an AI system often will not have information about how the system may be
used. AI risk management should be integrated and incorporated into broader enterprise
risk management strategies and processes. Treating AI risks along with other critical risks,
such as cybersecurity and privacy, will yield a more integrated outcome and organizational
efficiencies.
The AI RMF may be utilized along with related guidance and frameworks for managing
AI system risks or broader enterprise risks. Some risks related to AI systems are common
across other types of software development and deployment. Examples of overlapping risks
include: privacy concerns related to the use of underlying data to train AI systems; the en-
ergy and environmental implications associated with resource-heavy computing demands;
security concerns related to the confidentiality, integrity, and availability of the system and
its training and output data; and general security of the underlying software and hardware
for AI systems.
Page 8
NIST AI 100-1 AI RMF 1.0
Organizations need to establish and maintain the appropriate accountability mechanisms,
roles and responsibilities, culture, and incentive structures for risk management to be ef-
fective. Use of the AI RMF alone will not lead to these changes or provide the appropriate
incentives. Effective risk management is realized through organizational commitment at
senior levels and may require cultural change within an organization or industry. In addi-
tion, small to medium-sized organizations managing AI risks or implementing the AI RMF
may face different challenges than large organizations, depending on their capabilities and
resources.
2. Audience
Identifying and managing AI risks and potential impacts – both positive and negative – re-
quires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will
represent a diversity of experience, expertise, and backgrounds and comprise demograph-
ically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors
across the AI lifecycle and dimensions.
The OECD has developed a framework for classifying AI lifecycle activities according to
five key socio-technical dimensions, each with properties relevant for AI policy and gover-
nance, including risk management [OECD (2022) OECD Framework for the Classification
of AI systems — OECD Digital Economy Papers]. Figure 2 shows these dimensions,
slightly modified by NIST for purposes of this framework. The NIST modification high-
lights the importance of test, evaluation, verification, and validation (TEVV) processes
throughout an AI lifecycle and generalizes the operational context of an AI system.
AI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI
Model, and Task and Output. AI actors involved in these dimensions who perform or
manage the design, development, deployment, evaluation, and use of AI systems and drive
AI risk management efforts are the primary AI RMF audience.
Representative AI actors across the lifecycle dimensions are listed in Figure 3 and described
in detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks
and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific
expertise are integrated throughout the AI lifecycle and are especially likely to benefit from
the Framework. Performed regularly, TEVV tasks can provide insights relative to technical,
societal, legal, and ethical standards or norms, and can assist with anticipating impacts and
assessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV
allows for both mid-course remediation and post-hoc risk management.
The People & Planet dimension at the center of Figure 2 represents human rights and the
broader well-being of society and the planet. The AI actors in this dimension comprise
a separate AI RMF audience who informs the primary audience. These AI actors may in-
clude trade associations, standards developing organizations, researchers, advocacy groups,
Page 9
NIST AI 100-1 AI RMF 1.0
Fig. 2. Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD
Framework for the Classification of AI systems — OECD Digital Economy Papers. The two inner
circles show AI systems’ key dimensions and the outer circle shows AI lifecycle stages. Ideally,
risk management efforts start with the Plan and Design function in the application context and are
performed throughout the AI system lifecycle. See Figure 3 for representative AI actors.
environmental groups, civil society organizations, end users, and potentially impacted in-
dividuals and communities. These actors can:
• assist in providing context and understanding potential and actual impacts;
• be a source of formal or quasi-formal norms and guidance for AI risk management;
• designate boundaries for AI operation (technical, societal, legal, and ethical); and
• promote discussion of the tradeoffs needed to balance societal values and priorities
related to civil liberties and rights, equity, the environment and the planet, and the
economy.
Successful risk management depends upon a sense of collective responsibility among AI
actors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse
perspectives, disciplines, professions, and experiences. Diverse teams contribute to more
open sharing of ideas and assumptions about the purposes and functions of technology –
making these implicit aspects more explicit. This broader collective perspective creates
opportunities for surfacing problems and identifying existing and emergent risks.
Page 10
NIST AI 100-1 AI RMF 1.0
Fig. 3. AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing,
evaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with
those building and using the models separated from those verifying and validating the models.
Page 11
NIST AI 100-1 AI RMF 1.0
3. AI Risks and Trustworthiness
For AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-
teria that are of value to interested parties. Approaches which enhance AI trustworthiness
can reduce negative AI risks. This Framework articulates the following characteristics of
trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI
systems include: valid and reliable, safe, secure and resilient, accountable and trans-
parent, explainable and interpretable, privacy-enhanced, and fair with harmful bias
managed. Creating trustworthy AI requires balancing each of these characteristics based
on the AI system’s context of use. While all characteristics are socio-technical system at-
tributes, accountability and transparency also relate to the processes and activities internal
to an AI system and its external setting. Neglecting these characteristics can increase the
probability and magnitude of negative consequences.
Fig. 4. Characteristics of trustworthy AI systems. Valid & Reliable is a necessary condition of
trustworthiness and is shown as the base for other trustworthiness characteristics. Accountable &
Transparent is shown as a vertical box because it relates to all other characteristics.
Trustworthiness characteristics (shown in Figure 4) are inextricably tied to social and orga-
nizational behavior, the datasets used by AI systems, selection of AI models and algorithms
and the decisions made by those who build them, and the interactions with the humans who
provide insight from and oversight of such systems. Human judgment should be employed
when deciding on the specific metrics related to AI trustworthiness characteristics and the
precise threshold values for those metrics.
Addressing AI trustworthiness characteristics individually will not ensure AI system trust-
worthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-
ting, and some will be more or less important in any given situation. Ultimately, trustwor-
thiness is a social concept that ranges across a spectrum and is only as strong as its weakest
characteristics.
When managing AI risks, organizations can face difficult decisions in balancing these char-
acteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for
interpretability and achieving privacy. In other cases, organizations might face a tradeoff
between predictive accuracy and interpretability. Or, under certain conditions such as data
sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions
Page 12
NIST AI 100-1 AI RMF 1.0
about fairness and other values in certain domains. Dealing with tradeoffs requires tak-
ing into account the decision-making context. These analyses can highlight the existence
and extent of tradeoffs between different measures, but they do not answer questions about
how to navigate the tradeoff. Those depend on the values at play in the relevantcontext and
should be resolved in a manner that is both transparent and appropriately justifiable.
There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For
example, subject matter experts can assist in the evaluation of TEVV findings and work
with product and deployment teams to align TEVV parameters to requirements and de-
ployment conditions. When properly resourced, increasing the breadth and diversity of
input from interested parties and relevant AI actors throughout the AI lifecycle can en-
hance opportunities for informing contextually sensitive evaluations, and for identifying
AI system benefits and positive impacts. These practices can increase the likelihood that
risks arising in social contexts are managed appropriately.
Understanding and treatment of trustworthiness characteristics depends on an AI actor’s
particular role within the AI lifecycle. For any given AI system, an AI designer or developer
may have a different perception of the characteristics than the deployer.
Trustworthiness characteristics explained in this document influence each other.
Highly secure but unfair systems, accurate but opaque and uninterpretable systems,
and inaccurate but secure, privacy-enhanced, and transparent systems are all unde-
sirable. A comprehensive approach to risk management calls for balancing tradeoffs
among the trustworthiness characteristics. It is the joint responsibility of all AI ac-
tors to determine whether AI technology is an appropriate or necessary tool for a
given context or purpose, and how to use it responsibly. The decision to commission
or deploy an AI system should be based on a contextual assessment of trustworthi-
ness characteristics and the relative risks, impacts, costs, and benefits, and informed
by a broad set of interested parties.
3.1 Valid and Reliable
Validation is the “confirmation, through the provision of objective evidence, that the re-
quirements for a specific intended use or application have been fulfilled” (Source: ISO
9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener-
alized to data and settings beyond their training creates and increases negative AI risks and
reduces trustworthiness.
Reliability is defined in the same standard as the “ability of an item to perform as required,
without failure, for a given time interval, under given conditions” (Source: ISO /IEC TS
5723:2022). Reliability is a goal for overall correctness of AI system operation under the
conditions of expected use and over a given period of time, including the entire lifetime of
the system.
Page 13
NIST AI 100-1 AI RMF 1.0
Accuracy and robustness contribute to the validity and trustworthiness of AI systems, and
can be in tension with one another in AI systems.
Accuracy is defined by ISO /IEC TS 5723:2022 as “closeness of results of observations,
computations, or estimates to the true values or the values accepted as being true.” Mea-
sures of accuracy should consider computational-centric measures (e.g., false positive and
false negative rates), human-AI teaming, and demonstrate external validity (generalizable
beyond the training conditions). Accuracy measurements should always be paired with
clearly defined and realistic test sets – that are representative of conditions of expected use
– and details about test methodology; these should be included in associated documen-
tation. Accuracy measurements may include disaggregation of results for different data
segments.
Robustness or generalizability is defined as the “ability of a system to maintain its level
of performance under a variety of circumstances” (Source: ISO /IEC TS 5723:2022). Ro-
bustness is a goal for appropriate system functionality in a broad set of conditions and
circumstances, including uses of AI systems not initially anticipated. Robustness requires
not only that the system perform exactly as it does under expected uses, but also that it
should perform in ways that minimize potential harms to people if it is operating in an
unexpected setting.
Validity and reliability for deployed AI systems are often assessed by ongoing testing or
monitoring that confirms a system is performing as intended. Measurement of validity,
accuracy, robustness, and reliability contribute to trustworthiness and should take into con-
sideration that certain types of failures can cause greater harm. AI risk management efforts
should prioritize the minimization of potential negative impacts, and may need to include
human intervention in cases where the AI system cannot detect or correct errors.
3.2 Safe
AI systems should “not under defined conditions, lead to a state in which human life,
health, property, or the environment is endangered” (Source: ISO /IEC TS 5723:2022). Safe
operation of AI systems is improved through:
• responsible design, development, and deployment practices;
• clear information to deployers on responsible use of the system;
• responsible decision-making by deployers and end users; and
• explanations and documentation of risks based on empirical evidence of incidents.
Different types of safety risks may require tailored AI risk management approaches based
on context and the severity of potential risks presented. Safety risks that pose a potential
risk of serious injury or death call for the most urgent prioritization and most thorough risk
management process.
Page 14
NIST AI 100-1 AI RMF 1.0
Employing safety considerations during the lifecycle and starting as early as possible with
planning and design can prevent failures or conditions that can render a system dangerous.
Other practical approaches for AI safety often relate to rigorous simulation and in-domain
testing, real-time monitoring, and the ability to shut down, modify, or have human inter-
vention into systems that deviate from intended or expected functionality.
AI safety risk management approaches should take cues from efforts and guidelines for
safety in fields such as transportation and healthcare, and align with existing sector- or
application-specific guidelines or standards.
3.3 Secure and Resilient
AI systems, as well as the ecosystems in which they are deployed, may be said to be re-
silient if they can withstand unexpected adverse events or unexpected changes in their envi-
ronment or use – or if they can maintain their functions and structure in the face of internal
and external change and degrade safely and gracefully when this is necessary (Adapted
from: ISO /IEC TS 5723:2022). Common security concerns relate to adversarial examples,
data poisoning, and the exfiltration of models, training data, or other intellectual property
through AI system endpoints. AI systems that can maintain confidentiality, integrity, and
availability through protection mechanisms that prevent unauthorized access and use may
be said to be secure. Guidelines in the NIST Cybersecurity Framework and Risk Manage-
ment Framework are among those which are applicable here.
Security and resilience are related but distinct characteristics. While resilience is the abil-
ity to return to normal function after an unexpected adverse event, security includes re-
silience but also encompasses protocols to avoid, protect against, respond to, or recover
from attacks. Resilience relates to robustness and goes beyond the provenance of the data
to encompass unexpected or adversarial use (or abuse or misuse) of the model or data.
3.4 Accountable and Transparent
Trustworthy AI depends upon accountability. Accountability presupposes transparency.
Transparencyreflects the extent to which information about an AI system and its outputs is
available to individuals interacting with such a system – regardless of whether they are even
aware that they are doing so. Meaningful transparency provides access to appropriate levels
of information based on the stage of the AI lifecycle and tailored to the role or knowledge
of AI actors or individuals interacting with or using the AI system. By promoting higher
levels of understanding, transparency increases confidence in the AI system.
This characteristic’s scope spans from design decisions and training data to model train-
ing, the structure of the model, its intended use cases, and how and when deployment,
post-deployment, or end user decisions were made and by whom. Transparency is often
necessary for actionable redress related to AI system outputs that are incorrect or otherwise
lead to negative impacts. Transparency should consider human-AI interaction: for exam-
Page 15
NIST AI 100-1 AI RMF 1.0
ple, how a human operator or user is notified when a potential or actual adverse outcome
caused by an AI system is detected. A transparent system is not necessarily an accurate,
privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an
opaque system possesses such characteristics, and to do so over time as complex systems
evolve.
The role of AI actors should be considered when seeking accountability for the outcomes of
AI systems. The relationship between risk and accountability associated with AI and tech-
nological systems more broadly differs across cultural, legal, sectoral, and societal contexts.
When consequences are severe, such as when life and liberty are at stake, AI developers
and deployers should consider proportionally and proactively adjusting their transparency
and accountability practices. Maintaining organizational practices and governing structures
for harm reduction, like risk management, can help lead to more accountable systems.
Measures to enhance transparency and accountability should also consider the impact of
these efforts on the implementing entity, including the level of necessary resources and the
need to safeguard proprietary information.
Maintaining the provenance of training data and supporting attribution of the AI system’s
decisions to subsets of training data can assist with both transparency and accountability.
Training data may also be subject to copyright and should follow applicable intellectual
property rights laws.
As transparency tools for AI systems and related documentation continue to evolve, devel-
opers of AI systems are encouraged to test different types of transparency tools in cooper-
ation with AI deployers to ensure that AI systems are used as intended.
3.5 Explainable and Interpretable
Explainability refers to a representation of the mechanisms underlying AI systems’ oper-
ation, whereas interpretability refers to the meaning of AI systems’ output in the context
of their designed functional purposes. Together, explainability and interpretability assist
those operating or overseeing an AI system, as well as users of an AI system, to gain
deeper insights into the functionality and trustworthiness of the system, including its out-
puts. The underlying assumption is that perceptions of negative risk stem from a lack of
ability to make sense of, or contextualize, system output appropriately. Explainable and
interpretable AI systems offer information that will help end users understand the purposes
and potential impact of an AI system.
Risk from lack of explainability may be managed by describing how AI systems function,
with descriptions tailored to individual differences such as the user’s role, knowledge, and
skill level. Explainable systems can be debugged and monitored more easily, and they lend
themselves to more thorough documentation, audit, and governance.
Page 16
NIST AI 100-1 AI RMF 1.0
Risks to interpretability often can be addressed by communicating a description of why
an AI system made a particular prediction or recommendation. (See “Four Principles of
Explainable Artificial Intelligence” and “Psychological Foundations of Explainability and
Interpretability in Artificial Intelligence” found here.)
Transparency, explainability, and interpretability are distinct characteristics that support
each other. Transparency can answer the question of “what happened” in the system. Ex-
plainability can answer the question of “how” a decision was made in the system. Inter-
pretability can answer the question of “why” a decision was made by the system and its
meaning or context to the user.
3.6 Privacy-Enhanced
Privacy refers generally to the norms and practices that help to safeguard human autonomy,
identity, and dignity. These norms and practices typically address freedom from intrusion,
limiting observation, or individuals’ agency to consent to disclosure or control of facets of
their identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool
for Improving Privacy through Enterprise Risk Management.)
Privacy values such as anonymity, confidentiality, and control generally should guide choices
for AI system design, development, and deployment. Privacy-related risks may influence
security, bias, and transparency and come with tradeoffs with these other characteristics.
Like safety and security, specific technical features of an AI system may promote or reduce
privacy. AI systems can also present new risks to privacy by allowing inference to identify
individuals or previously private information about individuals.
Privacy-enhancing technologies (“PETs”) for AI, as well as data minimizing methods such
as de-identification and aggregation for certain model outputs, can support design for
privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-
enhancing techniques can result in a loss in accuracy, affecting decisions about fairness
and other values in certain domains.
3.7 Fair – with Harmful Bias Managed
Fairness in AI includes concerns for equality and equity by addressing issues such as harm-
ful bias and discrimination. Standards of fairness can be complex and difficult to define be-
cause perceptions of fairness differ among cultures and may shift depending on application.
Organizations’ risk management efforts will be enhanced by recognizing and considering
these differences. Systems in which harmful biases are mitigated are not necessarily fair.
For example, systems in which predictions are somewhat balanced across demographic
groups may still be inaccessible to individuals with disabilities or affected by the digital
divide or may exacerbate existing disparities or systemic biases.
Page 17
NIST AI 100-1 AI RMF 1.0
Bias is broader than demographic balance and data representativeness. NIST has identified
three major categories of AI bias to be considered and managed: systemic, computational
and statistical, and human-cognitive. Each of these can occur in the absence of prejudice,
partiality, or discriminatory intent. Systemic bias can be present in AI datasets, the orga-
nizational norms, practices, and processes across the AI lifecycle, and the broader society
that uses AI systems. Computational and statistical biases can be present in AI datasets
and algorithmic processes, and often stem from systematic errors due to non-representative
samples. Human-cognitive biases relate to how an individual or group perceives AI sys-
tem information to make a decision or fill in missing information, or how humans think
about purposes and functions of an AI system. Human-cognitive biases are omnipresent
in decision-making processes across the AI lifecycle and system use, including the design,
implementation, operation, and maintenance of AI.
Bias exists in many forms and can become ingrained in the automated systems that help
make decisions about our lives. While bias is not always a negative phenomenon, AI sys-
tems can potentially increase the speed and scale of biases and perpetuate and amplify
harms to individuals, groups, communities, organizations, and society. Bias is tightly asso-
ciated with the concepts of transparency as well as fairness in society. (For more informa-
tion about bias, including the three categories, see NIST Special Publication 1270, Towards
a Standard for Identifying and Managing Bias in Artificial Intelligence.)
Page 18
NIST AI 100-1 AI RMF 1.0
4. Effectiveness of the AI RMF
Evaluations of AI RMF effectiveness – including ways to measure bottom-line improve-
ments in the trustworthiness of AI systems – will be part of future NIST activities, in
conjunction with the AI community.
Organizations and other users of the Framework are encouraged to periodically evaluate
whether the AI RMF has improved their ability to manage AI risks, including but not lim-
ited to their policies, processes, practices, implementation plans, indicators, measurements,
and expected outcomes. NIST intends to work collaboratively with others to develop met-
rics, methodologies, and goals for evaluating the AI RMF’s effectiveness, and to broadly
share results and supporting information. Framework users are expected to benefit from:
• enhanced processes for governing, mapping, measuring, and managing AI risk, and
clearly documenting outcomes;
• improved awareness of the relationships and tradeoffs among trustworthiness char-
acteristics, socio-technical approaches, and AI risks;
• explicit processes for making go/no-go system commissioning and deployment deci-
sions;
• established policies, processes, practices, and procedures for improving organiza-
tional accountability efforts related to AI system risks;
• enhanced organizational culture which prioritizes the identification and management
of AI system risks and potential impacts to individuals, communities, organizations,
and society;
• better information sharing within and across organizations about risks, decision-
making processes, responsibilities, common pitfalls, TEVV practices, and approaches
for continuous improvement;
• greater contextual knowledge for increased awareness of downstream risks;
• strengthened engagement with interested parties and relevant AI actors; and
• augmented capacity for TEVV of AI systems and associated risks.
Page 19
NIST AI 100-1 AI RMF 1.0
Part 2: Core and Profiles
5. AI RMF Core
The AI RMF Core provides outcomes and actions that enable dialogue, understanding, and
activities to manage AI risks and responsibly develop trustworthy AI systems. As illus-
trated in Figure 5, the Core is composed of four functions: GOVERN , MAP, MEASURE ,
and MANAGE . Each of these high-level functions is broken down into categories and sub-
categories. Categories and subcategories are subdivided into specific actions and outcomes.
Actions do not constitute a checklist, nor are they necessarily an ordered set of steps.
Fig. 5. Functions organize AI risk management activities at their highest level to govern, map,
measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform
and be infused throughout the other three functions.
Risk management should be continuous, timely, and performed throughout the AI system
lifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects
diverse and multidisciplinary perspectives, potentially including the views of AI actors out-
side the organization. Having a diverse team contributes to more open sharing of ideas and
assumptions about purposes and functions of the technology being designed, developed,
Page 20
NIST AI 100-1 AI RMF 1.0
deployed, or evaluated – which can create opportunities to surface problems and identify
existing and emergent risks.
An online companion resource to the AI RMF, the NIST AI RMF Playbook, is available
to help organizations navigate the AI RMF and achieve its outcomes through suggested
tactical actions they can apply within their own contexts. Like the AI RMF, the Playbook
is voluntary and organizations can utilize the suggestions according to their needs and
interests. Playbook users can create tailored guidance selected from suggested material
for their own use and contribute their suggestions for sharing with the broader community.
Along with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI
Resource Center.
Framework users may apply these functions as best suits their needs for managing
AI risks based on their resources and capabilities. Some organizations may choose
to select from among the categories and subcategories; others may choose and have
the capacity to apply all categories and subcategories. Assuming a governance struc-
ture is in place, functions may be performed in any order across the AI lifecycle as
deemed to add value by a user of the framework. After instituting the outcomes in
GOVERN , most users of the AI RMF would start with the MAP function and con-
tinue to MEASURE or MANAGE . However users integrate the functions, the process
should be iterative, with cross-referencing between functions as necessary. Simi-
larly, there are categories and subcategories with elements that apply to multiple
functions, or that logically should take place before certain subcategory decisions.
5.1 Govern
The GOVERN function:
• cultivates and implements a culture of risk management within organizations design-
ing, developing, deploying, evaluating, or acquiring AI systems;
• outlines processes, documents, and organizational schemes that anticipate, identify,
and manage the risks a system can pose, including to users and others across society
– and procedures to achieve those outcomes;
• incorporates processes to assess potential impacts;
• provides a structure by which AI risk management functions can align with organi-
zational principles, policies, and strategic priorities;
• connects technical aspects of AI system design and development to organizational
values and principles, and enables organizational practices and competencies for the
individuals involved in acquiring, training, deploying, and monitoring such systems;
and
• addresses full product lifecycle and associated processes, including legal and other
issues concerning use of third-party software or hardware systems and data.
Page 21
NIST AI 100-1 AI RMF 1.0
GOVERN is a cross-cutting function that is infused throughout AI risk management and
enables the other functions of the process. Aspects of GOVERN , especially those related to
compliance or evaluation, should be integrated into each of the other functions. Attention
to governance is a continual and intrinsic requirement for effective AI risk management
over an AI system’s lifespan and the organization’s hierarchy.
Strong governance can drive and enhance internal practices and norms to facilitate orga-
nizational risk culture. Governing authorities can determine the overarching policies that
direct an organization’s mission, goals, values, culture, and risk tolerance. Senior leader-
ship sets the tone for risk management within an organization, and with it, organizational
culture. Management aligns the technical aspects of AI risk management to policies and
operations. Documentation can enhance transparency, improve human review processes,
and bolster accountability in AI system teams.
After putting in place the structures, systems, processes, and teams described in the GOV-
ERN function, organizations should benefit from a purpose-driven culture focused on risk
understanding and management. It is incumbent on Framework users to continue to ex-
ecute the GOVERN function as knowledge, cultures, and needs or expectations from AI
actors evolve over time.
Practices related to governing AI risks are described in the NIST AI RMF Playbook. Table
1 lists the GOVERN function’s categories and subcategories.
Table 1: Categories and subcategories for the GOVERN function.
GOVERN 1:
Policies, processes,
procedures, and
practices across the
organization related
to the mapping,
measuring, and
managing of AI
risks are in place,
transparent, and
implemented
effectively.
GOVERN 1.1: Legal and regulatory requirements involving AI
are understood, managed, and documented.
GOVERN 1.2: The characteristics of trustworthy AI are inte-
grated into organizational policies, processes, procedures, and
practices.
GOVERN 1.3: Processes, procedures, and practices are in place
to determine the needed level of risk management activities based
on the organization’s risk tolerance.
GOVERN 1.4: The risk management process and its outcomes are
established through transparent policies, procedures, and other
controls based on organizational risk priorities.
Categories Subcategories
Continued on next page
Page 22
NIST AI 100-1 AI RMF 1.0
Table 1: Categories and subcategories for the GOVERN function. (Continued)
GOVERN 1.5: Ongoing monitoring and periodic review of the
risk management process and its outcomes are planned and or-
ganizational roles and responsibilities clearly defined, including
determining the frequency of periodic review.
GOVERN 1.6: Mechanisms are in place to inventory AI systems
and are resourced according to organizational risk priorities.
GOVERN 1.7: Processes and procedures are in place for decom-
missioning and phasing out AI systems safely and in a man-
ner that does not increase risks or decrease the organization’s
trustworthiness.
GOVERN 2:
Accountability
structures are in
place so that the
appropriate teams
and individuals are
empowered,
responsible, and
trained for mapping,
measuring, and
managing AI risks.
GOVERN 2.1: Roles and responsibilities and lines of communi-
cation related to mapping, measuring, and managing AI risks are
documented and are clear to individuals and teams throughout
the organization.
GOVERN 2.2: The organization’s personnel and partners receive
AI risk management training to enable them to perform their du-
ties and responsibilities consistent with related policies, proce-
dures, and agreements.
GOVERN 2.3: Executive leadership of the organization takes re-
sponsibility for decisions about risks associated with AI system
development and deployment.
GOVERN 3:
Workforce diversity,
equity, inclusion,
and accessibility
processes are
prioritized in the
mapping,
measuring, and
managing of AI
risks throughout the
lifecycle.
GOVERN 3.1: Decision-making related to mapping, measuring,
and managing AI risks throughout the lifecycle is informed by a
diverse team (e.g., diversity of demographics, disciplines, expe-
rience, expertise, and backgrounds).
GOVERN 3.2: Policies and procedures are in place to define and
differentiate roles and responsibilities for human-AI configura-
tions and oversight of AI systems.
GOVERN 4:
Organizational
teams are committed
to a culture
GOVERN 4.1: Organizational policies and practices are in place
to foster a critical thinking and safety-first mindset in the design,
development, deployment, and uses of AI systems to minimize
potential negative impacts.
Categories Subcategories
Continued on next page
Page 23
NIST AI 100-1 AI RMF 1.0
Table 1: Categories and subcategories for the GOVERN function. (Continued)
that considers and
communicates AI
risk.
GOVERN 4.2: Organizational teams document the risks and po-
tential impacts of the AI technology they design, develop, deploy,
evaluate, and use, and they communicate about the impacts more
broadly.
GOVERN 4.3: Organizational practices are in place to enable AI
testing, identification of incidents, and information sharing.
GOVERN 5:
Processes are in
place for robust
engagement with
relevant AI actors.
GOVERN 5.1: Organizational policies and practices are in place
to collect, consider, prioritize, and integrate feedback from those
external to the team that developed or deployed the AI system
regarding the potential individual and societal impacts related to
AI risks.
GOVERN 5.2: Mechanisms are established to enable the team
that developed or deployed AI systems to regularly incorporate
adjudicated feedback from relevant AI actors into system design
and implementation.
GOVERN 6: Policies
and procedures are
in place to address
AI risks and benefits
arising from
third-party software
and data and other
supply chain issues.
GOVERN 6.1: Policies and procedures are in place that address
AI risks associated with third-party entities, including risks of in-
fringement of a third-party’s intellectual property or other rights.
GOVERN 6.2: Contingency processes are in place to handle
failures or incidents in third-party data or AI systems deemed to
be high-risk.
Categories Subcategories
5.2 Map
The MAP function establishes the context to frame risks related to an AI system. The AI
lifecycle consists of many interdependent activities involving a diverse set of actors (See
Figure 3). In practice, AI actors in charge of one part of the process often do not have full
visibility or control over other parts and their associated contexts. The interdependencies
between these activities, and among the relevant AI actors, can make it difficult to reliably
anticipate impacts of AI systems. For example, early decisions in identifying purposes and
objectives of an AI system can alter its behavior and capabilities, and the dynamics of de-
ployment setting (such as end users or impacted individuals) can shape the impacts of AI
system decisions. As a result, the best intentions within one dimension of the AI lifecycle
can be undermined via interactions with decisions and conditions in other, later activities.
Page 24
NIST AI 100-1 AI RMF 1.0
This complexity and varying levels of visibility can introduce uncertainty into risk man-
agement practices. Anticipating, assessing, and otherwise addressing potential sources of
negative risk can mitigate this uncertainty and enhance the integrity of the decision process.
The information gathered while carrying out the MAP function enables negative risk pre-
vention and informs decisions for processes such as model management, as well as an
initial decision about appropriateness or the need for an AI solution. Outcomes in the
MAP function are the basis for the MEASURE and MANAGE functions. Without contex-
tual knowledge, and awareness of risks within the identified contexts, risk management is
difficult to perform. The MAP function is intended to enhance an organization’s ability to
identify risks and broader contributing factors.
Implementation of this function is enhanced by incorporating perspectives from a diverse
internal team and engagement with those external to the team that developed or deployed
the AI system. Engagement with external collaborators, end users, potentially impacted
communities, and others may vary based on the risk level of a particular AI system, the
makeup of the internal team, and organizational policies. Gathering such broad perspec-
tives can help organizations proactively prevent negative risks and develop more trustwor-
thy AI systems by:
• improving their capacity for understanding contexts;
• checking their assumptions about context of use;
• enabling recognition of when systems are not functional within or out of their in-
tended context;
• identifying positive and beneficial uses of their existing AI systems;
• improving understanding of limitations in AI and ML processes;
• identifying constraints in real-world applications that may lead to negative impacts;
• identifying known and foreseeable negative impacts related to intended use of AI
systems; and
• anticipating risks of the use of AI systems beyond intended use.
After completing the MAP function, Framework users should have sufficient contextual
knowledge about AI system impacts to inform an initial go/no-go decision about whether
to design, develop, or deploy an AI system. If a decision is made to proceed, organizations
should utilize the MEASURE and MANAGE functions along with policies and procedures
put into place in the GOVERN function to assist in AI risk management efforts. It is incum-
bent on Framework users to continue applying the MAP function to AI systems as context,
capabilities, risks, benefits, and potential impacts evolve over time.
Practices related to mapping AI risks are described in the NIST AI RMF Playbook. Table
2 lists the MAP function’s categories and subcategories.
Page 25
NIST AI 100-1 AI RMF 1.0
Table 2: Categories and subcategories for the MAP function.
MAP 1: Context is
established and
understood.
MAP 1.1: Intended purposes, potentially beneficial uses, context-
specific laws, norms and expectations, and prospective settings in
which the AI system will be deployed are understood and docu-
mented. Considerations include: the specific set or types of users
along with their expectations; potential positive and negative im-
pacts of system uses to individuals, communities, organizations,
society, and the planet; assumptions and related limitations about
AI system purposes, uses, and risks across the development or
product AI lifecycle; and related TEVV and system metrics.
MAP 1.2: Interdisciplinary AI actors, competencies, skills, and
capacities for establishing context reflect demographic diversity
and broad domain and user experience expertise, and their par-
ticipation is documented. Opportunities for interdisciplinary col-
laboration are prioritized.
MAP 1.3: The organization’s mission and relevant goals for AI
technology are understood and documented.
MAP 1.4: The business value or context of business use has been
clearly defined or – in the case of assessing existing AI systems
– re-evaluated.
MAP 1.5: Organizational risk tolerances are determined and
documented.
MAP 1.6: System requirements (e.g., “the system shall respect
the privacy of its users”) are elicited from and understood by rel-
evant AI actors. Design decisions take socio-technical implica-
tions into account to address AI risks.
MAP 2:
Categorization of
the AI system is
performed.
MAP 2.1: The specific tasks and methods used to implement the
tasks that the AI system will support are defined (e.g., classifiers,
generative models, recommenders).
MAP 2.2: Information about the AI system’s knowledge limits
and how system output may be utilized and overseen by humans
is documented. Documentation provides sufficient information
to assist relevant AI actors when making decisions and taking
subsequent actions.
Categories Subcategories
Continued on next page
Page 26
NIST AI 100-1 AI RMF 1.0
Table 2: Categories and subcategories for the MAP function. (Continued)
MAP 2.3: Scientific integrity and TEVV considerations are iden-
tified and documented, including those related to experimental
design, data collection and selection (e.g., availability, repre-
sentativeness, suitability), system trustworthiness, and construct
validation.
MAP 3: AI
capabilities, targeted
usage, goals, and
expected benefits
and costs compared
with appropriate
benchmarks are
understood.
MAP 3.1: Potential benefits of intended AI system functionality
and performance are examined and documented.
MAP 3.2: Potential costs, including non-monetary costs, which
result from expected or realized AI errors or system functionality
and trustworthiness – as connected to organizational risk toler-
ance – are examined and documented.
MAP 3.3: Targeted application scope is specified and docu-
mented based on the system’s capability, established context, and
AI system categorization.
MAP 3.4: Processes for operator and practitioner proficiency
with AI system performance and trustworthiness – and relevant
technical standards and certifications – are defined, assessed, and
documented.
MAP 3.5: Processes for human oversight are defined, assessed,
and documented in accordance with organizational policies from
the GOVERN function.
MAP 4: Risks and
benefits are mapped
for all components
of the AI system
including third-party
software and data.
MAP 4.1: Approaches for mapping AI technology and legal risks
of its components – including the use of third-party data or soft-
ware – are in place, followed, and documented, as are risks of in-
fringement of a third party’s intellectual property or other rights.
MAP 4.2: Internal risk controls for components of the AI sys-
tem, including third-party AI technologies, are identified and
documented.
MAP 5: Impacts to
individuals, groups,
communities,
organizations, and
society are
characterized.
MAP 5.1: Likelihood and magnitude of each identified impact
(both potentially beneficial and harmful) based on expected use,
past uses of AI systems in similar contexts, public incident re-
ports, feedback from those external to the team that developed
or deployed the AI system, or other data are identified and
documented.
Categories Subcategories
Continued on next page
Page 27
NIST AI 100-1 AI RMF 1.0
Table 2: Categories and subcategories for the MAP function. (Continued)
MAP 5.2: Practices and personnel for supporting regular en-
gagement with relevant AI actors and integrating feedback about
positive, negative, and unanticipated impacts are in place and
documented.
Categories Subcategories
5.3 Measure
The MEASURE function employs quantitative, qualitative, or mixed-method tools, tech-
niques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related
impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs
the MANAGE function. AI systems should be tested before their deployment and regu-
larly while in operation. AI risk measurements include documenting aspects of systems’
functionality and trustworthiness.
Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact,
and human-AI configurations. Processes developed or adopted in the MEASURE function
should include rigorous software testing and performance assessment methodologies with
associated measures of uncertainty, comparisons to performance benchmarks, and formal-
ized reporting and documentation of results. Processes for independent review can improve
the effectiveness of testing and can mitigate internal biases and potential conflicts of inter-
est.
Where tradeoffs among the trustworthy characteristics arise, measurement provides a trace-
able basis to inform management decisions. Options may include recalibration, impact
mitigation, or removal of the system from design, development, production, or use, as well
as a range of compensating, detective, deterrent, directive, and recovery controls.
After completing the MEASURE function, objective, repeatable, or scalable test, evaluation,
verification, and validation (TEVV) processes including metrics, methods, and methodolo-
gies are in place, followed, and documented. Metrics and measurement methodologies
should adhere to scientific, legal, and ethical norms and be carried out in an open and trans-
parent process. New types of measurement, qualitative and quantitative, may need to be
developed. The degree to which each measurement type provides unique and meaningful
information to the assessment of AI risks should be considered. Framework users will en-
hance their capacity to comprehensively evaluate system trustworthiness, identify and track
existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will
be utilized in the MANAGE function to assist risk monitoring and response efforts. It is in-
cumbent on Framework users to continue applying the MEASURE function to AI systems
as knowledge, methodologies, risks, and impacts evolve over time.
Page 28
NIST AI 100-1 AI RMF 1.0
Practices related to measuring AI risks are described in the NIST AI RMF Playbook. Table
3 lists the MEASURE function’s categories and subcategories.
Table 3: Categories and subcategories for the MEASURE function.
MEASURE 1:
Appropriate
methods and metrics
are identified and
applied.
MEASURE 1.1: Approaches and metrics for measurement of AI
risks enumerated during the MAP function are selected for imple-
mentation starting with the most significant AI risks. The risks
or trustworthiness characteristics that will not – or cannot – be
measured are properly documented.
MEASURE 1.2: Appropriateness of AI metrics and effectiveness
of existing controls are regularly assessed and updated, including
reports of errors and potential impacts on affected communities.
MEASURE 1.3: Internal experts who did not serve as front-line
developers for the system and/or independent assessors are in-
volved in regular assessments and updates. Domain experts,
users, AI actors external to the team that developed or deployed
the AI system, and affected communities are consulted in support
of assessments as necessary per organizational risk tolerance.
MEASURE 2: AI
systems are
evaluated for
trustworthy
characteristics.
MEASURE 2.1: Test sets, metrics, and details about the tools used
during TEVV are documented.
MEASURE 2.2: Evaluations involving human subjects meet ap-
plicable requirements (including human subject protection) and
are representative of the relevant population.
MEASURE 2.3: AI system performance or assurance criteria
are measured qualitatively or quantitatively and demonstrated
for conditions similar to deployment setting(s). Measures are
documented.
MEASURE 2.4: The functionality and behavior of the AI sys-
tem and its components – as identified in the MAP function – are
monitored when in production.
MEASURE 2.5: The AI system to be deployed is demonstrated
to be valid and reliable. Limitations of the generalizability be-
yond the conditions under which the technology was developed
are documented.
Categories Subcategories
Continued on next page
Page 29
NIST AI 100-1 AI RMF 1.0
Table 3: Categories and subcategories for the MEASURE function. (Continued)
MEASURE 2.6: The AI system is evaluated regularly for safety
risks – as identified in theMAP function. The AI system to be de-
ployed is demonstrated to be safe, its residual negative risk does
not exceed the risk tolerance, and it can fail safely, particularly if
made to operate beyond its knowledge limits. Safety metrics re-
flect system reliability and robustness, real-time monitoring, and
response times for AI system failures.
MEASURE 2.7: AI system security and resilience – as identified
in the MAP function – are evaluated and documented.
MEASURE 2.8: Risks associated with transparency and account-
ability – as identified in the MAP function – are examined and
documented.
MEASURE 2.9: The AI model is explained, validated, and docu-
mented, and AI system output is interpreted within its context –
as identified in the MAP function – to inform responsible use and
governance.
MEASURE 2.10: Privacy risk of the AI system – as identified in
the MAP function – is examined and documented.
MEASURE 2.11: Fairness and bias – as identified in the MAP
function – are evaluated and results are documented.
MEASURE 2.12: Environmental impact and sustainability of AI
model training and management activities – as identified in the
MAP function – are assessed and documented.
MEASURE 2.13: Effectiveness of the employed TEVV met-
rics and processes in the MEASURE function are evaluated and
documented.
MEASURE 3:
Mechanisms for
tracking identified
AI risks over time
are in place.
MEASURE 3.1: Approaches, personnel, and documentation are
in place to regularly identify and track existing, unanticipated,
and emergent AI risks based on factors such as intended and ac-
tual performance in deployed contexts.
MEASURE 3.2: Risk tracking approaches are considered for
settings where AI risks are difficult to assess using currently
available measurement techniques or where metrics are not yet
available.
Categories Subcategories
Continued on next page
Page 30
NIST AI 100-1 AI RMF 1.0
Table 3: Categories and subcategories for the MEASURE function. (Continued)
MEASURE 3.3: Feedback processes for end users and impacted
communities to report problems and appeal system outcomes are
established and integrated into AI system evaluation metrics.
MEASURE 4:
Feedback about
efficacy of
measurement is
gathered and
assessed.
MEASURE 4.1: Measurement approaches for identifying AI risks
are connected to deployment context(s) and informed through
consultation with domain experts and other end users. Ap-
proaches are documented.
MEASURE 4.2: Measurement results regarding AI system trust-
worthiness in deployment context(s) and across the AI lifecycle
are informed by input from domain experts and relevant AI ac-
tors to validate whether the system is performing consistently as
intended. Results are documented.
MEASURE 4.3: Measurable performance improvements or de-
clines based on consultations with relevant AI actors, in-
cluding affected communities, and field data about context-
relevant risks and trustworthiness characteristics are identified
and documented.
Categories Subcategories
5.4 Manage
The MANAGE function entails allocating risk resources to mapped and measured risks on
a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to
respond to, recover from, and communicate about incidents or events.
Contextual information gleaned from expert consultation and input from relevant AI actors
– established in GOVERN and carried out in MAP – is utilized in this function to decrease
the likelihood of system failures and negative impacts. Systematic documentation practices
established in GOVERN and utilized in MAP and MEASURE bolster AI risk management
efforts and increase transparency and accountability. Processes for assessing emergent risks
are in place, along with mechanisms for continual improvement.
After completing the MANAGE function, plans for prioritizing risk and regular monitoring
and improvement will be in place. Framework users will have enhanced capacity to man-
age the risks of deployed AI systems and to allocate risk management resources based on
assessed and prioritized risks. It is incumbent on Framework users to continue to apply
the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or
expectations from relevant AI actors evolve over time.
Page 31
NIST AI 100-1 AI RMF 1.0
Practices related to managing AI risks are described in the NIST AI RMF Playbook. Table
4 lists the MANAGE function’s categories and subcategories.
Table 4: Categories and subcategories for the MANAGE function.
MANAGE 1: AI
risks based on
assessments and
other analytical
output from the
MAP and MEASURE
functions are
prioritized,
responded to, and
managed.
MANAGE 1.1: A determination is made as to whether the AI
system achieves its intended purposes and stated objectives and
whether its development or deployment should proceed.
MANAGE 1.2: Treatment of documented AI risks is prioritized
based on impact, likelihood, and available resources or methods.
MANAGE 1.3: Responses to the AI risks deemed high priority, as
identified by the MAP function, are developed, planned, and doc-
umented. Risk response options can include mitigating, transfer-
ring, avoiding, or accepting.
MANAGE 1.4: Negative residual risks (defined as the sum of all
unmitigated risks) to both downstream acquirers of AI systems
and end users are documented.
MANAGE 2:
Strategies to
maximize AI
benefits and
minimize negative
impacts are planned,
prepared,
implemented,
documented, and
informed by input
from relevant AI
actors.
MANAGE 2.1: Resources required to manage AI risks are taken
into account – along with viable non-AI alternative systems, ap-
proaches, or methods – to reduce the magnitude or likelihood of
potential impacts.
MANAGE 2.2: Mechanisms are in place and applied to sustain
the value of deployed AI systems.
MANAGE 2.3: Procedures are followed to respond to and recover
from a previously unknown risk when it is identified.
MANAGE 2.4: Mechanisms are in place and applied, and respon-
sibilities are assigned and understood, to supersede, disengage, or
deactivate AI systems that demonstrate performance or outcomes
inconsistent with intended use.
MANAGE 3: AI
risks and benefits
from third-party
entities are
managed.
MANAGE 3.1: AI risks and benefits from third-party resources
are regularly monitored, and risk controls are applied and
documented.
MANAGE 3.2: Pre-trained models which are used for develop-
ment are monitored as part of AI system regular monitoring and
maintenance.
Categories Subcategories
Continued on next page
Page 32
NIST AI 100-1 AI RMF 1.0
Table 4: Categories and subcategories for the MANAGE function. (Continued)
MANAGE 4: Risk
treatments,
including response
and recovery, and
communication
plans for the
identified and
measured AI risks
are documented and
monitored regularly.
MANAGE 4.1: Post-deployment AI system monitoring plans
are implemented, including mechanisms for capturing and eval-
uating input from users and other relevant AI actors, appeal
and override, decommissioning, incident response, recovery, and
change management.
MANAGE 4.2: Measurable activities for continual improvements
are integrated into AI system updates and include regular engage-
ment with interested parties, including relevant AI actors.
MANAGE 4.3: Incidents and errors are communicated to relevant
AI actors, including affected communities. Processes for track-
ing, responding to, and recovering from incidents and errors are
followed and documented.
Categories Subcategories
6. AI RMF Profiles
AI RMF use-case profiles are implementations of the AI RMF functions, categories, and
subcategories for a specific setting or application based on the requirements, risk tolerance,
and resources of the Framework user: for example, an AI RMF hiring profile or an AI
RMF fair housing profile. Profiles may illustrate and offer insights into how risk can be
managed at various stages of the AI lifecycle or in specific sector, technology, or end-use
applications. AI RMF profiles assist organizations in deciding how they might best manage
AI risk that is well-aligned with their goals, considers legal/regulatory requirements and
best practices, and reflects risk management priorities.
AI RMF temporal profiles are descriptions of either the current state or the desired, target
state of specific AI risk management activities within a given sector, industry, organization,
or application context. An AI RMF Current Profile indicates how AI is currently being
managed and the related risks in terms of current outcomes. A Target Profile indicates the
outcomes needed to achieve the desired or target AI risk management goals.
Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk
management objectives. Action plans can be developed to address these gaps to fulfill
outcomes in a given category or subcategory. Prioritization of gap mitigation is driven by
the user’s needs and risk management processes. This risk-based approach also enables
Framework users to compare their approaches with other approaches and to gauge the
resources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-
effective, prioritized manner.
Page 33
NIST AI 100-1 AI RMF 1.0
AI RMF cross-sectoral profilescover risks of models or applications that can be used across
use cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure,
and manage risks for activities or business processes common across sectors such as the
use of large language models, cloud-based services or acquisition.
This Framework does not prescribe profile templates, allowing for flexibility in implemen-
tation.
Page 34
NIST AI 100-1 AI RMF 1.0
Appendix A:
Descriptions of AI Actor Tasks from Figures 2 and 3
AI Design tasks are performed during the Application Context and Data and Input phases
of the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI
systems and are responsible for the planning, design, and data collection and processing
tasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include ar-
ticulating and documenting the system’s concept and objectives, underlying assumptions,
context, and requirements; gathering and cleaning data; and documenting the metadata
and characteristics of the dataset. AI actors in this category include data scientists, do-
main experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion,
and accessibility, members of impacted communities, human factors experts (e.g., UX/UI
design), governance experts, data engineers, data providers, system funders, product man-
agers, third-party entities, evaluators, and legal and privacy governance.
AI Development tasks are performed during the AI Model phase of the lifecycle in Figure
2. AI Development actors provide the initial infrastructure of AI systems and are responsi-
ble for model building and interpretation tasks, which involve the creation, selection, cali-
bration, training, and/or testing of models or algorithms. AI actors in this category include
machine learning experts, data scientists, developers, third-party entities, legal and privacy
governance experts, and experts in the socio-cultural and contextual factors associated with
the deployment setting.
AI Deployment tasks are performed during the Task and Output phase of the lifecycle in
Figure 2. AI Deployment actors are responsible for contextual decisions relating to how
the AI system is used to assure deployment of the system into production. Related tasks
include piloting the system, checking compatibility with legacy systems, ensuring regu-
latory compliance, managing organizational change, and evaluating user experience. AI
actors in this category include system integrators, software developers, end users, oper-
ators and practitioners, evaluators, and domain experts with expertise in human factors,
socio-cultural analysis, and governance.
Operation and Monitoring tasks are performed in the Application Context/Operate and
Monitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are
responsible for operating the AI system and working with others to regularly assess system
output and impacts. AI actors in this category include system operators, domain experts, AI
designers, users who interpret or incorporate the output of AI systems, product developers,
evaluators and auditors, compliance experts, organizational management, and members of
the research community.
Test, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout
the AI lifecycle. They are carried out by AI actors who examine the AI system or its
components, or detect and remediate problems. Ideally, AI actors carrying out verification
Page 35
NIST AI 100-1 AI RMF 1.0
and validation tasks are distinct from those who perform test and evaluation actions. Tasks
can be incorporated into a phase as early as design, where tests are planned in accordance
with the design requirement.
• TEVV tasks for design, planning, and data may center on internal and external vali-
dation of assumptions for system design, data collection, and measurements relative
to the intended context of deployment or application.
• TEVV tasks for development (i.e., model building) include model validation and
assessment.
• TEVV tasks for deployment include system validation and integration in production,
with testing, and recalibration for systems and process integration, user experience,
and compliance with existing legal, regulatory, and ethical specifications.
• TEVV tasks for operations involve ongoing monitoring for periodic updates, testing,
and subject matter expert (SME) recalibration of models, the tracking of incidents
or errors reported and their management, the detection of emergent properties and
related impacts, and processes for redress and response.
Human Factors tasks and activities are found throughout the dimensions of the AI life-
cycle. They include human-centered design practices and methodologies, promoting the
active involvement of end users and other interested parties and relevant AI actors, incor-
porating context-specific norms and values in system design, evaluating and adapting end
user experiences, and broad integration of humans and human dynamics in all phases of the
AI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives
to understand context of use, inform interdisciplinary and demographic diversity, engage
in consultative processes, design and evaluate user experience, perform human-centered
evaluation and testing, and inform impact assessments.
Domain Expert tasks involve input from multidisciplinary practitioners or scholars who
provide knowledge or expertise in – and about – an industry sector, economic sector, con-
text, or application area where an AI system is being used. AI actors who are domain
experts can provide essential guidance for AI system design and development, and inter-
pret outputs in support of work performed by TEVV and AI impact assessment teams.
AI Impact Assessment tasks include assessing and evaluating requirements for AI system
accountability, combating harmful bias, examining impacts of AI systems, product safety,
liability, and security, among others. AI actors such as impact assessors and evaluators
provide technical, human factor, socio-cultural, and legal expertise.
Procurement tasks are conducted by AI actors with financial, legal, or policy management
authority for acquisition of AI models, products, or services from a third-party developer,
vendor, or contractor.
Governance and Oversight tasks are assumed by AI actors with management, fiduciary,
and legal authority and responsibility for the organization in which an AI system is de-
Page 36
NIST AI 100-1 AI RMF 1.0
signed, developed, and/or deployed. Key AI actors responsible for AI governance include
organizational management, senior leadership, and the Board of Directors. These actors
are parties that are concerned with the impact and sustainability of the organization as a
whole.
Additional AI Actors
Third-party entities include providers, developers, vendors, and evaluators of data, al-
gorithms, models, and/or systems and related services for another organization or the or-
ganization’s customers or clients. Third-party entities are responsible for AI design and
development tasks, in whole or in part. By definition, they are external to the design, devel-
opment, or deployment team of the organization that acquires its technologies or services.
The technologies acquired from third-party entities may be complex or opaque, and risk
tolerances may not align with the deploying or operating organization.
End users of an AI system are the individuals or groups that use the system for specific
purposes. These individuals or groups interact with an AI system in a specific context. End
users can range in competency from AI experts to first-time technology end users.
Affected individuals/communities encompass all individuals, groups, communities, or
organizations directly or indirectly affected by AI systems or decisions based on the output
of AI systems. These individuals do not necessarily interact with the deployed system or
application.
Other AI actors may provide formal or quasi-formal norms or guidance for specifying
and managing AI risks. They can include trade associations, standards developing or-
ganizations, advocacy groups, researchers, environmental groups, and civil society
organizations.
The general public is most likely to directly experience positive and negative impacts of
AI technologies. They may provide the motivation for actions taken by the AI actors. This
group can include individuals, communities, and consumers associated with the context in
which an AI system is developed or deployed.
Page 37
NIST AI 100-1 AI RMF 1.0
Appendix B:
How AI Risks Differ from Traditional Software Risks
As with traditional software, risks from AI-based technology can be bigger than an en-
terprise, span organizations, and lead to societal impacts. AI systems also bring a set of
risks that are not comprehensively addressed by current risk frameworks and approaches.
Some AI system features that present risks also can be beneficial. For example, pre-trained
models and transfer learning can advance research and increase accuracy and resilience
when compared to other models and approaches. Identifying contextual factors in theMAP
function will assist AI actors in determining the level of risk and potential management
efforts.
Compared to traditional software, AI-specific risks that are new or increased include the
following:
• The data used for building an AI system may not be a true or appropriate representa-
tion of the context or intended use of the AI system, and the ground truth may either
not exist or not be available. Additionally, harmful bias and other data quality issues
can affect AI system trustworthiness, which could lead to negative impacts.
• AI system dependency and reliance on data for training tasks, combined with in-
creased volume and complexity typically associated with such data.
• Intentional or unintentional changes during training may fundamentally alter AI sys-
tem performance.
• Datasets used to train AI systems may become detached from their original and in-
tended context or may become stale or outdated relative to deployment context.
• AI system scale and complexity (many systems contain billions or even trillions of
decision points) housed within more traditional software applications.
• Use of pre-trained models that can advance research and improve performance can
also increase levels of statistical uncertainty and cause issues with bias management,
scientific validity, and reproducibility.
• Higher degree of difficulty in predicting failure modes for emergent properties of
large-scale pre-trained models.
• Privacy risk due to enhanced data aggregation capability for AI systems.
• AI systems may require more frequent maintenance and triggers for conducting cor-
rective maintenance due to data, model, or concept drift.
• Increased opacity and concerns about reproducibility.
• Underdeveloped software testing standards and inability to document AI-based prac-
tices to the standard expected of traditionally engineered software for all but the
simplest of cases.
• Difficulty in performing regular AI-based software testing, or determining what to
test, since AI systems are not subject to the same controls as traditional code devel-
opment.
Page 38
NIST AI 100-1 AI RMF 1.0
• Computational costs for developing AI systems and their impact on the environment
and planet.
• Inability to predict or detect the side effects of AI-based systems beyond statistical
measures.
Privacy and cybersecurity risk management considerations and approaches are applicable
in the design, development, deployment, evaluation, and use of AI systems. Privacy and
cybersecurity risks are also considered as part of broader enterprise risk management con-
siderations, which may incorporate AI risks. As part of the effort to address AI trustworthi-
ness characteristics such as “Secure and Resilient” and “Privacy-Enhanced,” organizations
may consider leveraging available standards and guidance that provide broad guidance to
organizations to reduce security and privacy risks, such as, but not limited to, the NIST Cy-
bersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Frame-
work, and the Secure Software Development Framework. These frameworks have some
features in common with the AI RMF. Like most risk management approaches, they are
outcome-based rather than prescriptive and are often structured around a Core set of func-
tions, categories, and subcategories. While there are significant differences between these
frameworks based on the domain addressed – and because AI risk management calls for
addressing many other types of risks – frameworks like those mentioned above may inform
security and privacy considerations in the MAP , MEASURE , and MANAGE functions of the
AI RMF.
At the same time, guidance available before publication of this AI RMF does not compre-
hensively address many AI system risks. For example, existing frameworks and guidance
are unable to:
• adequately manage the problem of harmful bias in AI systems;
• confront the challenging risks related to generative AI;
• comprehensively address security concerns related to evasion, model extraction, mem-
bership inference, availability, or other machine learning attacks;
• account for the complex attack surface of AI systems or other security abuses enabled
by AI systems; and
• consider risks associated with third-party AI technologies, transfer learning, and off-
label use where AI systems may be trained for decision-making outside an organiza-
tion’s security controls or trained in one domain and then “fine-tuned” for another.
Both AI and traditional software technologies and systems are subject to rapid innovation.
Technology advances should be monitored and deployed to take advantage of those devel-
opments and work towards a future of AI that is both trustworthy and responsible.
Page 39
NIST AI 100-1 AI RMF 1.0
Appendix C:
AI Risk Management and Human-AI Interaction
Organizations that design, develop, or deploy AI systems for use in operational settings
may enhance their AI risk management by understanding current limitations of human-
AI interaction. The AI RMF provides opportunities to clearly define and differentiate the
various human roles and responsibilities when using, interacting with, or managing AI
systems.
Many of the data-driven approaches that AI systems rely on attempt to convert or represent
individual and social observational and decision-making practices into measurable quanti-
ties. Representing complex human phenomena with mathematical models can come at the
cost of removing necessary context. This loss of context may in turn make it difficult to
understand individual and societal impacts that are key to AI risk management efforts.
Issues that merit further consideration and research include:
1. Human roles and responsibilities in decision making and overseeing AI systems
need to be clearly defined and differentiated. Human-AI configurations can span
from fully autonomous to fully manual. AI systems can autonomously make deci-
sions, defer decision making to a human expert, or be used by a human decision
maker as an additional opinion. Some AI systems may not require human oversight,
such as models used to improve video compression. Other systems may specifically
require human oversight.
2. Decisions that go into the design, development, deployment, evaluation, and use
of AI systems reflect systemic and human cognitive biases. AI actors bring their
cognitive biases, both individual and group, into the process. Biases can stem from
end-user decision-making tasks and be introduced across the AI lifecycle via human
assumptions, expectations, and decisions during design and modeling tasks. These
biases, which are not necessarily always harmful, may be exacerbated by AI system
opacity and the resulting lack of transparency. Systemic biases at the organizational
level can influence how teams are structured and who controls the decision-making
processes throughout the AI lifecycle. These biases can also influence downstream
decisions by end users, decision makers, and policy makers and may lead to negative
impacts.
3. Human-AI interaction results vary. Under certain conditions – for example, in
perceptual-based judgment tasks – the AI part of the human-AI interaction can am-
plify human biases, leading to more biased decisions than the AI or human alone.
When these variations are judiciously taken into account in organizing human-AI
teams, however, they can result in complementarity and improved overall perfor-
mance.
Page 40
NIST AI 100-1 AI RMF 1.0
4. Presenting AI system information to humans is complex. Humans perceive and
derive meaning from AI system output and explanations in different ways, reflecting
different individual preferences, traits, and skills.
The GOVERN function provides organizations with the opportunity to clarify and define
the roles and responsibilities for the humans in the Human-AI team configurations and
those who are overseeing the AI system performance. The GOVERN function also creates
mechanisms for organizations to make their decision-making processes more explicit, to
help counter systemic biases.
The MAP function suggests opportunities to define and document processes for operator
and practitioner proficiency with AI system performance and trustworthiness concepts, and
to define relevant technical standards and certifications. Implementing MAP function cat-
egories and subcategories may help organizations improve their internal competency for
analyzing context, identifying procedural and system limitations, exploring and examining
impacts of AI-based systems in the real world, and evaluating decision-making processes
throughout the AI lifecycle.
The GOVERN and MAP functions describe the importance of interdisciplinarity and demo-
graphically diverse teams and utilizing feedback from potentially impacted individuals and
communities. AI actors called out in the AI RMF who perform human factors tasks and
activities can assist technical teams by anchoring in design and development practices to
user intentions and representatives of the broader AI community, and societal values. These
actors further help to incorporate context-specific norms and values in system design and
evaluate end user experiences – in conjunction with AI systems.
AI risk management approaches for human-AI configurations will be augmented by on-
going research and evaluation. For example, the degree to which humans are empowered
and incentivized to challenge AI system output requires further studies. Data about the fre-
quency and rationale with which humans overrule AI system output in deployed systems
may be useful to collect and analyze.
Page 41
NIST AI 100-1 AI RMF 1.0
Appendix D:
Attributes of the AI RMF
NIST described several key attributes of the AI RMF when work on the Framework first
began. These attributes have remained intact and were used to guide the AI RMF’s devel-
opment. They are provided here as a reference.
The AI RMF strives to:
1. Be risk-based, resource-efficient, pro-innovation, and voluntary.
2. Be consensus-driven and developed and regularly updated through an open, trans-
parent process. All stakeholders should have the opportunity to contribute to the AI
RMF’s development.
3. Use clear and plain language that is understandable by a broad audience, including
senior executives, government officials, non-governmental organization leadership,
and those who are not AI professionals – while still of sufficient technical depth to
be useful to practitioners. The AI RMF should allow for communication of AI risks
across an organization, between organizations, with customers, and to the public at
large.
4. Provide common language and understanding to manage AI risks. The AI RMF
should offer taxonomy, terminology, definitions, metrics, and characterizations for
AI risk.
5. Be easily usable and fit well with other aspects of risk management. Use of the
Framework should be intuitive and readily adaptable as part of an organization’s
broader risk management strategy and processes. It should be consistent or aligned
with other approaches to managing AI risks.
6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI
RMF should be universally applicable to any AI technology and to context-specific
use cases.
7. Be outcome-focused and non-prescriptive. The Framework should provide a catalog
of outcomes and approaches rather than prescribe one-size-fits-all requirements.
8. Take advantage of and foster greater awareness of existing standards, guidelines, best
practices, methodologies, and tools for managing AI risks – as well as illustrate the
need for additional, improved resources.
9. Be law- and regulation-agnostic. The Framework should support organizations’
abilities to operate under applicable domestic and international legal or regulatory
regimes.
10. Be a living document. The AI RMF should be readily updated as technology, under-
standing, and approaches to AI trustworthiness and uses of AI change and as stake-
holders learn from implementing AI risk management generally and this framework
in particular.
Page 42
This publication is available free of charge from:
https://doi.org/10.6028/NIST.AI.100-1

",0.00,2025-06-11 04:20:23.497724,2025-06-14 02:22:15.977106,Framework,https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf,,NIST,2023-01-01,This framework establishes a framework for emerging technology governance governance and implementation. It provides national guidance for National Institute of Standards and Technology Laurie E and related organizations.,34,,92,,US,0.38095238095238093,Pattern matching found 4 indicators,AI,True,Valid,2025-06-14 03:38:53.444850,
27,NASA's Responsible AI Plan,"NASA’s Response Plan: Executive Order 13960 - Promoting the Use of Trustworthy Artificial Intelligence (AI) in the Federal Government Foreword Artificial Intelligence (AI) is an integral part of today’s process of conducting science and technology development. At NASA, AI has become an integral and ...","  
 
  

NASA’s Response Plan:  Executive Order 13960 - Promoting the Use of Trustworthy 
Artificial Intelligence (AI) in the Federal Government 
 
Foreword  
Artificial Intelligence (AI) is an integral part of today’s process of conducting science and technology 
development. At NASA, AI has become an integral and important tool for researchers, engineers, data 
scientists, and technologists in pursuing the ground-breaking discoveries that we are known for, including 
the command and controlling of our spacecraft and other supporting infrastructures.  Consequently, 
research and engineering efforts incorporating AI have permeated almost every area of our work . It is 
contributing to NASA’s drive toward the future, not just of space science, but for society here at home. 
We are dedicated to continuing the use of AI in a safe and fully transparent approach so that the public 
can have high confidence in the outcomes and b enefits. We believe that the plan outlined here will be 
responsive and contribute to the call for openness across the federal government. 
 
 
Executive Summary 
NASA is committed to responsible use of AI in all of its activities and in all phases of developme nt and 
deployment of its space and terrestrial programs missions. NASA does not deliberately focus on “ AI 
Research” as a separate field (we have no single “AI office” or “AI program”), rather NASA uses AI to build 
tools for its programs. This plan, being put forward, adheres to the Responsible AI (RAI) principles set and 
laid down by the White House in its Presidential Executive Order 13960. Our research, engineering and 
technical communities have been made aware of these guidelines and we are committed to an on-going 
process of educating and monitoring its implementation to ensure adherence to those principles. The vast 
majority of NASA’s use cases, which number almost 75 today, are geared toward analyzing the petabytes 
of data that NASA collects from its fleet of spacecraft across all disciplines, in human space exploration, 
and in aeronautics, etc.  
 
  
Overview 
This plan outlines how NASA intends to implement the Executive Order 13960. This plan will guide NASA 
and all its Centers.  
According to the President’s Executive Order 13960 -  Promoting the Use of Trustworthy Artificial 
Intelligence (AI) in the Federal Go vernment promises to drive the growth of the United States economy 
and improve the quality of life of all Americans. The order further expects that, in alignment with Executive 
Order 13859 of February 11, 2019 -  Maintaining American Leadership in Artificial Intelligence, executive 
departments and agencies have recognized the power of AI to improve their operations, processes, and 
procedures; meet strategic goals; reduce costs; enhance oversight of the use of taxpayer funds; increase 
efficiency and mission effectiveness; improve quality of services; improve safety; train workforces; and 
support decision making by the Federal workforce, among other positive developments. Given the broad 
applicability of AI, nearly every agency and those served by those agencies can benefit from the safe and 
appropriate use of AI. 
For the purposes of this plan, NASA uses the definition of Responsible AI from EO 13960. Namely, the 
term ‘‘artificial intelligence’’ includes the following:  
(1) Any artificial system that performs tasks under varying and unpredictable circumstances w ithout 
significant human oversight, or that can learn from experience and improve performance when exposed to 
data sets;  
(2) An artificial system developed in computer software, physical hardware, or other context that solves 
tasks requiring human-like perception, cognition, planning, learning, communication, or physical action ;  
(3) An artificial system designed to think or act like a human, including cognitive architectures and neural 
networks; 
(4) A set of techniques, including machine learning, that is designed to approximate a cognitive task; and 
(5) An artificial system designed to act rationally, including an intelligent software agent or embodied robot 
that achieves goals using perception, planning, reasoning, learning, communicating, decision makin g, and 
acting. 
 
Dr. Bhavya Lal, Associate Administrator for Office of Technology, Policy, and Strategy  (OTPS) and Dr. 
Katherine Calvin, NASA Chief Scientist are the Responsible AI Officials for the agency. 
 
Details of Plan 
The following steps outlines or p rovides high-level details on how NASA intends to execute its response 
to the Presidents Executive Order 13960 - Responsible Artificial Intelligence (AI).   This plan consists of 
NASA’s approach to developing an Inventory of AI Use Cases, Oversight applied to AI applications, 
Education of the researcher community, Transparency of the results, and Reporting.  
• Inventory of NASA’s AI Activities – AI Use Cases 
NASA will identify, capture, and document all relevant NASA AI use cases and release descriptions to the 
public. This will be accomplished by leveraging our AI Community of Practice (CoP) which has ~300+ 
members, our Autonomous Systems Software team (ASST), and Digital Transformation team (DT), Center 
representatives and through internal discussions with NASA Directorate leaders. We have reviewed the 
use of AI at a leadership retreat as well. AI researchers have or will report their activities to the RAI leaders 
for inclusion in our database of activities which will be maintained within the agency.  
As stated earlier, NASA does not deliberately focus on developing better AI techniques. NASA uses known 
AI techniques/applications to build tools for its programs and projects , for example, for managing 
spacecraft autonomy or airline ground operations, image processing, etc. This makes NASA more of a user 
of AI, not an “AI developer” per se.   
Examples of the types of work NASA carries out with AI are shown in the following table.  
Mission Operations and 
Planning 
Vehicle Navigation Data Communications 
Management 
Trust and 
Trustworthiness 
(Validation and 
Verification) 
Anomaly detection and 
Avoidance 
Human-System 
Interaction 
Crew and System 
Health Management 
Materials Discovery 
Hazard detection and 
Avoidance 
Weather Predictions Test and Evaluation In-Situ Resource 
Utilization 
 
 
• Oversight 
NASA will implement a tiered approach to focus more oversight on the more critical / matured AI projects, 
while providing initial guidance to immature projects in low -threat / early incubation stages.  NASA will 
sort out its AI use cases by the NASA Technology Readiness Levels (TRL)1 ranges:  
• TRL 1 - 4 (Early Incubation)  
• TRL 5 - 7 (Mid Maturity)  
• TRL 8 - 9 (High Maturity/Currently Deployed) 
The high maturity AI cases are subjected to more scrutiny (more frequent assessments) while the other 
cases receive less oversight. Assessments will most likely be conducted annually. Oversight will include a 
review by experts to confirm adherence to the nine EO AI principles  set forth by the White House. NASA 
will follow its established software / safety processes (in particular NPR 7150.2, NASA-STD-8739.82) in all 
AI applications. NASA’s NPR 7150.2, Software Engineering Requirements, includes detailed requirements 
for all software products. NASA STD 8739.8 - Software Assurance and Safety Standards, will also be applied 
per our usual processes. 
 
 
1 https://www.nasa.gov/directorates/heo/scan/engineering/technology/technology_readiness_level   
2 https://nodis3.gsfc.nasa.gov/displayDir.cfm?t=NPR&c=7150&s=2B, 
https://standards.nasa.gov/standard/NASA/NASA-STD-87398  
NASA is moving forward with a plan to produce a draft strategy and appropriate guidance for its 
researchers, with both a short -term and a long- term component. NASA intends to roll out this in the 
upcoming fiscal year.  
 
• Education 
NASA is already engaged in Ethical AI training and has conducted workshops on ethical AI in 2021. Ethical 
AI is a part of Responsible AI, and we will continue, and if needed, to expand these Ethical AI discussions.  
NASA will develop educational materials for the NASA community of researchers, engineers, technologist, 
etc. based around the nine pieces of responsible AI principles and practices.  NASA will o rganize 
workshop(s) on RAI within next 12 months. T raining will be arranged around existing scientific meetings 
where AI is presented, such as the AGU 2022 Fall meeting, where three workshops are being planned for 
AI practitioners .  If practical, NASA would like to partner with other government agencies to share 
educational materials, participate and co -host workshops, including the materials developed for the  
Transform to Open Science (TOPS) initiative3.  
• Transparency 
NASA will promote transparency of its RAI activities to the public by posting its RAI inventory on an Agency 
website. In addition, by promoting its RAI inventory, the agency will bring visibility to participants, users, 
innovator, researchers, etc.  for the work they’ve already done via status reporting to the White House 
EOP and to the NASA leadership. NASA plans to publicize use cases with high interest. 
Proper training of AI applications is crucial for transparency, including the selection of training data. Data 
Scientists and AI/ML practitioners at NASA actively employ the best practices of assessing all data for bias 
and mitigating as much bias as possible, and using separate data sets for training models, testing models, 
and validating models.  NASA will continue to encourage these practices, to include continuing education 
in data handling and including data handling checks / balances in relevant processes. 
 
• Reporting 
Each organization within NASA ( including those using AI techniques ) will follow its standard procedures 
and guidelines for allocating and tracking resources, including AI activities. This data will be available to 
the RAI Officials. 
A final report will be presented to the Administrator  and the entire agency leadership  on the Agency’s 
Responsible AI activities and plans for continuous oversight and improvements.  The public AI Use Case 
inventory will be updated annually.  
• Roles and Responsibilities 
The Responsible AI Officials at NASA carry out the following responsibilities: 
 
 
3 https://science.nasa.gov/open-science/transform-to-open-science  
• Respond to Executive Orders (EOs) regarding Responsible AI  
• Collect AI Use cases from across the Agency 
• Develop Responsible AI Policy and promote Responsible AI within NASA 
• Provide training on Responsible AI Use within NASA 
• Report as required on NASA’s Responsible AI activities 
• Respond to audits (e.g., GAO) 
 
• Deliverables and Timeline.  
The following is a notional time-line of activities to be carried out.  
• August 2022: Publicly posted AI inventory (removing non-releasable items).  
• August 2022: NASA’s Responsible AI Principles Application Plan.   
• TBD: Review Responsible AI Principles and determine how they apply to NASA’s AI portfolio. 
• October 15th, 2022: Triage NASA’s use cases and choose high-maturity projects for early focus and 
greater attention, le veraging existing processes as much as possible, and adding new AI ethics 
mechanisms only as-needed.  
• December 2022: Carry out workshops at the Fall AGU meeting. 
• December 2022: Investigate and initiate the d evelopment of an appropriate tool to assess use 
cases with respect to the Responsible AI principles.  
• FY2023: Develop a strategy for Responsible AI use.  
• December 2022/Annually: Educate the community via discussion sessions, FAQ, examples, etc. 
and encourage low and mid TRL Responsible AI principles as part of their profession and with their 
existing mechanisms. 
• Annually: Track progress and report to the Responsible AI Officers and other interested 
stakeholders.  
• 16 January 2023: RAI Principles Application Plan completion.  
 
Summary 
NASA has robust communities of practice for data science, engineering and system design, AI/ ML, and 
autonomous systems, all of which are actively employing AI in their solution space and identifying and 
promulgating emerging best practices.   These communit ies will support the guidance efforts of the 
Responsible AI Officials.   In addition to governance and guidance, including possibly an AI governance 
board, NASA is encouraging widespread AI workforce development, which will assist in embedding best 
practices into use of AI. 
NASA will continue to foster open community debate regarding AI ethics, which not only evolves best 
practices, but also encourages participants to employ AI ethics in their emerging work. 
AI is an exciting growth area with potential to amplify and impact nearly all aspects of NASA’s work. This 
growth process will be governed by existing policies and processes.  The unique aspects of AI, for example, 
possible autonomy and self-learning capabilities, make it a powerful tool.   NASA is carefully tracking the 
unique aspects of AI in our applications.  
NASA will continue to practice Responsible AI and apply the nine principles set down in the EO 13960. The 
agency has been engaged in ethical AI alr eady and will increase our transparency by making our AI Use 
Cases public and providing annual updates.  
  
 
",0.00,2025-06-11 17:04:12.285324,2025-06-14 02:23:18.757749,Policy,https://ntrs.nasa.gov/api/citations/20220013471/downloads/RAI%20Plan%20Sept%201%202022.pdf,,NIST,2022-01-01,This policy addresses emerging technology governance governance and implementation. It provides national guidance for NASA and related organizations.,30,,53,,US,0.9,Address pattern detected: \b\d{5}(-\d{4})?\b,AI,True,Valid,2025-06-14 03:39:11.885949,
30,NIST SP 800-218A Secure Software Development Practices for Generative AI and Dual-Use Foundation Models,NIST Special Publication 800 NIST SP 800-218A Secure Software Development Practices for Generative AI and Dual-Use Foundation Models An SSDF Community Profile Harold Booth Murugiah Souppaya Apostol Vassilev Michael Ogata Martin Stanley Karen Scarfone This publication is available free of charge from...," 
NIST Special Publication 800  
NIST SP 800-218A 
Secure Software Development Practices 
for Generative AI and Dual-Use 
Foundation Models 
An SSDF Community Profile 
Harold Booth 
Murugiah Souppaya 
Apostol Vassilev 
Michael Ogata 
Martin Stanley 
Karen Scarfone 
This publication is available free of charge from:  
https://doi.org/10.6028/NIST.SP.800-218A 
 

NIST Special Publication 800  
NIST SP 800-218A 
Secure Software Development Practices 
for Generative AI and Dual-Use 
Foundation Models 
An SSDF Community Profile 
 
Harold Booth 
Murugiah Souppaya 
Apostol Vassilev 
Computer Security Division 
Information Technology Laboratory 
 
Michael Ogata 
Applied Cybersecurity Division 
Information Technology Laboratory 
Martin Stanley 
Cybersecurity and Infrastructure Security 
Agency (CISA) 
 
 
 
Karen Scarfone 
Scarfone Cybersecurity 
 
 
This publication is available free of charge from:  
https://doi.org/10.6028/NIST.SP.800-218A 
July 2024 
 
 
U.S. Department of Commerce  
Gina M. Raimondo, Secretary 
National Institute of Standards and Technology  
Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology  

NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
Certain equipment, instruments, software, or materials, commercial or non-commercial, are identified in this 
paper in order to specify the experimental procedure adequately. Such identification does not imply 
recommendation or endorsement of any product or service by NIST, nor does it imply that the materials or 
equipment identified are necessarily the best available for the purpose .  
There may be references in this publication to other publications currently under development by NIST in 
accordance with its assigned statutory responsibilities. The information in this publication, including concepts and 
methodologies, may be used by federal agencies even before the completion of such companion publications. 
Thus, until each publication is completed, current requirements, guidelines, and procedures, where they exist, 
remain operative. For planning and transition purposes, federal agencies may wish to closely follow the 
development of these new publications by NIST.   
Organizations are encouraged to review all draft publications during public comment periods and provide feedback 
to NIST. Many NIST cybersecurity publications, other than the ones noted above, are available at 
https://csrc.nist.gov/publications
. 
Authority 
This publication has been developed by NIST in accordance with its statutory responsibilities under the Federal 
Information Security Modernization Act (FISMA) of 2014, 44 U.S.C. § 3551 et seq., Public Law (P.L.) 113- 283. NIST is 
responsible for developing information security standards and guidelines, including minimum requirements for 
federal information systems, but such standards and guidelines shall not apply to national security systems 
without the express approval of appropriate federal officials exercising policy authority over such systems. This 
guideline is consistent with the requirements of the Office of Management and Budget (OMB) Circular  A-130. 
 
Nothing in this publication should be taken to contradict the standards and guidelines made mandatory and 
binding on federal agencies by the Secretary of Commerce under statutory authority. Nor should these guidelines 
be interpreted as altering or superseding the existing authorities of the Secretary of Commerce, Director of the 
OMB, or any other federal official.  This publication may be used by nongovernmental organizations on a voluntary 
basis and is not subject to copyright in the United States. Attribution would, however, be appreciated by NIST.  
NIST Technical Series Policies 
Copyright, Use, and Licensing Statements
 
NIST Technical Series Publication Identifier Syntax 
Publication History 
Approved by the NIST Editorial Review Board on 2024-07-25 
How to Cite this NIST Technical Series Publication 
Booth H, Souppaya M, Vassilev A, Ogata M, Stanley M, Scarfone K (2024) Secure Development Practices for 
Generative AI and Dual-Use Foundation AI Models: An SSDF Community Profile. (National Institute of Standards 
and Technology, Gaithersburg, MD), NIST Special Publication (SP) NIST SP 800-218A. 
https://doi.org/10.6028/NIST.SP.800-218A   
Author ORCID iDs 
Harold Booth: 0000-0003-0373-6219 
Murugiah Souppaya: 0000-0002-8055-8527 
Apostol Vassilev: 0000-0002-9081-3042 
Michael Ogata: 0000-0002-8457-2430 
Karen Scarfone: 0000-0001-6334-9486 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
Contact Information 
ssdf@nist.gov  
 
National Institute of Standards and Technology 
Attn: Applied Cybersecurity Division, Information Technology Laboratory 
100 Bureau Drive (Mail Stop 2000) Gaithersburg, MD 20899-2000 
Additional Information 
Additional information about this publication is available at https://csrc.nist.gov/pubs/sp/800/218/a/final, 
including related content, potential updates, and document history.  
All comments are subject to release under the Freedom of Information Act (FOIA).  
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
i 
Abstract 
This document augments the secure software development practices and tasks defined in 
Secure Software Development Framework (SSDF) version 1.1 by adding practices, tasks, 
recommendations, considerations, notes, and informative references that are specific to AI 
model development throughout the software development life cycle. These additions are 
documented in the form of an SSDF Community Profile to support Executive Order (EO) 14110, 
Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, which tasked NIST 
with “developing a companion resource to the [SSDF] to incorporate secure development 
practices for generative AI and for dual-use foundation models.” This Community Profile is 
intended to be useful to the producers of AI models, the producers of AI systems that use those 
models, and the acquirers of those AI systems. This Profile should be used in conjunction with 
NIST Special Publication (SP) 800-218, Secure Software Development Framework (SSDF) Version 
1.1: Recommendations for Mitigating the Risk of Software Vulnerabilities. 
Keywords 
artificial intelligence; artificial intelligence model; cybersecurity risk management; generative 
artificial intelligence; secure software development; Secure Software Development Framework 
(SSDF); software acquisition; software development; software security. 
Reports on Computer Systems Technology 
The Information Technology Laboratory (ITL) at the National Institute of Standards and 
Technology (NIST) promotes the U.S. economy and public welfare by providing technical 
leadership for the Nation’s measurement and standards infrastructure. ITL develops tests, test 
methods, reference data, proof of concept implementations, and technical analyses to advance 
the development and productive use of information technology. ITL’s responsibilities include 
the development of management, administrative, technical, and physical standards and 
guidelines for the cost-effective security and privacy of other than national security-related 
information in federal information systems. The Special Publication 800-series reports on ITL’s 
research, guidelines, and outreach efforts in information system security, and its collaborative 
activities with industry, government, and academic organizations. 
  
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
ii 
Audience 
There are three primary audiences for this document: 
• AI model producers — Organizations that are developing their own generative AI and 
dual-use foundation models, as defined in EO 14110 
• AI system producers — Organizations that are developing software that leverages a 
generative AI or dual-use foundation model 
• AI system acquirers1 — Organizations that are acquiring a product or service that utilizes 
one or more AI systems 
Individuals who are interested in better understanding secure software development practices 
for AI models may also benefit from this document. 
Readers are not expected to be experts in secure software development or AI model 
development, but such expertise may be needed to implement these recommended practices. 
Note to Readers 
If you are from a standards developing organization (SDO) or another organization that is 
defining a set of secure practices for AI model development and you would like to map your 
standard or guidance to the SSDF profile, please contact the authors at ssdf@nist.gov
. They will 
introduce you to the National Online Informative References Program (OLIR), where you can 
submit your mapping to augment the existing set of informative references.  
The authors also welcome feedback at any time on any part of the document, as well as 
suggestions for Implementation Examples and Informative References to add to this document. 
All feedback should be sent to ssdf@nist.gov.  
Trademark Information 
All registered trademarks belong to their respective organizations. 
Acknowledgments 
The authors thank all of the organizations and individuals who provided numerous public 
comments and other thoughtful input for this publication. In response to Executive Order (EO) 
14110, Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, NIST held a 
January 2024 workshop, where speakers and attendees shared suggestions for adapting secure 
software development practices and tasks to accommodate the unique aspects of AI model 
development and the software that leverages them. The authors also thank all of their NIST 
colleagues and external experts who provided suggestions and feedback that helped shape this 
publication. 
  
 
1 The terms “producer” and “acquirer” were selected for consistency with the Audience statement in NIST SP 800-218. 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
iii 
Patent Disclosure Notice 
NOTICE: ITL has requested that holders of patent claims whose use may be required for 
compliance with the guidance or requirements of this publication disclose such patent claims to 
ITL. However, holders of patents are not obligated to respond to ITL calls for patents and ITL has 
not undertaken a patent search in order to identify which, if any, patents may apply to this 
publication. 
As of the date of publication and following call(s) for the identification of patent claims whose 
use may be required for compliance with the guidance or requirements of this publication, no 
such patent claims have been identified to ITL.  
No representation is made or implied by ITL that licenses are not required to avoid patent 
infringement in the use of this publication.  
 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
iv 
Table of Contents 
1. Introduction ...................................................................................................................................1  
2. Using the SSDF Community Profile ..................................................................................................4  
3. SSDF Community Profile for AI Model Development .......................................................................6  
References ....................................................................................................................................... 20 
Appendix A. Glossary ....................................................................................................................... 22 
List of Tables 
Table 1. SSDF Community Profile for AI Model Development ..............................................................8  
 
 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
1 
1. Introduction 
Section 4.1.a of Executive Order (EO) 14110, Safe, Secure, and Trustworthy Development and 
Use of Artificial Intelligence [1], tasked NIST with “developing a companion resource to the 
Secure Software Development Framework to incorporate secure development practices for 
generative AI and for dual-use foundation models.” This document is that companion resource. 
The software development and use of AI models and AI systems inherit much of the same risk 
as any other digital system. A unique challenge for this community is the blurring of traditional 
boundaries between system code and system data, as well as the use of plain human language 
as the means of interaction with the systems. AI models and systems, their configuration 
parameters (e.g., model weights), and the data they interact with (e.g., training data, user 
queries, etc.) can form closed loops that can be manipulated for unintended functionality.  
AI model and system development is still much more of an art than an exact science, requiring 
developers to interact with model code, training data, and other parameters over multiple 
iterations. Training datasets may be acquired from unknown, untrusted sources. Model weights 
and other training parameters can be susceptible to malicious tampering. Some models may be 
complex to the point that they cannot easily be thoroughly inspected, potentially allowing for 
undetectable execution of arbitrary code. User queries can be crafted to produce undesirable 
or objectionable output and — if not sanitized properly — can be leveraged for injection-style 
attacks. The goal of this document is to identify the practices and tasks needed to address these 
novel risks.   
 Purpose 
The SSDF provides a common language for describing secure software development practices 
throughout the software development life cycle. This document augments the practices and 
tasks defined in SSDF version 1.1 by adding recommendations, considerations, notes, and 
informative references that are specific to generative AI and dual-use foundation model 
development. These additions are documented in the form of an SSDF Community Profile 
(“Profile”), which is a baseline of SSDF practices and tasks that have been enhanced to address 
a particular use case. An example of an addition is, “Secure code storage should include AI 
models, model weights, pipelines, reward models, and any other AI model elements that need 
their confidentiality, integrity, and/or availability protected.”  
This Profile supplements what SSDF version 1.1 already includes. The Profile is intended to be 
used in conjunction with NIST Special Publication (SP) 800-218, Secure Software Development 
Framework (SSDF) Version 1.1: Recommendations for Mitigating the Risk of Software 
Vulnerabilities [6] and should not be used without SP 800-218. Readers should also utilize the 
implementation examples and informative references defined in SP 800-218 for additional 
information on how to perform each SSDF practice and task for all types of software 
development, as they are also generally applicable to AI model and AI system development.  
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
2 
 Scope 
This Profile’s scope is AI model development, which includes data sourcing for, designing, 
training, fine-tuning, and evaluating AI models, as well as incorporating and integrating AI 
models into other software. Consistent with SSDF version 1.1 and EO 14110, practices for the 
deployment and operation of AI systems with AI models are out of scope. Similarly, while 
cybersecurity practices for training data and other forms of data being used for AI model 
development are in scope, the rest of the data governance and management life cycle is out of 
scope. 
Practices and tasks in this Profile do not distinguish between human-written and AI-generated 
source code, because it is assumed that all source code should be evaluated for vulnerabilities 
and other issues before use. 
 Sources of Expertise 
This document leverages and integrates numerous sources of expertise, including: 
• NIST research and publications on trustworthy and responsible AI, including the Artificial 
Intelligence Risk Management Framework (AI RMF 1.0) [2], Adversarial Machine 
Learning: A Taxonomy and Terminology of Attacks and Mitigations [3], Towards a 
Standard for Identifying and Managing Bias in Artificial Intelligence [4], and the Dioptra 
experimentation testbed for security evaluations of machine learning algorithms [5].  
• NIST’s Secure Software Development Framework (SSDF) Version 1.1 [6], which is a set of 
fundamental, sound, and secure software development practices. It provides a common 
language to help facilitate communications among stakeholders, including software 
producers and software acquirers. The SSDF has also been used in support of EO 14028, 
Improving the Nation’s Cybersecurity [7], to enhance software supply chain security. 
• NIST general cybersecurity resources, including The NIST Cybersecurity Framework (CSF) 
2.0 [8], Security and Privacy Controls for Information Systems and Organizations [9], and 
Cybersecurity Supply Chain Risk Management Practices for Systems and Organizations 
[10]. 
• AI model developers, AI researchers, AI system developers, and secure software 
practitioners from industry and government with expertise in the unique security 
challenges of AI models and the practices for addressing those challenges. This expertise 
was primarily captured through NIST’s January 2024 workshop
, where speakers and 
attendees shared suggestions for adapting secure software development practices and 
tasks to accommodate the unique aspects of AI model development and the software 
leveraging them. 
 Document Structure 
This document is structured as follows: 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
3 
• Section 2 provides additional background on the SSDF and explains what an SSDF 
Community Profile is and how it can be used. 
• Section 3 defines the SSDF Community Profile for AI Model Development. 
• The References section lists all references cited in this document. 
• Appendix A provides a glossary of selected terms used within this document.  
 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
4 
2. Using the SSDF Community Profile 
AI model producers, AI system producers, AI system acquirers, and others can use the SSDF to 
foster their communications regarding secure AI model development throughout the software 
development life cycle.2 Following SSDF practices should help AI model producers reduce the 
number of vulnerabilities in their AI models, reduce the potential impacts of the exploitation of 
undetected or unaddressed vulnerabilities, and address the root causes of vulnerabilities to 
prevent recurrences. AI system producers can use the SSDF’s common vocabulary when 
communicating with AI model producers regarding their security practices for AI model 
development and when integrating AI models into the software they are developing. AI system 
acquirers can also use SSDF terms to better communicate their cybersecurity requirements and 
needs to AI model producers and AI system producers, such as during acquisition processes. 
The SSDF Community Profile is not a checklist to follow, but rather a starting point for planning 
and implementing a risk-based approach to adopting secure software development practices 
involving AI models. The contents of the Profile are meant to be adapted and customized, as 
not all practices and tasks are applicable to all use cases. Organizations should adopt a risk-
based approach to determine what practices and tasks are relevant, appropriate, and effective 
to mitigate the threats to software development practices from the organization’s perspective 
as an AI model producer, AI system producer, or AI system acquirer. Factors such as risk, cost, 
feasibility, and applicability should be considered when deciding which practices and tasks to 
use and how much time and resources to devote to each one. Cost models may need to be 
updated to effectively consider the costs inherent to AI model development. A risk-based 
approach to secure software development may change over time as an organization responds 
to new or elevated capabilities and risks associated with an AI model or system. 
Generative AI and dual-use foundation models present additional challenges in tracking model 
versioning and lineage. Source code for defining the model architecture and building model 
binaries is amenable to secure software engineering practices for versioning, lineage, and 
reproducibility. However, the final model weights are defined only after the model is trained 
and fine-tuned; this is where limitations in tracking all aspects of collection, processing, and 
training arise. Organizations should follow secure software development practices for the parts 
of a model that can be covered fully and strive to introduce secure practices to the extent 
possible for the stages and corresponding artifacts where obtaining such security guarantees is 
hard to achieve. Organizations should document the parts and artifacts that are not covered by 
the secure software development practices. 
The Profile’s practices, tasks, recommendations, and considerations can be integrated into 
machine learning operations (MLOps) along with other software assets within a continuous 
integration/continuous delivery (CI/CD) pipeline.  
The responsibility for implementing SSDF practices in the Profile may be shared among multiple 
organizations. For example, an AI model could be produced by one organization and executed 
within an AI system hosted by a second organization, which is then used by other organizations. 
 
2 For consistency with SSDF 1.1, this document uses a general software development life cycle. Organizations using this document are 
encouraged to adapt it to any machine learning-specific life cycle they are using. 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
5 
In these situations, there is likely a shared responsibility model involving the AI model producer, 
AI system producer, and AI system acquirer. An AI system acquirer can establish an agreement 
with an AI system producer and/or AI model producer that specifies which party is responsible 
for each practice and task and how each party will attest to its conformance with the 
agreement. 
A limitation of the SSDF and this Profile is that they only address cybersecurity risk 
management. There are many other types of risks to AI systems (e.g., data privacy, intellectual 
property, and bias) that organizations should manage along with cybersecurity risk as part of a 
mature enterprise risk management program. NIST resources on identifying and managing 
other types of risk include:  
• AI Risk Management Framework (AI RMF) [2] and the NIST AI RMF Playbook [11]  
• Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations 
[3] 
• Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence 
Profile [12] 
• Towards a Standard for Identifying and Managing Bias in Artificial Intelligence [4] 
• Cybersecurity Supply Chain Risk Management Practices for Systems and Organizations 
[10] 
• NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk 
Management, Version 1.0 [13] 
• Integrating Cybersecurity and Enterprise Risk Management (ERM) [14] 
 
 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
6 
3. SSDF Community Profile for AI Model Development 
Table 1 defines the SSDF Community Profile for AI Model Development. The meanings of each 
column are as follows: 
• Practice contains the name of the practice and a unique identifier, followed by a brief 
explanation of what the practice is and why it is beneficial. 
Task specifies one or more actions that may be needed to perform a practice. Each task 
includes a unique identifier and a brief explanation. 
All practices and tasks are unchanged from SSDF version 1.1 unless they are explicitly 
tagged as “Modified from SSDF 1.1” or “Not part of SSDF 1.1.” An example is the PW.3 
practice, “Confirm the Integrity of Training, Testing, Fine-Tuning, and Aligning Data 
Before Use” and all of its tasks. 
• Priority reflects the suggested relative importance of each task within the context of the 
profile and is intended to be a starting point for organizations to assign their own 
priorities: 
o High: Critically important for AI model development security compared to other 
tasks 
o Medium: Directly supports AI model development security  
o Low: Beneficial for secure software development but is generally not more 
important than most other tasks 
• Recommendations, Considerations, and Notes Specific to AI Model Development may 
contain one or more items that recommend what to do or describe additional 
considerations for a particular task. Organizations are expected to adapt, customize, and 
omit items as necessary as part of the risk-based approach described in Section 2.  
Each item has an ID starting with one of the following:  
o “R” (recommendation: something the organization should do) 
o “C” (consideration: something the organization should consider doing) 
o “N” (note: additional information besides recommendations and considerations)  
An R, C, or N designation and its number can be appended to the task ID to create a 
unique identifier (e.g., “PO.1.2.R1” is the first recommendation for task PO.1.2).  
Note that a value of “No additions to SSDF 1.1” in this column indicates that the Profile 
does not contain recommendations, considerations, or notes specific to AI model 
development for the task. Refer to SSDF version 1.1 [6] for baseline guidance on the 
secure development task in question and to the other references in this document for 
additional information related to the task. 
• Informative References point (map) to parts of standards, guidance, and other content 
containing requirements, recommendations, considerations, or other supporting 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
7 
information on performing a particular task. The Informative References come from the 
following sources: 
o AI Risk Management Framework 1.0 [2]. Several crosswalks have already been 
defined between the AI RMF and other guidance and standards; see 
https://airc.nist.gov/AI_RMF_Knowledge_Base/Crosswalks for the current set. 
o OWASP Top 10 for LLM Applications Version 1.1 [15]. Each identifier indicates 
one of the top 10 vulnerability types and might also refer to an individual 
prevention and mitigation strategy for that vulnerability type. 
o Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and 
Mitigations [3]. This report outlines key types of machine learning attack stages 
and attacker goals, objectives, and capabilities, as well as corresponding 
methods for mitigating and managing the consequences of attacks. 
NIST is also considering adding a column for Implementation Examples in a future version of the 
Profile. An Implementation Example is a single sentence that suggests a way to accomplish part 
or all of a task. While the Recommendations and Considerations column describes the “what,” 
Implementation Examples would describe options for the “how.” Such examples added to this 
Profile would supplement those already defined in SSDF version 1.1. See the Note to Readers
 
for more information on providing input on additional Informative References and 
Implementation Examples. 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
8 
Note: This Profile supplements what SSDF version 1.1 [6] already includes and is intended to be used in conjunction with it, not on its own. As a 
reminder, the deployment and operation of AI systems with AI models are out of the Profile’s scope, as are most parts of the data governance and 
management life cycle. 
There are gaps in the numbering of some SSDF practices and tasks. For example, the PW.4 practice has three tasks: PW.4.1, PW.4.2, and PW.4.4. 
PW.4.3 was a task in SSDF version 1.0 that was moved elsewhere for version 1.1, so its ID was not reused. 
Table 1. SSDF Community Profile for AI Model Development 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
Prepare the Organization (PO)     
Define Security Requirements for Software 
Development (PO.1): Ensure that security 
requirements for software development are 
known at all times so that they can be taken 
into account throughout the software 
development life cycle (SDLC) and duplication 
of effort can be minimized because the 
requirements information can be collected 
once and shared. This includes requirements 
from internal sources (e.g., the organization’s 
policies, business objectives, and risk 
management strategy) and external sources 
(e.g., applicable laws and regulations). 
PO.1.1: Identify and document all security 
requirements for the organization’s 
software development infrastructures and 
processes, and maintain the requirements 
over time. 
High R1: Include AI model development in the 
security requirements for software 
development infrastructure and processes. 
R2: Identify and select appropriate AI model 
architectures and training techniques in 
accordance with recommended practices for 
cybersecurity, privacy, and reproducibility. 
AI RMF: Map 
1.3, 1.5, 1.6 
PO.1.2: Identify and document all security 
requirements for organization-developed 
software to meet, and maintain the 
requirements over time. 
High R1: Organizational policies should support all 
current requirements specific to AI model 
development security for organization-
developed software. These requirements 
should include the areas of AI model 
development, AI model operations, and data 
science. Requirements may come from many 
sources, including laws, regulations, contracts, 
and standards.  
C1: Consider reusing or expanding the 
organization’s existing data classification policy 
and processes. 
N1: Possible forms of AI model documentation 
include data, model, and system cards. 
AI RMF: 
Govern 1.1, 
1.2, 3.2, 4.1, 
5.1, 6.1; Map 
1.1 
PO.1.3: Communicate requirements to all 
third parties who will provide commercial 
software components to the organization 
for use by the organization’s own 
software. [Modified from SSDF 1.1] 
Medium R1: Include AI model development security in 
the requirements being communicated for 
third-party software components. 
AI RMF: Map 
4.1, 4.2 
OWASP: 
LLM05-1 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
9 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
Implement Roles and Responsibilities (PO.2): 
Ensure that everyone inside and outside of the 
organization involved in the SDLC is prepared 
to perform their SDLC-related roles and 
responsibilities throughout the SDLC. 
PO.2.1: Create new roles and alter 
responsibilities for existing roles as 
needed to encompass all parts of the 
SDLC. Periodically review and maintain the 
defined roles and responsibilities, 
updating them as needed. 
High R1: Include AI model development security in 
SDLC-related roles and responsibilities 
throughout the SDLC. The roles and 
responsibilities should include, but are not 
limited to, AI model development, AI model 
operations, and data science. 
N1: Roles and responsibilities involving AI 
system producers, AI model producers, and 
other third-party providers can be documented 
in agreements. 
AI RMF: 
Govern 2.1 
PO.2.2: Provide role-based training for all 
personnel with responsibilities that 
contribute to secure development. 
Periodically review personnel proficiency 
and role-based training, and update the 
training as needed. 
High R1: Role-based training should include 
understanding cybersecurity vulnerabilities and 
threats to AI models and their possible 
mitigations. 
AI RMF: 
Govern 2.2 
OWASP: 
LLM04-7 
PO.2.3: Obtain upper management or 
authorizing official commitment to secure 
development, and convey that 
commitment to all with development-
related roles and responsibilities. 
Medium R1: Leadership should commit to secure 
development practices involving AI models. 
AI RMF: 
Govern 2.3 
Implement Supporting Toolchains (PO.3): Use 
automation to reduce human effort and 
improve the accuracy, reproducibility, usability, 
and comprehensiveness of security practices 
throughout the SDLC, as well as provide a way 
to document and demonstrate the use of these 
practices. Toolchains and tools may be used at 
different levels of the organization, such as 
organization-wide or project-specific, and may 
address a particular part of the SDLC, like a 
build pipeline. 
PO.3.1: Specify which tools or tool types 
must or should be included in each 
toolchain to mitigate identified risks, as 
well as how the toolchain components are 
to be integrated with each other. 
High R1: Plan to develop and implement automated 
toolchains that secure AI model development 
and reduce human effort, especially at the scale 
often used by AI models. 
N1: Ideally, automated toolchains will perform 
the vast majority of the work related to 
securing AI model development. 
N2: See PO.4, PO.5, PS, and PW for information 
on tool types. 
AI RMF: 
Measure 2.1 
OWASP: LLM08 
PO.3.2: Follow recommended security 
practices to deploy, operate, and maintain 
tools and toolchains. 
High R1: Execute the plan to develop and implement 
automated toolchains that secure AI model 
development and reduce human effort, 
especially at the scale often used by AI models. 
R2: Verify the security of toolchains at a 
frequency commensurate with risk. 
AI RMF: 
Measure 2.1 
OWASP: 
LLM05-3, 
LLM05-9, 
LLM08, LLM09 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
10 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
PO.3.3: Configure tools to generate 
artifacts of their support of secure 
software development practices as 
defined by the organization. 
Medium N1: An artifact is “a piece of evidence” [16]. 
Evidence is “grounds for belief or disbelief; data 
on which to base proof or to establish truth or 
falsehood” [17]. Artifacts provide records of 
secure software development practices. 
Examples of artifacts specific to AI model 
development include attestations of the 
integrity and provenance of training datasets.  
AI RMF: 
Measure 2.1 
Define and Use Criteria for Software Security 
Checks (PO.4): Help ensure that the software 
resulting from the SDLC meets the 
organization’s expectations by defining and 
using criteria for checking the software’s 
security during development. 
PO.4.1: Define criteria for software 
security checks and track throughout the 
SDLC. 
Medium R1: Implement guardrails and other controls 
throughout the AI development life cycle, 
extending beyond the traditional SDLC. 
C1: Consider requiring review and approval 
from a human-in-the-loop for software security 
checks beyond risk-based thresholds. 
AI RMF: 
Measure 2.3, 
2.7; Manage 
1.1 
OWASP: 
LLM01-2 
PO.4.2: Implement processes, 
mechanisms, etc. to gather and safeguard 
the necessary information in support of 
the criteria. 
Low No additions to SSDF 1.1 AI RMF: 
Measure 2.3, 
2.7; Manage 
1.1 
OWASP: 
LLM01-2 
Implement and Maintain Secure 
Environments for Software Development 
(PO.5): Ensure that all components of the 
environments for software development are 
strongly protected from internal and external 
threats to prevent compromises of the 
environments or the software being developed 
or maintained within them. Examples of 
environments for software development 
include development, AI model training, build, 
test, and distribution environments. [Modified 
from SSDF 1.1] 
PO.5.1: Separate and protect each 
environment involved in software 
development. 
High C1: Consider separating execution 
environments from each other to the extent 
feasible, such as through isolation, 
segmentation, containment, access via APIs, or 
other means.  
R1: Monitor, track, and limit resource usage 
and rates for AI model users during model 
development. 
R2: Only store sensitive data used during AI 
model development, including production data, 
within organization-approved environments 
and locations within those environments. 
R3: Protect all training pipelines, model 
registries, and other components within the 
environments according to the principle of least 
privilege. 
OWASP: 
LLM01-1, 
LLM01-4, 
LLM04, LLM08, 
LLM10 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
11 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
R4: Continuously monitor training-related 
activity in pipelines and model modifications in 
the model registry. 
R5: Follow recommended practices for securely 
configuring each environment. 
R6: Continuously monitor each environment for 
plaintext secrets. 
PO.5.2: Secure and harden development 
endpoints (endpoints for software 
designers, developers, testers, builders, 
etc.) to perform development tasks using 
a risk-based approach. 
Medium No additions to SSDF 1.1 OWASP: 
LLM01-1, 
LLM05-3, 
LLM05-9, 
LLM08 
PO.5.3: Continuously monitor software 
execution performance and behavior in 
software development environments to 
identify potential suspicious activity and 
other issues. [Not part of SSDF 1.1] 
High R1: Perform continuous security monitoring for 
all development environment components that 
host an AI model or related resources (e.g., 
model APIs, weights, configuration parameters, 
training datasets).  
R2: Continuous monitoring and analysis tools 
should generate alerts when detected activity 
involving an AI model passes a risk threshold or 
otherwise merits additional investigation. 
AI RMF: 
Measure 2.4 
OWASP: 
LLM03-7, 
LLM04, LLM05-
8, LLM09, 
LLM10 
Protect Software (PS)     
Protect All Forms of Code and Data from 
Unauthorized Access and Tampering (PS.1): 
Help prevent unauthorized changes to code 
and data, both inadvertent and intentional, 
which could circumvent or negate the intended 
security characteristics of the software. For 
code and data that are not intended to be 
publicly accessible, this helps prevent theft of 
the software and may make it more difficult or 
time-consuming for attackers to find 
vulnerabilities in the software. [Modified from 
SSDF 1.1] 
PS.1.1: Store all forms of code – including 
source code, executable code, and 
configuration-as-code – based on the 
principle of least privilege so that only 
authorized personnel, tools, services, etc. 
have access. 
High R1: Secure code storage should include AI 
models, model weights, pipelines, reward 
models, and any other AI model elements that 
need their confidentiality, integrity, and/or 
availability protected. These elements do not all 
have to be stored in the same place or through 
the same type of mechanism. 
R2: Follow the principle of least privilege to 
minimize direct access to AI models and model 
elements regardless of where they are stored or 
executed.  
R3: Store reward models separately from AI 
models and data. 
OWASP: LLM10 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
12 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
R4: Permit indirect access only to model 
weights. 
C1: Consider preventing all human access to 
model weights.  
C2: Consider requiring all AI model 
development to be performed within 
organization-approved environments only. 
PS.1.2: Protect all training, testing, fine-
tuning, and aligning data from 
unauthorized access and modification. 
[Not part of SSDF 1.1] 
High R1: Continuously monitor the confidentiality 
(for non-public data only) and integrity of 
training, testing, fine-tuning, and aligning data.  
C1: Consider securely storing training, testing, 
fine-tuning, and aligning data for future use and 
reference if feasible. 
OWASP: 
LLM03, LLM06, 
LLM10 
PS.1.3: Protect all model weights and 
configuration parameter data from 
unauthorized access and modification. 
[Not part of SSDF 1.1] 
High R1: Keep model weights and configuration 
parameters separate from training, testing, 
fine-tuning, and aligning data.  
R2: Continuously monitor the confidentiality 
(for closed models only) and integrity of model 
weights and configuration parameters. 
R3: Follow the principle of least privilege to 
restrict access to AI model weights, 
configuration parameters, and services during 
development. 
R4: Specify and implement additional risk-
proportionate cybersecurity practices around 
model weights, such as encryption, 
cryptographic hashes, digital signatures, multi-
party authorization, and air-gapped 
environments. 
OWASP: LLM10 
Provide a Mechanism for Verifying Software 
Release Integrity (PS.2): Help software 
acquirers ensure that the software they 
acquire is legitimate and has not been 
tampered with. 
PS.2.1: Make software integrity 
verification information available to 
software acquirers. 
Medium R1: Generate and provide cryptographic hashes 
or digital signatures for an AI model and its 
components, artifacts, and documentation. 
R2: Provide digital signatures for AI model 
changes. 
OWASP: 
LLM05-6 
Archive and Protect Each Software Release 
(PS.3): Preserve software releases in order to 
PS.3.1: Securely archive the necessary files 
and supporting data (e.g., integrity 
Low R1: Perform versioning and tracking for 
infrastructure tools (e.g., pre-processing, 
OWASP: LLM10 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
13 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
help identify, analyze, and eliminate 
vulnerabilities discovered in the software after 
release. 
verification information, provenance data) 
to be retained for each software release. 
transforms, collection) that support dataset 
creation and model training. 
R2: Include documentation of the justification 
for AI model selection in the retained 
information. 
R3: Include documentation of the entire 
training process, such as data preprocessing 
and model architecture. 
N1: AI models and their components may need 
to be added at this time to an organization’s 
asset inventories. 
PS.3.2: Collect, safeguard, maintain, and 
share provenance data for all components 
of each software release (e.g., in a 
software bill of materials [SBOM], through 
Supply-chain Levels for Software Artifacts 
[SLSA]). [Modified from SSDF 1.1] 
Medium R1: Track the provenance of an AI model and its 
components and derivatives, including the 
training libraries, frameworks, and pipelines 
used to build the model. 
R2: Track AI models that were trained on 
sensitive data (e.g., payment card data, 
protected health information, other types of 
personally identifiable information), and 
determine if access to the models should be 
restricted to individuals who already have 
access to the sensitive data used for training. 
C1: Consider disclosing the provenance of the 
training, testing, fine-tuning, and aligning data 
used for an AI model. 
OWASP: 
LLM03-1, 
LLM05-4, 
LLM05-5, 
LLM10 
Produce Well-Secured Software (PW)     
Design Software to Meet Security 
Requirements and Mitigate Security Risks 
(PW.1): Identify and evaluate the security 
requirements for the software; determine 
what security risks the software is likely to face 
during operation and how the software’s 
design and architecture should mitigate those 
risks; and justify any cases where risk-based 
analysis indicates that security requirements 
should be relaxed or waived. Addressing 
PW.1.1: Use forms of risk modeling – such 
as threat modeling, attack modeling, or 
attack surface mapping – to help assess 
the security risk for the software. 
High R1: Incorporate relevant AI model-specific 
vulnerability and threat types in risk modeling. 
Examples of these vulnerability and threat types 
include poisoning of training data, malicious 
code or other unwanted content in inputs and 
outputs, denial-of-service conditions arising 
from adversarial prompts, supply chain attacks, 
unauthorized information disclosure, theft of AI 
model weights, and misconfiguration of data 
pipelines. [3] 
AI RMF: 
Govern 4.1, 
4.2; Map 5.1; 
Measure 1.1; 
Manage 1.2, 
1.3 
OWASP: 
LLM01, LLM02, 
LLM03, LLM04, 
LLM05, LLM06, 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
14 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
security requirements and risks during 
software design (secure by design) is key for 
improving software security and also helps 
improve development efficiency. 
C1: Consider periodic risk modeling updates for 
future AI model versions and derivatives after 
AI model release. 
C2: During risk modeling, consider checking that 
the AI model is not in a critical path to make 
significant security decisions without a human 
in the loop. 
LLM07, LLM08, 
LLM09, LLM10 
PW.1.2: Track and maintain the software’s 
security requirements, risks, and design 
decisions. 
Medium No additions to SSDF 1.1 AI RMF: 
Govern 4.1, 
4.2; Map 2.1, 
2.2, 2.3, 3.2, 
3.3, 4.1, 4.2, 
5.2; Manage 
1.2, 1.3, 1.4 
PW.1.3: Where appropriate, build in 
support for using standardized security 
features and services (e.g., enabling 
software to integrate with existing log 
management, identity management, 
access control, and vulnerability 
management systems) instead of creating 
proprietary implementations of security 
features and services. 
Medium No additions to SSDF 1.1  
Review the Software Design to Verify 
Compliance with Security Requirements and 
Risk Information (PW.2): Help ensure that the 
software will meet the security requirements 
and satisfactorily address the identified risk 
information. 
PW.2.1: Have 1) a qualified person (or 
people) who were not involved with the 
design and 2) automated processes 
instantiated in the toolchain review the 
software design to confirm and enforce 
that it meets all of the security 
requirements and satisfactorily addresses 
the identified risk information. [Modified 
from SSDF 1.1] 
High No additions to SSDF 1.1 AI RMF: 
Measure 2.7; 
Manage 1.1 
Confirm the Integrity of Training, Testing, 
Fine-Tuning, and Aligning Data Before Use 
(PW.3): Prevent data that is likely to negatively 
impact the cybersecurity of the AI model from 
PW.3.1: Analyze data for signs of data 
poisoning, bias, homogeneity, and 
tampering before using it for AI model 
training, testing, fine-tuning, or aligning 
High R1: Verify the provenance (when known) and 
integrity of training, testing, fine-tuning, and 
aligning data before use.  
AI RMF: 
Measure 2.1; 
Manage 1.2, 
1.3 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
15 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
being consumed as part of AI model training, 
testing, fine-tuning, and aligning. [Not part of 
SSDF 1.1] 
purposes, and mitigate the risks as 
necessary. [Not part of SSDF 1.1] 
R2: Select and apply appropriate methods for 
analyzing and altering the training, testing, fine-
tuning, and aligning data for an AI model. 
Examples of methods include anomaly 
detection, bias detection, data cleaning, data 
curation, data filtering, data sanitization, fact-
checking, and noise reduction.  
C1: Consider using a human-in-the-loop to 
examine data, such as with exploratory data 
analysis techniques [18]. 
OWASP: 
LLM03, LLM06 
PW.3.2: Track the provenance, when 
known, of all training, testing, fine-tuning, 
and aligning data used for an AI model, 
and document which data do not have 
known provenance. [Not part of SSDF 1.1] 
Medium N1: Provenance verification is not possible in all 
cases because provenance is not always known. 
However, it is still beneficial for security 
purposes to track and verify provenance 
whenever possible, and to track when 
provenance is unknown.  
AI RMF: 
Measure 2.1 
OWASP: 
LLM03-1 
Adv ML 
PW.3.3: Include adversarial samples in the 
training and testing data to improve 
attack prevention. [Not part of SSDF 1.1] 
Medium R1: Use a process and corresponding controls to 
test the adversarial samples and put 
appropriate guardrails on training and testing 
use. 
OWASP: 
LLM03-6, 
LLM05-7 
Adv ML 
Reuse Existing, Well-Secured Software When 
Feasible Instead of Duplicating Functionality 
(PW.4): Lower the costs of software 
development, expedite software development, 
and decrease the likelihood of introducing 
additional security vulnerabilities into the 
software by reusing software modules and 
services that have already had their security 
posture checked. This is particularly important 
for software that implements security 
functionality, such as cryptographic modules 
and protocols. 
PW.4.1: Acquire and maintain well-
secured software components (e.g., 
software libraries, modules, middleware, 
frameworks) from commercial, open-
source, and other third-party developers 
for use by the organization’s software. 
Medium C1: Consider using an existing AI model instead 
of creating a new one. 
OWASP: LLM05 
 
PW.4.2: Create and maintain well-secured 
software components in-house following 
SDLC processes to meet common internal 
software development needs that cannot 
be better met by third-party software 
components. 
Low No additions to SSDF 1.1  
PW.4.4: Verify that acquired commercial, 
open-source, and all other third-party 
software components comply with the 
High R1: Verify the integrity, provenance, and 
security of an existing AI model or any other 
acquired AI components — including training, 
testing, fine-tuning, and aligning datasets; 
OWASP: 
LLM05-2, 
LLM05-6 
Adv ML 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
16 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
requirements, as defined by the 
organization, throughout their life cycles. 
reward models; adaptation layers; and 
configuration parameters — before using them.  
R2: Scan and thoroughly test acquired AI 
models and their components for vulnerabilities 
and malicious content before use. 
Create Source Code by Adhering to Secure 
Coding Practices (PW.5): Decrease the number 
of security vulnerabilities in the software, and 
reduce costs by minimizing vulnerabilities 
introduced during source code creation that 
meet or exceed organization-defined 
vulnerability severity criteria. 
PW.5.1: Follow all secure coding practices 
that are appropriate to the development 
languages and environment to meet the 
organization’s requirements. 
High R1: Expand secure coding practices to include AI 
technology-specific considerations.  
R2: Code the handling of inputs (including 
prompts and user data) and outputs carefully. 
All inputs and outputs should be logged, 
analyzed, and validated within the context of 
the AI model, and those with issues should be 
sanitized or dropped.  
R3: Encode inputs and outputs to prevent the 
execution of unauthorized code. 
AI RMF: 
Manage 1.2, 
1.3, 1.4 
OWASP: 
LLM01, LLM02, 
LLM04-1, 
LLM06, LLM07, 
LLM09-9, 
LLM10 
Configure the Compilation, Interpreter, and 
Build Processes to Improve Executable 
Security (PW.6): Decrease the number of 
security vulnerabilities in the software and 
reduce costs by eliminating vulnerabilities 
before testing occurs. 
PW.6.1: Use compiler, interpreter, and 
build tools that offer features to improve 
executable security. 
Low C1: Consider using secure model serialization 
mechanisms that reduce or eliminate vectors 
for the introduction of malicious content. 
 
PW.6.2: Determine which compiler, 
interpreter, and build tool features should 
be used and how each should be 
configured, then implement and use the 
approved configurations. 
Low C1: Consider capturing compiler, interpreter, 
and build tool versions and features as part of 
the provenance tracking. 
 
Review and/or Analyze Human-Readable 
Code to Identify Vulnerabilities and Verify 
Compliance with Security Requirements 
(PW.7): Help identify vulnerabilities so that 
they can be corrected before the software is 
released to prevent exploitation. Using 
automated methods lowers the effort and 
resources needed to detect vulnerabilities. 
Human-readable code includes source code, 
scripts, and any other form of code that an 
organization deems human-readable. 
PW.7.1: Determine whether code review 
(a person looks directly at the code to find 
issues) and/or code analysis (tools are 
used to find issues in code, either in a fully 
automated way or in conjunction with a 
person) should be used, as defined by the 
organization. 
Medium R1: Code review and analysis policies or 
guidelines should include code for AI models 
and other related components. 
C1: Consider performing scans of AI model code 
in addition to testing the AI models. 
 
PW.7.2: Perform the code review and/or 
code analysis based on the organization’s 
secure coding standards, and record and 
triage all discovered issues and 
recommended remediations in the 
High R1: Scan all AI models for malware, 
vulnerabilities, backdoors, and other security 
issues in accordance with the organization’s 
code review and analysis policies or guidelines. 
AI RMF: 
Measure 2.3, 
2.7; Manage 
1.1, 1.2, 1.3, 
1.4 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
17 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
development team’s workflow or issue 
tracking system. 
OWASP: 
LLM03-7d, 
LLM07-4 
Test Executable Code to Identify 
Vulnerabilities and Verify Compliance with 
Security Requirements (PW.8): Help identify 
vulnerabilities so that they can be corrected 
before the software is released in order to 
prevent exploitation. Using automated 
methods lowers the effort and resources 
needed to detect vulnerabilities and improves 
traceability and repeatability. Executable code 
includes binaries, directly executed bytecode 
and source code, and any other form of code 
that an organization deems executable. 
PW.8.1: Determine whether executable 
code testing should be performed to find 
vulnerabilities not identified by previous 
reviews, analysis, or testing and, if so, 
which types of testing should be used. 
High R1: Include AI models in code testing policies 
and guidelines. Several forms of code testing 
can be used for AI models, including unit 
testing, integration testing, penetration testing, 
red teaming, use case testing, and adversarial 
testing. 
C1: Consider automating tests within a 
development pipeline as part of regression 
testing where possible. 
 
PW.8.2: Scope the testing, design the 
tests, perform the testing, and document 
the results, including recording and 
triaging all discovered issues and 
recommended remediations in the 
development team’s workflow or issue 
tracking system. 
High R1: Test all AI models for vulnerabilities in 
accordance with the organization’s code testing 
policies or guidelines. 
R2: Retest AI models when they are retrained or 
new data sources are added. 
AI RMF: 
Measure 2.2, 
2.3, 2.7; 
Manage 1.1, 
1.2, 1.3, 1.4 
OWASP: 
LLM03-7d, 
LLM05-7, 
LLM07-4 
Configure Software to Have Secure Settings by 
Default (PW.9): Help improve the security of 
the software at the time of installation to 
reduce the likelihood of the software being 
deployed with weak security settings, putting it 
at greater risk of compromise. 
PW.9.1: Define a secure baseline by 
determining how to configure each setting 
that has an effect on security or a security-
related setting so that the default settings 
are secure and do not weaken the security 
functions provided by the platform, 
network infrastructure, or services. 
Medium No additions to SSDF 1.1 AI RMF: 
Measure 2.7 
PW.9.2: Implement the default settings 
(or groups of default settings, if 
applicable), and document each setting 
for software administrators. 
Medium N1: Documenting settings can be performed 
earlier in the process, such as when defining a 
secure baseline (see PW.9.1). 
AI RMF: 
Measure 2.7; 
Manage 1.2, 
1.3, 1.4 
Respond to Vulnerabilities (RV)     
Identify and Confirm Vulnerabilities on an 
Ongoing Basis (RV.1): Help ensure that 
vulnerabilities are identified more quickly so 
RV.1.1: Gather information from software 
acquirers, users, and public sources on 
potential vulnerabilities in the software 
High R1: Log, monitor, and analyze all inputs and 
outputs for AI models to detect possible 
security and performance issues (see PO.5.3).  
AI RMF: 
Govern 4.3, 
5.1, 6.1, 6.2; 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
18 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
that they can be remediated more quickly in 
accordance with risk, reducing the window of 
opportunity for attackers. 
and third-party components that the 
software uses, and investigate all credible 
reports. 
R2: Make the users of AI models aware of 
mechanisms for reporting potential security and 
performance issues. 
N1: In this context, “users” refers to AI system 
producers and acquirers who are using an AI 
model. 
R3: Monitor vulnerability and incident 
databases for information on AI-related 
concerns, including the machine learning 
frameworks and libraries used to build AI 
models. 
Measure 1.2, 
2.4, 2.5, 2.7, 
3.1, 3.2, 3.3; 
Manage 4.1 
OWASP: 
LLM03-7a, 
LLM09, LLM10 
RV.1.2: Review, analyze, and/or test the 
software’s code to identify or confirm the 
presence of previously undetected 
vulnerabilities. 
Medium R1: Scan and test AI models frequently to 
identify previously undetected vulnerabilities.  
R2: Rely mainly on automation for ongoing 
scanning and testing, and involve a human-in-
the-loop as needed.  
R3: Conduct periodic audits of AI models. 
AI RMF: 
Govern 4.3; 
Measure 1.3, 
2.4, 2.7, 3.1; 
Manage 4.1 
OWASP: 
LLM03-7b, 
LLM03-7d 
RV.1.3: Have a policy that addresses 
vulnerability disclosure and remediation, 
and implement the roles, responsibilities, 
and processes needed to support that 
policy. 
Medium R1: Include AI model vulnerabilities in 
organization vulnerability disclosure and 
remediation policies.  
R2: Make users of AI models aware of their 
inherent limitations and how to report any 
cybersecurity problems that they encounter. 
AI RMF: 
Govern 4.3, 
5.1, 6.1; 
Measure 3.1, 
3.3; Manage 
4.3 
Assess, Prioritize, and Remediate 
Vulnerabilities (RV.2): Help ensure that 
vulnerabilities are remediated in accordance 
with risk to reduce the window of opportunity 
for attackers. 
RV.2.1: Analyze each vulnerability to 
gather sufficient information about risk to 
plan its remediation or other risk 
response. 
Medium N1: This may include deep analysis of 
generative AI and dual-use foundation model 
input and output to detect deviations from 
normal behavior. 
AI RMF: 
Govern 4.3, 
5.1, 6.1; 
Measure 2.7, 
3.1; Manage 
1.2, 2.3, 4.1 
Adv ML 
RV.2.2: Plan and implement risk responses 
for vulnerabilities. 
High R1: Risk responses for AI models should 
consider the time and expenses that may be 
associated with rebuilding them.  
AI RMF: 
Govern 5.1, 
5.2, 6.1; 
Measure 3.3; 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
19 
 
Practice Task Priority Recommendations [R], Considerations [C], and 
Notes [N] Specific to AI Model Development 
Informative 
References 
R2: Establish and implement criteria and 
processes for when to stop using an AI model 
and when to roll back to a previous version and 
its components. 
C1: Consider being prepared to stop using an AI 
model at any time and to continue operations 
through other means until the AI model’s risks 
are sufficiently addressed. 
Manage 1.3, 
2.1, 2.3, 2.4, 
4.1 
 
Analyze Vulnerabilities to Identify Their Root 
Causes (RV.3): Help reduce the frequency of 
vulnerabilities in the future. 
RV.3.1: Analyze identified vulnerabilities 
to determine their root causes. 
Medium N1: The ability to review training, testing, fine-
tuning, and aligning data after the fact can help 
identify some root causes. 
AI RMF: 
Govern 5.1, 
6.1; Measure 
2.7, 3.1; 
Manage 2.3, 
4.1 
RV.3.2: Analyze the root causes over time 
to identify patterns, such as a particular 
secure coding practice not being followed 
consistently. 
Medium No additions to SSDF 1.1 AI RMF: 
Govern 5.1, 
6.1; Measure 
2.7, 3.1; 
Manage 4.1, 
4.3 
RV.3.3: Review the software for similar 
vulnerabilities to eradicate a class of 
vulnerabilities, and proactively fix them 
rather than waiting for external reports. 
Medium No additions to SSDF 1.1 AI RMF: 
Govern 5.1, 
5.2, 6.1; 
Measure 2.7, 
3.1; Manage 
4.1, 4.2, 4.3 
RV.3.4: Review the SDLC process, and 
update it if appropriate to prevent (or 
reduce the likelihood of) the root cause 
recurring in updates to the software or in 
new software that is created. 
Medium No additions to SSDF 1.1 AI RMF: 
Govern 5.2, 
6.1; Measure 
2.7, 3.1; 
Manage 4.2, 
4.3 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
20 
References 
[1] Executive Order 14110 (2023) Safe, Secure, and Trustworthy Development and Use of 
Artificial Intelligence. (The White House, Washington, DC), DCPD-202300949, October 
30, 2022. Available at https://www.govinfo.gov/app/details/DCPD-202300949   
[2] National Institute of Standards and Technology (2023) Artificial Intelligence Risk 
Management Framework (AI RMF 1.0). (National Institute of Standards and Technology, 
Gaithersburg, MD) NIST Artificial Intelligence (AI) Report, NIST AI 100-1. 
https://doi.org/10.6028/NIST.AI.100-1
 
[3] Vassilev A, Oprea A, Fordyce A, Anderson H (2024) Adversarial Machine Learning: A 
Taxonomy and Terminology of Attacks and Mitigations. (National Institute of Standards 
and Technology, Gaithersburg, MD) NIST Artificial Intelligence (AI) Report, NIST AI 100-
2e2023. https://doi.org/10.6028/NIST.AI.100-2e2023  
[4] Schwartz R, Vassilev A, Greene K, Perine L, Burt A, Hall P (2022) Towards a Standard for 
Identifying and Managing Bias in Artificial Intelligence. (National Institute of Standards 
and Technology, Gaithersburg, MD), NIST Special Publication (SP) 1270. 
https://doi.org/10.6028/NIST.SP.1270    
[5] NIST (2024) Dioptra. (National Institute of Standards and Technology, Gaithersburg, 
MD.) Available at https://pages.nist.gov/dioptra/  
[6] Souppaya MP, Scarfone KA, Dodson DF (2022) Secure Software Development 
Framework (SSDF) Version 1.1: Recommendations for Mitigating the Risk of Software 
Vulnerabilities. (National Institute of Standards and Technology, Gaithersburg, MD), 
NIST Special Publication (SP) 800-218. https://doi.org/10.6028/NIST.SP.800-218
  
[7] Executive Order 14028 (2021) Improving the Nation’s Cybersecurity. (The White House, 
Washington, DC), DCPD-202100401, May 12, 2021. Available at 
https://www.govinfo.gov/app/details/DCPD-202100401
  
[8] National Institute of Standards and Technology (2024) The NIST Cybersecurity 
Framework (CSF) 2.0 (National Institute of Standards and Technology, Gaithersburg, 
MD). https://doi.org/10.6028/NIST.CSWP.29
  
[9] Joint Task Force (2020) Security and Privacy Controls for Information Systems and 
Organizations. (National Institute of Standards and Technology, Gaithersburg, MD), NIST 
Special Publication (SP) 800-53, Rev. 5. Includes updates as of December 10, 2020. 
https://doi.org/10.6028/NIST.SP.800-53r5 
[10] Boyens JM, Smith AM, Bartol N, Winkler K, Holbrook A, Fallon M (2022) Cybersecurity 
Supply Chain Risk Management Practices for Systems and Organizations. (National 
Institute of Standards and Technology, Gaithersburg, MD), NIST Special Publication (SP) 
800-161r1. https://doi.org/10.6028/NIST.SP.800-161r1
 
[11] NIST (2023) NIST AI RMF Playbook. (National Institute of Standards and Technology, 
Gaithersburg, MD.) Available at 
https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook
  
[12] National Institute of Standards and Technology (2024) Artificial Intelligence Risk 
Management Framework: Generative Artificial Intelligence Profile. (National Institute of 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
21 
Standards and Technology, Gaithersburg, MD), NIST Artificial Intelligence (AI) Report, 
NIST AI 600-1. Available at https://airc.nist.gov/docs/NIST.AI.600-1.GenAI-Profile.ipd.pdf  
[13] National Institute of Standards and Technology (2020) NIST Privacy Framework: A Tool 
for Improving Privacy Through Enterprise Risk Management, Version 1.0. (National 
Institute of Standards and Technology, Gaithersburg, MD), NIST Cybersecurity White 
Paper (CSWP) NIST CSWP 10. https://doi.org/10.6028/NIST.CSWP.10
  
[14] Stine KM, Quinn SD, Witte GA, Gardner RK (2020) Integrating Cybersecurity and 
Enterprise Risk Management (ERM). (National Institute of Standards and Technology, 
Gaithersburg, MD), NIST Interagency or Internal Report (IR) 8286. 
https://doi.org/10.6028/NIST.IR.8286
  
[15] OWASP (2023) OWASP Top 10 for LLM Applications Version 1.1. Available at 
https://llmtop10.com
  
[16] Waltermire DA, Scarfone KA, Casipe M (2011) Specification for the Open Checklist 
Interactive Language (OCIL) Version 2.0. (National Institute of Standards and 
Technology, Gaithersburg, MD), NIST Interagency or Internal Report (IR) 7692. 
https://doi.org/10.6028/NIST.IR.7692
  
[17] Ross RS, McEvilley M, Winstead M (2022) Engineering Trustworthy Secure Systems. 
(National Institute of Standards and Technology, Gaithersburg, MD), NIST Special 
Publication (SP) NIST SP 800-160v1r1. https://doi.org/10.6028/NIST.SP.800-160v1r1
  
[18] NIST/SEMATECH (2012) What is EDA? Engineering Statistics Handbook, eds Croarkin C, 
Tobias P (National Institute of Standards and Technology, Gaithersburg, MD), Section 
1.1.1. Available at https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm 
[19] Reznik L (2022) Intelligent Security Systems: How Artificial Intelligence, Machine 
Learning and Data Science Work For and Against Computer Security. (Wiley-IEEE Press.) 
Available at https://ieeexplore.ieee.org/book/9562694
   
 
NIST SP 800-218A   Secure Software Development Practices for 
July 2024  Generative AI and Dual-Use Foundation Models 
22 
Appendix A. Glossary 
artificial intelligence 
A machine-based system that can, for a given set of human-defined objectives, make predictions, 
recommendations, or decisions influencing real or virtual environments. [1]  
artificial intelligence model 
A component of an information system that implements AI technology and uses computational, statistical, or 
machine-learning techniques to produce outputs from a given set of inputs.  [1] 
artificial intelligence red-teaming 
A structured testing effort to find flaws and vulnerabilities in an AI system, often in a controlled environment and 
in collaboration with developers of AI. [1] 
artificial intelligence system 
Any data system, software, hardware, application, tool, or utility that operates in whole or in part using AI. [1]  
data science 
The field that combines domain expertise, programming skills, and knowledge of mathematics and statistics to 
extract meaningful insights from data. [19] 
dual-use foundation model 
An AI model that is trained on broad data; generally uses self-supervision; contains at least tens of billions of 
parameters; is applicable across a wide range of contexts; and that exhibits, or could be easily modified to exhibit, 
high levels of performance at tasks that pose a serious risk to security, national economic security, national public 
health or safety, or any combination of those matters, such as by: 
(i) substantially lowering the barrier of entry for non-experts to design, synthesize, acquire, or use 
chemical, biological, radiological, or nuclear (CBRN) weapons;  
(ii) enabling powerful offensive cyber operations through automated vulnerability discovery and 
exploitation against a wide range of potential targets of cyber attacks; or 
(iii) permitting the evasion of human control or oversight through means of deception or obfuscation.  
Models meet this definition even if they are provided to end users with technical safeguards that attempt to 
prevent users from taking advantage of the relevant unsafe capabilities. [1] 
generative artificial intelligence 
The class of AI models that emulate the structure and characteristics of input data in order to generate derived 
synthetic content. This can include images, videos, audio, text, and other digital content. [1] 
model weight 
A numerical parameter within an AI model that helps determine the model’s outputs in response to i nputs. [1] 
provenance 
Metadata pertaining to the origination or source of specified data.  [13] 
",0.00,2025-06-11 19:48:15.801171,2025-06-14 03:11:28.682084,Standard,https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-218a.pdf,,NIST,2024-05-01,This standard defines standards for emerging technology governance governance and implementation. The document focuses on Use Foundation implementations and requirements.,75,,65,,Unknown,0.0,,AI,True,Valid,2025-06-14 03:38:52.123222,
43,NIST AI Risk Management Framework,"Download the AI RMF 1.0 View the AI RMF Playbook Visit the AI Resource Center In collaboration with the private and public sectors, NIST has developed a framework to better manage risks to individuals, organizations, and society associated with artificial intelligence (AI). The NIST AI Risk Manageme...","Download the AI RMF 1.0 View the AI RMF Playbook Visit the AI Resource Center
In collaboration with the private and public sectors, NIST has developed a framework to better manage risks to individuals, organizations, and society associated with artificial intelligence (AI). The NIST AI Risk Management Framework (AI RMF) is intended for voluntary use and to improve the ability to incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems.
Released on January 26, 2023, the Framework was developed through a consensus-driven, open, transparent, and collaborative process that included a Request for Information, several draft versions for public comments, multiple workshops, and other opportunities to provide input. It is intended to build on, align with, and support AI risk management efforts by others (Fact Sheet).
A companion NIST AI RMF Playbook also has been published by NIST along with an AI RMF Roadmap, AI RMF Crosswalk, and various Perspectives.
On March 30, 2023, NIST launched the Trustworthy and Responsible AI Resource Center, which will facilitate implementation of, and international alignment with, the AI RMF. Examples of how other organizations are building on and using the AI RMF can be found via the AIRC’s Use Case page.
On July 26, 2024, NIST released NIST-AI-600-1, Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile. The profile can help organizations identify unique risks posed by generative AI and proposes actions for generative AI risk management that best aligns with their goals and priorities.
To view public comments received on the previous drafts of the AI RMF and Requests for Information, see the AI RMF Development page.",0.00,2025-06-13 06:05:16.364851,2025-06-14 02:23:18.757749,Framework,https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf,"{""scoring_method"": ""content_analysis"", ""topic_corrected"": true, ""framework_scores"": {""ai_ethics"": 10, ""ai_cybersecurity"": 45}}",NIST,2023-01-26,This framework establishes a framework for emerging technology governance governance and implementation. The document focuses on NIST implementations and requirements.,45,,10,,Unknown,0.0,,AI,True,valid,2025-06-14 03:58:35.809242,
47,The U.S. Approach to Quantum Policy,"Approach to Quantum Policy By Hodan Omaar | October 10, 2023 The government’s interest in quantum technologies dates back at least to the mid-1990s, when the National Institute of Standards and Technology (NIST), Department of Defense (DOD), and National Science Foundation (NSF) held their first wor...","CENTER FOR DATA INNOVATION 1 
The U.S. Approach to Quantum Policy 
By Hodan Omaar  |  October 10, 2023 
The government’s interest in quantum technologies dates 
back at least to the mid-1990s, when the National 
Institute of Standards and Technology (NIST), Department 
of Defense (DOD), and National Science Foundation (NSF) 
held their first workshops on the topic.1 NSF described the 
field of quantum information science in a 1999 workshop 
as “a new field of science and technology, combining and 
drawing on the disciplines of physical science, 
mathematics, computer science, and engineering. Its aim 
is to understand how certain fundamental laws of physics 
discovered earlier in this century can be harnessed to 
dramatically improve the acquisition, transmission, and 
processing of information.”
2 In the nearly 25 years since 
NSF’s first workshop, quantum information science has 
advanced and its potential to drive major advances in 
computing power, secure communication, and scientific 
discovery have become more apparent. The U.S. 
government has rightly recognized that it needs to play an 
active role in ensuring the nation remains competitive in 
this critical field. 
OVERVIEW OF CURRENT U.S. POLICY APPROACH 
Quantum information science (QIS) is an umbrella term encompassing 
several different technologies. In this report, “QIS” or “quantum” 
encompasses the following five technologies: 
 Quantum sensing and metrology, which refers to the use of 
quantum mechanics to enhance sensors and measurement 
science. 
  
CENTER FOR DATA INNOVATION 2 
 Quantum computing, which refers to the development of 
computers that use quantum mechanics to perform calculations 
exponentially faster than classical computers. 
 Quantum networking, which refers to the development of secure 
communication protocols that use the principles of quantum 
mechanics to ensure the confidentiality and integrity of transmitted 
information. 
 QIS for advancing fundamental science, which refers to using 
quantum devices and QIS theory to expand fundamental 
knowledge in other disciplines; for example, to improve 
understanding of biology, chemistry, and energy science. 
 Quantum technology, which catalogs several topics including using 
quantum technologies to create practical applications; creating the 
necessary infrastructure and manufacturing techniques for 
electronics, photonics, and cryogenics; and minimizing the risks 
associated with quantum technologies, such as developing post-
quantum cryptography to protect sensitive information.3 
There has been important action from both the executive branch and the 
legislative branch in recent years to shape QIS policy.  
On the executive side, the White House has issued two seminal reports 
articulating a national strategic approach to QIS through the National 
Science and Technology Council (NSTC), which is the principal body 
through which the executive branch coordinates quantum policy across the 
diverse entities that make up the federal research and development (R&D) 
enterprise. NSTC published its first report titled Advancing Quantum 
Information Science: National Challenges and Opportunities in July 2016 
under President Obama.4 This report outlined three principles to help guide 
an “all-of-government approach to QIS,” which were to maintain stable and 
sustained core programs that could be enhanced as new opportunities 
appear and restructured as impediments evolve; invest strategically in 
targeted, time-limited programs to achieve concrete, measurable 
objectives; and closely monitor the QIS field to evaluate the outcome of 
federal QIS investments and quickly adapt programs to take advantage of 
technical breakthroughs as they are made.
5 
NSTC released its second report, National Strategic Overview for Quantum 
Information Science, in September 2018 under President Trump, and this 
report identified six policy opportunities and priorities for federal quantum 
investments:  
 Choosing a science-first approach to QIS 
 Creating a quantum-smart workforce for tomorrow 
  
CENTER FOR DATA INNOVATION 3 
 Deepening engagement with quantum industry 
 Providing critical infrastructure 
 Maintaining national security and economic growth 
 Advancing international cooperation6 
On the legislative side, the most significant piece of legislation related to 
quantum to date has been the National Quantum Initiative Act (NQIA), a bill 
signed into federal law in December 2018 that was designed to accelerate 
and advance quantum science and technology in the United States. 
Essentially, the NQIA created a framework for quantum R&D and 
authorized just over $1.2 billion in funding over five years (fiscal years 
2019 to 2023) for a variety of initiatives, allocated primarily across the 
three agencies that have historically been heavily involved in QIS R&D: 
NIST, NSF, and the Department of Energy (DOE). Some of the NQIA’s key 
components include authorizing these agencies to strengthen QIS 
programs and research centers; establishing a new federal agency called 
the National Quantum Coordination Office (NQCO), housed under the Office 
of Science and Technology Policy (OSTP), and tasking it with coordinating 
QIS activities across the federal government, industry, and academia; and 
establishing a new federal advisory committee called the National 
Quantum Initiative Advisory Committee (NQIAC), composed of experts from 
academia, industry, and government and tasking it with providing 
independent assessment of and recommendations for the NQIA program. 
The programs the NQIA authorize expired on September 30, 2023, and the 
bill needs to be reauthorized in order to continue U.S. leadership in this 
critical field. 
The CHIPS and Science Act of 2022 amended the NQIA to authorize R&D in 
quantum networking infrastructure; instruct NIST to develop standards for 
quantum networking and communication; establish a DOE program to 
facilitate a competitive, merit-reviewed base process for access to U.S.-
based quantum computing resources for research purposes; and require 
NSF to support the integration of QIS into the science, technology, 
engineering, and mathematics (STEM) curriculum at all education levels.
7 It 
also explicitly includes QIS in the new NSF directorate focused on emerging 
technologies, the Directorate for Technology, Innovation, and Partnerships 
(TIP).
8 
This rest of this report explores four broad policy areas the U.S. 
government uses to promote competitiveness in quantum. These are 
policies that support quantum R&D, strengthen the quantum workforce, 
build a quantum ecosystem, and collaborate with international partners.  
This report also makes 10 recommendations across these policy areas to 
Congress: 
  
CENTER FOR DATA INNOVATION 4 
1. Reauthorize the NQIA and appropriate at least $525 million per 
year (in addition to the CHIPS funding) for FY 2024 to FY 2028. 
2. Fully fund the quantum user expansion for science and technology 
(QUEST) program authorized by the CHIPS and Science Act to 
improve researcher accessibility to U.S. quantum computing 
resources.  
3. Establish a quantum infrastructure program within DOE to help 
meet the equipment needs of researchers as part of the 
reauthorization of the NQIA.  
4. Fully fund the NSF Quantum Education Pilot Program authorized in 
the CHIPS and Science Act, which would allocate $32 million over 
the next five years to support the education of K-12 students and 
the training of teachers in the fundamental principles of QIS. 
5. Direct NSF to collaborate with NIST to conduct a systematic study 
of quantum workforce needs, trends, and education capacity.  
6. Authorize and fund a DOE-led training program that partners 
students studying toward bachelor’s, master’s, or Ph.D. degrees 
with DOE national labs for hands-on QIS experience.  
7. Direct the Department of Commerce to work with the Quantum 
Economic Development Consortium (QED-C) to review the quantum 
supply chain and identify risks.  
8. Direct and fund the recently established Directorate for TIP within 
NSF to establish quantum testbeds for use-inspired research. 
9. Direct DOE to establish and lead a program that invites allied 
nations to co-invest in quantum moonshots. 
10. Direct NIST to prioritize promoting U.S. participation, particularly 
from U.S. industry stakeholders, in international standards fora in 
the reauthorization of the NQIA. 
SUPPORTING QUANTUM RESEARCH & DEVELOPMENT 
While civilian, defense, and intelligence agencies have a long history of 
investing in quantum R&D, the government has taken important steps 
recently to accelerate, strengthen, and coordinate federal quantum R&D 
investments with the NQIA and CHIPS and Science Act.
9 Three of the most 
important government actions supporting QIS R&D are in increasing QIS 
R&D funding, facilitating interdisciplinary research, and facilitating access 
to R&D resources. 
  
CENTER FOR DATA INNOVATION 5 
Increasing QIS R&D Funding 
The NQIA catalyzed a significant increase in federal funding for QIS R&D, 
roughly doubling federal funding between FY 2019 and FY 2023. It is 
important to note that while the NQIA sets funding targets and priorities for 
QIS R&D across various federal agencies, it does not guarantee specific 
funding amounts. The president and Congress set nondefense quantum 
R&D priorities and funding for each federal agency through an annual 
fiscal year budget, with defense spending set through a separate bill called 
the National Defense Authorization Act.  
Figure 1 shows U.S. R&D budgets for QIS since the inception of the NQIA, 
with agencies reporting actual budget expenditures for quantum R&D of 
$449 million in FY 2019, $672 million in FY 2020, and $855 million in FY 
2021, followed by $918 million of enacted budget authority for quantum 
R&D in FY 2022 and a requested budget authority of $844 million for 
quantum R&D in FY 2023.
 10 The portion of each bar in figure 1 marked 
“NQI” identifies funding allocated for NQIA-authorized activities, meaning it 
is additional funding on top of the budgets for baseline QIS R&D activities. 
Figure 1: U.S. Quantum Information Science R&D budgets after 
National Quantum Initiative Act was enacted11 
 
 
Figure 2 shows that the government has increased and sustained funding 
across all five program component areas that were classified in the 
National Strategic Overview for QIS, namely quantum sensing and 
metrology (QSENS), quantum computing (QCOMP), quantum networking 
(QNET), quantum advancements (QADV), and quantum technology (QT). 
$0M
$200M
$400M
$600M
$800M
$1,000M
FY 2019
Actual
FY 2020
Actual
FY 2021
Actual
FY 2022
Estimated
FY 2023
Proposed
Base NQI
  
CENTER FOR DATA INNOVATION 6 
Figure 2: U.S. Quantum Information Science R&D by program 
component area12 
 
There is widespread consensus that increased funding is necessary. Indeed, 
the NQIAC discussed its recommendations to Congress on how it should fund 
the next iteration of the National Quantum Initiative and one of its central 
recommendations was that sustained and increased funding “will be 
necessary for our nation to win the race to realize the benefits of QIS.”
13 
This is true. Quantum technologies are still in the very early stages and the 
road to maturity and diffusion is long. The first step in the innovation 
process is what Princeton Professor Donald Stokes called “Pasteur’s 
quadrant” research—basic research directed at a specific challenge or 
problem.
14 This type of research provides foundational, generic knowledge 
that industry can draw on for ideas and innovation. The problem is the 
private sector is not sufficiently incentivized to conduct fundamental 
research because it is almost never able to capture all the spillover 
benefits of initial investments, or capture these benefits fast enough, to 
justify investing at the same level as the government. This is especially true 
in the case of basic research, which is costlier and riskier than applied 
R&D.
15 Also, the private sector tends to narrowly focus its research on only 
the fields that are commercially relevant and economically beneficial, 
rather than on all those that might advance the public good. Federal 
funding for QIS R&D is therefore critical to ensure the effective 
development of new knowledge, techniques, and technologies. 
Increased and sustained funding is particularly necessary to stay 
competitive because several other countries are investing in QIS R&D, 
some funding QIS far more than the United States is. The United Kingdom, 
for example, is launching a 10-year program as part of its National 
Quantum Strategy, which promises to invest £2.5 billion ($3.1 billion) in 
$0M
$200M
$400M
$600M
$800M
$1,000M
FY 2019
Actual
FY 2020
Actual
FY 2021
Actual
FY 2022
Estimated
FY 2023
Proposed
QSENS QCOMP QNET QADV QT
  
CENTER FOR DATA INNOVATION 7 
quantum R&D starting in 2024 with the aim of attracting an additional £1 
billion ($1.3 billion) of private investment.16 And the EU is investing public 
funding for quantum computing that is almost four times that of the United 
States, while China’s is almost eight times that of the United States.17 
The question becomes, How much funding is enough to accelerate U.S. QIS 
innovation and keep the nation competitive? That is difficult to answer in 
part because, while the government’s efforts to increase QIS R&D funding 
through the NQIA are easily quantifiable, the benefits of these efforts are 
more difficult to quantitatively translate because there exist few consistent, 
comprehensive measures of how much U.S. QIS research has changed 
over time. It might be the case that the NQIAC has this data as part of its 
assessment of the NQI program, but it has not publicly released this 
information. However, in its written comments to the NQIAC in March 
2023, the Energy Sciences Coalition, a broad-based coalition of over 100 
organizations representing scientists, engineers, and mathematicians in 
universities and industry and national laboratories, recommended “at least 
$675 million each year over five years from FY 2024 through FY 2028.”
18 
While there are reasons to be hopeful that there will be substantial funding 
for quantum, there are also reasons to be less optimistic. On the positive 
side, Congress has already authorized significant amounts of quantum 
funding through the recently passed CHIPS and Science Act of 2022. 
Although no funds have been appropriated, the CHIPS and Science Act is 
authorizing legislation that sets funding targets that aim to energize 
American innovation across a variety of industries—one of which is 
quantum. It directly authorizes new investments in core quantum research 
programs, such as $500 million toward an R&D program for quantum 
networking infrastructure, but it also significantly increases investments in 
many other critical industries that will feed into quantum applications, such 
as $2 billion for a DOD-led microelectronics R&D program that will be pay 
huge dividends to the development of quantum systems that rely on 
microelectronic components. 
Unfortunately, despite many political leaders paying lip service to the act’s 
goal of bolstering American competitiveness in key innovation industries, 
the CHIPS and Science Act is not getting the funding it needs. Neither the 
Biden administration’s FY 2024 budget request nor the federal 
government’s omnibus spending bill for the 2023 fiscal year have met the 
funding target set by CHIPS. The administration’s budget request falls short 
of agency targets by more than $5 billion, while the omnibus funding is 
nearly $3 billion short of the authorized targets for NSF, DOE’s Office of 
Science, and NIST.
19  
What’s more, funding for CHIPS and the reauthorization of the NQIA look to 
be tight given recent battles over lowering overall federal spending.20 As a 
member of the House Science, Space, and Technology Committee working 
on the reauthorization noted in a 2023 Center for Data Innovation panel, 
“There are different parameters in this Congress than there were last year 
  
CENTER FOR DATA INNOVATION 8 
in the 116th [Congress] … I don’t think that we’re going to be seeing a 
CHIPS-like program.”21 However, as another member on the committee put 
it, “It is essential that the ‘Science’ part of CHIPS and Science is 
appropriated money and that will pay huge dividends in quantum 
information science, in both basic and applied research.”22 
Facilitating Interdisciplinary Research  
NSF presciently identified in its workshop back in 1999 that quantum 
research is profoundly interdisciplinary and advancements in the field 
would require “the combined effort of people with expertise in a wide 
variety of disciplines, including mathematics, computer science and 
information theory, theoretical and experimental physics, chemistry, 
materials science, and engineering.”
23 Consider a research project 
investigating the development of a scalable, fault-tolerant quantum 
communication system. Developing a system that can operate in the 
presence of noise and other disturbances requires expertise in quantum 
communication protocols, computer algorithms, hardware design, and 
experimental physics. 
Unfortunately, research institutes in the United States have not historically 
collaborated on quantum research across disciplinary boundaries very 
effectively. A 2018 report on quantum from the Congressional Research 
Service, a public policy research institute of Congress, found that “federal 
departments, and even agencies and offices within a department, have 
sponsored R&D at universities in different disciplines to address unique 
federal mission requirements. As a result, coordination and collaboration 
among university researchers is difficult.”
24  
Both NSF and DOE are working to surmount these institutional barriers by 
supporting the establishment of interdisciplinary research centers, though 
they have taken different approaches that reflect their different missions 
and funding priorities. NSF is focused on facilitating faculty collaboration 
across departmental boundaries at university-based centers and institutes. 
As of March 2023, the agency had funded five Quantum Leap Challenge 
Institutes, which are large-scale interdisciplinary research institutes led by 
universities to advance specific frontiers of QIS and engineering. For 
instance, the University of Maryland is leading one of the five institutes, the 
Quantum Leap Challenge Institute for Robust Quantum Simulation, which 
is focused on building systems that can robustly simulate the behavior of 
quantum systems.
25 This institute brings together researchers from five 
universities: Duke, Princeton, North Carolina State University, Yale, and the 
University of Maryland. 
Figure 3 shows that NSF is not only funding research centers focused on 
the broader study of quantum science and technology but also funding 
several quantum foundries, which are specialized research centers that 
focus on developing and manufacturing the materials quantum 
technologies need, such as qubits and quantum sensors.
 
CENTER FOR DATA INNOVATION 9 
Figure 3: NSF’s center-scale investments in QIS research centers26 
 

 
CENTER FOR DATA INNOVATION 10 
On the other hand, DOE has built interdisciplinary quantum research 
centers at its own national laboratories, where it can leverage its existing 
infrastructure and expertise to conduct cutting-edge QIS research. It has 
established five national QIS research centers:  
 Co-design Center for Quantum Advantage, led by Brookhaven 
National Laboratory, focused on building the tools necessary to 
create fault-tolerant quantum computer systems  
 Q-NEXT, led by Argonne National Laboratory, focused on how to 
reliably control, store, and transmit quantum information across 
distances  
 Quantum Science Center, led by Oak Ridge National Laboratory, 
focused on advancing the science of quantum materials, sensors, 
and algorithms  
 Quantum Systems Accelerator, led by Lawrence Berkeley National 
Laboratory, focused on developing a range of scalable quantum 
systems  
 Superconducting Quantum Materials and Systems Center, led by 
Fermi National Accelerator Laboratory, focused on developing 
superconducting materials and devices for next-generation 
quantum computers  
The NQIAC, as part of its assessment of the National Quantum Initiative, 
has evaluated progress in quantum collaborations and partnerships and 
found that overall, collaboration across centers is developing well. 
However, one issue hindering progress is the administrative burden on 
academic researchers. This problem is not new or specific to quantum 
research centers; there have been several reports over the last decade 
that indicate federal requirements imposed on research universities are 
excessive, impeding the efficiency and productivity of university research.
27 
The NQIAC has found that these problems are impacting NSF center 
collaborations for quantum research and are getting worse over time, as 
well as limiting industry participation.28 The committee’s recommendations 
to Congress include that Congress should augment NSF center funding to 
support professional administrative staff and that it should support efforts 
to homogenize the forms and agreements that permit these 
collaborations.29  
Facilitating Access to Quantum R&D Facilities 
An important component of R&D leadership is the availability of world-class 
research facilities both at universities and at national labs. There are 
several types of research facilities any country needs to advance QIS, but 
perhaps three of the most important types are quantum user facilities, 
quantum foundries, and quantum testbeds.  
  
CENTER FOR DATA INNOVATION 11 
A quantum user facility is a research facility that provides access to 
advanced quantum systems, such as quantum computers, for researchers 
and other users who may not have the resources or expertise to build or 
operate their own. Because user facilities are typically places where 
students, postdocs, and researchers go to use tools, they are uniquely 
positioned to support workforce development in ways that other R&D 
facilities might not. The Quantum Computing User Program at DOE’s Oak 
Ridge National Laboratory, which provides access to state-of-the-art 
quantum computing resources, is an example of such a facility. 
A quantum foundry is a facility or organization that specializes in 
developing and producing materials for quantum devices and systems, 
such as qubits, which are the basic building blocks of quantum computers. 
NSF funds quantum foundries at universities, such as the quantum foundry 
hosted at the University of California, Santa Barbara and the MonArk 
quantum foundry jointly led by Montana State University and the University 
of Arkansas.
30 DOE also funds and hosts quantum foundries, such as the 
Argonne Quantum Foundry, which is a 6,000-square-foot facility focused on 
developing scalable semiconductor quantum systems located at Argonne 
national laboratory. 
Figure 4: A dilution refrigerator, which creates the ideal 
environment for qubit performance, at Argonne Quantum Foundry 
  
A quantum testbed is a platform or system that is used for testing and 
experimenting with quantum computing hardware and software. These 
testbeds can include actual quantum computers, as well as simulation 
tools that allow researchers to simulate the behavior of quantum systems. 

  
CENTER FOR DATA INNOVATION 12 
Testbeds are typically used to develop and refine quantum algorithms, 
software, and applications, and to test the performance of different types 
of quantum hardware.  
The United States was once unique in providing world-leading research 
facilities to its researchers, but that is no longer the case. In fact, today, the 
United States lags behind other countries. DOE published a report in 2021 
investigating how countries around the world are investing in constructing 
and upgrading research facilities for several critical fields, one of which 
was QIS, and found:  
While facilities in the U.S. set the pace technically, demand for access 
to them far exceeds their current capacity; access to comparable 
facilities is more extensive in other countries, especially in Europe. 
Additionally, supporting resources such as the number of staff 
scientists available to assist both university and industrial users of 
these complex facilities are more extensive outside the U.S.
31  
The NQIAC subcommittee on science and infrastructure reiterated these 
findings in its own evaluation of the status of research facilities. However, 
the committee noted that U.S. private sector research facilities surpass 
those of other nations and the nation should leverage this advantage. 
The QUEST program authorized by the CHIPS and Science Act seeks to do 
just that. The bill is authorized at $165.8 million over five years and tasks 
DOE with working to improve accessibility to U.S. quantum computing 
resources, including private sector resources, for U.S.-based researchers 
and laboratories through a transparent, merit-review application process. 
Recommendations 
 Congress should reauthorize the NQIA and appropriate at least 
$525 million per year (in addition to the CHIPS funding) for FY 
2024 to FY 2028. To ensure U.S. leadership in quantum, Congress 
should fund all the activities in the NQIA at the authorized level. 
 Congress should fully fund the QUEST program authorized by the 
CHIPS and Science Act to improve researcher accessibility to U.S. 
quantum computing resources. The CHIPS and Science Act 
authorizes the largest publicly funded R&D program in U.S. 
history.32 Funding for QIS is included in the “Science” part of the 
CHIPS and Science Act, while the CHIPS part provides for American 
semiconductor R&D. Unfortunately, government appropriations as 
they currently stand have fallen short of the targets set forth in the 
Act and have focused predominantly on the CHIPS portion. To 
properly drive quantum innovation, Congress should sufficiently 
fund the “Science” portion of the Act and, in particular, should fund 
the QUEST program. 
  
CENTER FOR DATA INNOVATION 13 
 Congress should establish a quantum infrastructure program 
within the DOE to help meet the equipment needs of researchers 
as part of the reauthorization of the NQIA. Currently, there is no 
specialized program or funding source dedicated to meeting the 
distinct infrastructure requirements of quantum researchers and 
developers, and the NQIA so far has not sufficiently focused on 
supply chain needs and manufacturing capabilities. Creating a 
DOE-led program as part of the reauthorization could address this 
gap and should focus on establishing quantum foundries, 
specialized equipment, and laboratory facilities. The program 
should be authorized for at least $300 million over five years.  
STRENGTHENING THE QUANTUM WORKFORCE 
U.S. policy related to talent covers QIS education at the K-12 and higher 
education level, workforce training for the existing quantum workforce, and 
immigration policies to attract and retain foreign talent. 
Quantum Education 
Primary and Secondary Education 
Quantum education at the K-12 level is just getting started. In the United 
States, the responsibility for primary and secondary education, including 
school financing, teaching credentials, and curricula fall on the states, but 
the federal government recognizes that it has a role to play in supporting 
their efforts to ensure the nation has a skilled quantum workforce.  
At this stage, the focus of federal programs for K-12 quantum education is 
outreach and engagement, meaning introducing concepts of quantum 
technologies and science to students in middle and high school. OSTP and 
NSF joined forces in August 2020 to create the National Q-12 Education 
Partnership between the federal government, industry, professional 
societies, and the education community to provide a foundation for 
classroom and curricula materials.
33 Resources it has created include 
frameworks to help educators integrate QIS into STEM lessons and 
curricula such as those used in chemistry, physics, and computer science 
(CS) classes, as well as a repository of useful tools such as textbooks, 
lecture notes, and online courses for quantum-related education.
34  
However, one of the disadvantages of the decentralized U.S. approach to 
education is that it can lead to disparities in the depth and scope of 
integration efforts. Consider the integration of QIS into CS courses, which 
the national QIS partnership identifies as one of the most promising 
potential avenues for introducing students to QIS concepts.
35 Only 53 
percent of U.S. high schools offer foundational CS and only 27 of the 50 
states and District of Columbia require all high schools to offer CS.36 
Moreover, an NSF-funded study in 2018 found geographic disparities in 
where CS is taught, with schools in the West (44 percent) and Northeast 
  
CENTER FOR DATA INNOVATION 14 
(43 percent) more likely to offer CS courses than schools in the Midwest 
(30 percent) and South (24 percent).37 These disparities may mean only a 
limited number of schools can effectively integrate QIS into CS in the first 
place.
38  
An alternative to the U.S. approach is a more centralized, national 
government-mandated approach to QIS integration in STEM subjects such 
as that of the Netherlands. The Netherlands has a long history of studying 
quantum physics—some of the early pioneers of the field were Dutch—but 
the country only recently started teaching quantum physics as part of the 
national curriculum.39 For context, the Dutch education system, much like 
that of several other European countries, has several tracks that provide 
students with different levels of education and prepare them for different 
career paths. The most academically challenging of these tracks is VWO, or 
voorbereidend wetenschappelijk onderwijs, which is a six-year program 
that prepares students for academic education at a research university 
and accounts for approximately a fifth of Dutch high school students.40 In 
2014, the Minister of Education amended the national VWO curriculum to 
include quantum physics into the syllabus and made this topic a 
compulsory part of the final exam. Some of the learning outcomes of the 
new syllabus are relatively complex, requiring students be able to “describe 
quantum phenomena in terms of the confinement of a particle” and 
“describe the quantum tunnelling effect by means of a simple model and 
indicate how the probability of tunnelling depends on the mass of the 
particle and the height and width of the energy barrier.”
41 This approach 
may not be realistic within the realities of the U.S. education system, but 
the U.S. government should recognize that other countries are working to 
adapt their education systems to prepare their own future quantum 
workforces and consider how it can best ensure domestic education is 
equitable and functions as effectively as possible. 
Besides formal classroom teaching, there are several informal learning 
opportunities under development for teachers, students, and families. 
Several nonprofits, learning programs, and courses have sprung up to 
provide accessible and inclusive quantum education to high school 
students such as Qubit by Qubit, an initiative of the California-based 
nonprofit organization the Coding School, which runs quantum summer 
camps for students in middle and high school as well as a year-long course 
in quantum computing that students in more than 150 U.S. high schools 
have taken.
42 This course is accredited by the Western Association of 
Schools and Colleges, meaning students can earn high school credits for 
taking it, and is approved by the University of California as a college 
preparatory course.43 
The private sector is also reinforcing QIS integration in schools through a 
number of different initiatives ranging from after-school programs to 
hackathons. IBM has launched the IBM Quantum Educators program, 
which provides educators with resources and support to teach quantum 
  
CENTER FOR DATA INNOVATION 15 
computing concepts and skills. Microsoft has also launched the Quantum 
Development Kit, which includes resources for educators to teach 
quantum computing concepts, as well as tutorials and examples for 
students to learn how to program quantum computers. Finally, Intel’s 
Quantum Computing Education program provides resources and support to 
educators and students to help them learn about quantum computing and 
its potential applications. While there may not be comprehensive data on 
the exact amount for-profit companies are contributing to quantum 
education, such initiatives and programs demonstrate that there is a 
growing interest and investment in supporting quantum education. 
Higher Education 
Unlike U.S. high schools, where QIS-related STEM education is subpar, 
there are U.S. institutions of higher education that boast strong STEM 
programs and are increasingly integrating quantum courses, drawing 
students from around the world. 
At the undergraduate level, very few universities offer specific QIS majors. 
Instead, QIS-related courses are usually taken as electives at the upper 
division of STEM bachelor’s degrees and mostly cover fundamental 
concepts or offer a broad introduction to quantum information topics. The 
University of Colorado Boulder, for example, offers a quantum engineering 
minor in a broad range of disciplines, and the University of Mexico offers a 
10-week summer undergraduate research program in quantum 
technologies.
44 
Specific quantum information-related tracks are more common at the 
master’s level in disciplines such as physics, engineering, and computer 
science. Some schools have entire master’s programs in quantum science, 
including University of California, Berkeley, Duke University, and Columbia 
University. Similarly, QIS-related tracks are often offered within existing 
Ph.D. programs such as in physics or computer engineering, though 
recently, Harvard University launched a Ph.D. program in quantum science 
and engineering that stands on its own.
45 
Ideally, colleges and universities would adapt their degree offerings to 
ensure students who want to study quantum-related fields can take the 
courses and learning opportunities they need in order to be prepared to 
thrive in future work. But in reality, universities face several challenges to 
reform. 
From a financial perspective, quantum research and education require 
universities to invest in expensive equipment and facilities and hire 
qualified faculty to teach and develop curricula, all of which can be a 
significant financial burden. Budget restraints are particularly tight for 
publicly funded universities. More and more states in the last 10 years 
have changed how they give funding, from only considering how many 
students are enrolled to also rewarding universities for how many students 
  
CENTER FOR DATA INNOVATION 16 
complete their degrees.46 A 2021 paper published in the International 
Journal of Teaching and Learning in Higher Education explains that “this 
model incentivizes maintaining student enrollment counts through large 
and required courses and may dissuade faculty from exploring innovative 
curricula that could reduce enrollments in courses within their own 
disciplines but ultimately better serve the needs of students.”
47 Given a 
deepening enrollment crisis exacerbated by the COVID-19 pandemic, it is 
hard to imagine many public colleges and universities will have the 
bandwidth to effectively innovative with new quantum curricula without 
support.
48 
One thing that could help universities make more-informed decisions about 
how best to educate the future quantum workforce is knowing the type of 
quantum jobs available for their students and what skills and degrees are 
most relevant for those new jobs. Fortunately, a team of researchers led by 
DOE’s Fermilab recently published a survey that assesses the degrees 
needed for different job roles in the U.S. quantum industry.
49 Figure 5 
shows the different levels of education and qualifications 57 organizations 
said they would require for various jobs. 
Figure 5: The distribution of degrees needed for different job roles 
in the quantum industry50 
 
Two important takeaways emerge from the survey. First, there are a 
number of job opportunities in the quantum industry ranging from highly 
specific ones, such as quantum algorithm developer and error correction 
scientist, to broader jobs categories within the business, software, and 
hardware sectors. These broader jobs require a range of skills, most of 
which are not quantum related. Educators developing new curricula and 
degree programs should consider the balance between quantum-specific 
courses and more-general STEM courses. Second, companies are looking 
for a range of degree levels to fill new quantum positions, from bachelor’s 

  
CENTER FOR DATA INNOVATION 17 
to master’s to Ph.Ds., but a requirement for postgraduate degrees is more 
common. Therefore, universities may be wise to continue the trend of 
offering QIS-specific programs at the master’s level and integrating singular 
classes or courses in existing programs at the bachelor level.  
Workforce Training 
The quantum field is creating more jobs in academia, industry, national 
labs, and government than can currently be filled, according to the QIST 
Workforce Development National Strategic Plan published in February 
2022.
51 There are four workforce policies it identifies as key for the 
government: developing and maintaining an understanding of the 
workforce needs in the QIST ecosystem, introducing broader audiences to 
QIST through public outreach and educational materials, addressing QIST-
specific gaps in professional education and training opportunities, and 
making careers in QIST and related fields more accessible and equitable.52  
Several bodies, including the NQIAC and the QED-C, have echoed the 
urgency of the first action, understanding the workforce needs of the QIST 
ecosystem. But despite broad consensus that there is a skills gap problem, 
there does not seem to be a commonly agreed upon definition of what 
constitutes “QIS expertise” or the “QIS workforce.” Without a common 
definition, it is difficult to truly understand the pervasiveness, scale, and 
concentration of skills misalignments. As figure 5 demonstrates, there are 
many types of expertise one can include in a measure of the QIS workforce, 
ranging from top error correction specialist to an entry-level technician who 
can assemble hardware. There are also many different domains of QIS 
expertise; a team wanting to use quantum computing to simulate chemical 
molecules would need expertise in quantum hardware, software, and 
algorithms, as well as an in-depth understanding of chemistry. Therefore, a 
first step would be for the government to clarify what constitutes the QIST 
workforce.  
Attracting Foreign Quantum Talent 
Attracting and securing highly skilled foreign-born talent plays a vital role in 
U.S. innovation and competitiveness in quantum. Consider that more than 
half of doctoral students who graduate with QIST-relevant backgrounds are 
non-U.S. citizens or non-permanent residents, or that many of the most 
important companies in the QIS ecosystem employ and are led by foreign-
born workers.53 Google’s Quantum team, for instance, is led by Hartmut 
Neven, who was born in Germany, IBM’s quantum computing team is led 
by Jay Gambetta, who was born in Australia, and California-based quantum 
computing company Rigetti Computing was founded by Canadian-born 
Chad Rigetti. Given the importance of foreign-born QIS workers to U.S. 
innovation success, the nation needs policies to strengthen and expand 
the immigration pipeline that allows highly trained QIS talent to innovate in 
the United States, including foreign STEM graduates of U.S. colleges and 
universities.  
  
CENTER FOR DATA INNOVATION 18 
But while many competitor nations, including the United Kingdom, China, 
Canada, France, and Australia, have adopted flexible immigration policies 
to attract foreign talent in QIS and other technical fields, the U.S. 
immigration system has remained largely the same for the last 50 years. 
Its outmoded visa laws, as well as international competition for talent from 
other countries, are causing many international scientists and engineers to 
look outside the United States for education and employment. U.S. industry 
leaders note that foreign companies in countries with strong quantum 
backgrounds such as Canada, China, France, and the United Kingdom 
have been particularly adept at attracting the talent the United States has 
historically profited from.
54 Five years later, this trend does not seem much 
improved, as a February 2023 article in U.S.-based political newspaper The 
Hill indicates: “In the last decade, Canada has fostered an influx of new 
immigrant STEM workers and university students while the U.S. has done 
the opposite and is increasingly trending towards fewer immigrant STEM 
professionals working here.”
55  
Recommendations 
 Congress should fully fund the NSF Quantum Education Pilot 
Program authorized in the CHIPS and Science Act, which would 
allocate $32 million to support the education of K-12 students and 
the training of teachers in the fundamental principles of QIS. It 
would direct NSF to offer competitive, merit-based grants to 
institutions of higher education, nonprofits, and other organizations 
that would then partner with K-12 schools to develop and 
implement QIS curricula, incorporate QIS into the broader STEM 
curricula, offer opportunities for students to explore QIS higher 
education programs and career paths, and develop professional 
development and training programs in QIS for educators.  
 Congress should direct NSF to collaborate with NIST to conduct a 
systematic study of quantum workforce needs, trends, and 
education capacity. There is little reliable data on the current and 
future workforce needs of the quantum industry or the capacity of 
U.S. institutions to effectively nurture quantum talent. This data will 
be key to inform ongoing investments in quantum education and 
workforce development programs. NSF should therefore lead a 
holistic study that elucidates the size and makeup of the supply 
and demand for talent, with well-defined methodologies for data 
collection. This study should be conducted and monitored once 
every two years to effectively assess trends, provide forecasts, and 
inform contingency strategies. 
 Congress should authorize and fund a DOE-led traineeship program 
that partners students studying toward bachelor’s, master’s, or 
Ph.D. degrees with DOE national labs for hands-on QIS experience. 
  
CENTER FOR DATA INNOVATION 19 
This program should ensure it considers how to expand 
participation of underrepresented groups and institutions in QIS, 
including non-R1 academic institutions, Historically Black Colleges 
and Universities, and Minority Serving Institutions. 
BUILDING A QUANTUM ECOSYSTEM 
The United States has established a problem-focused, industry-led 
consortium called the QED-C whose primary focus is to enable and grow 
the quantum industry. The United States is not alone in creating such a 
body, as three other regions have similar consortiums: Japan’s Quantum 
Strategic industry Alliance for Revolution, Quantum Industry Canada, and 
the European Quantum Industry Consortium. All are doing similar and 
important work, but QED-C has been critical for the United States in two 
particular areas: identifying supply chain dependencies and supporting 
commercialization.  
Identifying Supply Chain Dependencies 
Quantum computers appear to be the QIS technology with the most high-
profile supply chain issues. In a 2022 report, the QED-C noted, “Based on a 
survey of quantum computing (QC) commercial entities spanning the QC 
ecosystem, there are significant concerns that there could be a serious QC-
related supply chain disruption in the next few years. Potential choke 
points are widely dispersed across the supply chain spanning assured 
access to necessary raw materials to a steady supply of trained software 
experts.”
56 The Government Accountability Office found similar results for 
QIS technologies generally, noting in a 2021 report, “The quantum 
technology supply chain is global and specialized. Given the complexity of 
the supply chain, if a single link in the chain is unavailable, that could 
cause technology development delays and other setbacks.”57   
In some instances, the United States is reliant on its allies. For example, 
Finland and the United Kingdom are leaders in the development and 
production of cryogenic devices, which are indispensable to creating the 
extremely cold conditions needed for certain quantum computers to 
operate. In other instances, however, the United States is reliant on China. 
China dominates the market for rare-earth ions, which constitute one of 
the most versatile materials for building QIS technologies because they can 
maintain their quantum states for relatively long periods of time and emit 
and absorb light at very specific wavelengths, making them useful for 
applications such as quantum communication, quantum sensing, and 
quantum computing. Today, China accounts for 63 percent of the world’s 
rare-earth mining, 85 percent of rare-earth processing, and 92 percent of 
rare-earth magnet production.
58  
At this time, QIS technologies do not have stable supply chains because 
the field is constantly evolving, meaning the importance of vulnerabilities 
continually rises and falls. It could be the case that rare-earth ions become 
  
CENTER FOR DATA INNOVATION 20 
the de facto material for qubits in the coming years, but it could also be the 
case that they do not and a different implementation for quantum 
computers wins out. This is one reason policymakers should continually 
track the dynamics of the U.S. quantum supply chain and foreign 
government policy initiatives that may have an impact.  
For near-term issues, QED-C worked with market intelligence firm Hyperion 
Research in 2022 to conduct a survey exploring issues QC companies may 
face in the next three years with supply chains for materials, components, 
and QC-finished products. Of the 47 respondents in U.S. industry, 58 
percent of organizations said they would “experience at least some QC-
related supply chain disruption that would affect their ability to either 
supply materials, components, or sub-assemblies to the QC sector or 
directly market QC-related goods and/or services.”59 When asked what 
would be the single most likely cause of a such a disruption, access to key 
raw materials and manufacturing or assembly equipment were the two 
most popular answers.60 
Supporting Commercialization 
The development of large-scale quantum systems, particularly quantum 
computers, depends on the ability to scale the smaller systems in play 
today. As a 2019 report from the National Academies of Sciences, 
Engineering, and Medicine points out, historically, growth in technological 
systems has resulted from a virtuous cycle wherein better technology 
generates more revenue, which companies reinvest in R&D, which in turn 
attracts both new talent and companies that have helped bring the 
technology to the next level (see figure 6).
61 To begin such a virtuous cycle 
for QIS technologies, the key will be to create a growing market for the 
near-term applications of quantum technologies currently under 
development, which in turn depends on a vibrant ecosystem of academic, 
government, and commercial actors. 
  
CENTER FOR DATA INNOVATION 21 
Figure 6: Virtuous cycle for scaling a new technology 
 
 
Unfortunately, U.S. policy is not sufficiently focused on supporting near-
term quantum applications. Recognizing this, QED-C published a report in 
September 2022 pushing for the U.S. government to support public-private 
partnerships (PPPs) that can help accelerate near-term applications 
specifically for quantum computers. The report notes:  
the federal government should consider establishing a PPP or 
leveraging an existing PPP (e.g., QED-C) whose mission is to find 
possible near-term QC applications by facilitating planned interaction 
and cooperation among QC hardware and software experts, 
application domain experts, user communities, and policy and market 
experts. Such a partnership should be organized thematically around a 
significant area of public interest, such as climate and sustainability or 
public health, where there is an emerging critical mass of quantum 
R&D already underway.
62 
Several of these comments echo those the Center for Data Innovation 
made in its 2021 report “Why the United States Needs to Support Near-
Term Quantum Computing Applications.”63 In particular, both reports call 
on the U.S. federal government to establish a program that challenges 
companies to come up with innovative quantum solutions to public sector 
problems. By challenging industry to develop innovative solutions for public 
sector needs from the demand side, the government is offering up U.S. 
cities as successful first customers, thereby increasing market demand for 
nascent near-term quantum computing technologies and enabling 
companies to create competitive advantage in the market. 
Other countries are already pursuing this. The United Kingdom, for 
instance, has established a Commercializing Quantum Technologies 
More 
improvements 
in technology
Growing 
capabilities
Growing 
market for 
technology
More 
investment 
(R&D)
  
CENTER FOR DATA INNOVATION 22 
challenge that provides around £174 million ($214 million) of government 
funding, supported by £390 million ($480 million) in funding from industry, 
for industry-led projects that address four themes of the government’s 
industrial strategy: clean growth, aging society, the future of mobility, and 
artificial intelligence. As of fall 2022, this challenge had provided funding 
for 139 projects led by U.K.-registered businesses.64 
The Canadian government is also focused on commercialization. In 2020, 
it released a request for proposals to develop “quantum computing as-a-
service.”65 The goal of this challenge is for technology providers to make 
quantum computing accessible to domain experts in fields such as finance 
and logistics by creating tools that let them easily express and manipulate 
problems without having to understand much about how quantum 
computing works.66 Such a tool is somewhat analogous to platforms such 
as Microsoft Azure that let businesses develop, test, and run applications 
through Microsoft-managed data centers, thereby insulating them from 
needing to know how to build and manage the platform or underlying 
infrastructure and allowing them to focus on the problem instead. By 
focusing on growing a market for quantum computing technologies, 
Canada is better fueling the commercial interest needed to create a 
snowball effect in investment. The Canadian government also released a 
challenge in 2022 that is “seeking pre-commercial innovative prototypes 
that can be tested in real life settings and address a variety of priorities 
within the Government of Canada.”67 This pilot project gives small and 
medium-sized enterprises the opportunity to sell their innovations directly 
to the government of Canada.68 
Recommendations 
 Congress should direct the Department of Commerce to work with 
the QED-C to review the quantum supply chain and identify risks. 
The United States will need comprehensive innovation and 
competitiveness strategies to spur investments in R&D, 
infrastructure, and skills in order to stay competitive, but 
policymakers cannot formulate effective policies and programs 
without first knowing what the quantum supply chain looks like 
today and how it is likely to develop. To mitigate supply chain 
vulnerabilities, the Department of Commerce should work with 
relevant agencies to track and assess global supply chains for 
critical components, materials, and equipment and submit a report 
reviewing the quantum supply chain to the Assistant to the 
President for National Security Affairs and the Assistant to the 
President for Economic Policy. 
 Congress should direct and fund the recently established 
Directorate for TIP within NSF to establish quantum testbeds for 
use-inspired research. The CHIPS and Science Act charged TIP with 
accelerating the development of key technologies, one of which is 
quantum. By providing funding for a TIP-led program to establish 
  
CENTER FOR DATA INNOVATION 23 
quantum testbeds, policymakers can help ensure quantum 
research is effectively translated into real-world applications. 
Ideally, this program would encourage and support research 
projects that focus on near-term applications and align with 
regional economic development goals by fostering collaboration 
and partnerships between universities, local businesses, and state 
and local governments. 
INTERNATIONAL COLLABORATION ON QUANTUM 
Quantum is emerging in a geopolitical environment. The United States is 
rightly trying to work with like-minded partners to coordinate QIS technology 
development. It is also considering creating export controls to protect QIS.  
Coordinated Quantum Technology Development 
The United States has signed several cooperative bilateral agreements on 
QIS with countries including Australia, Canada, Denmark, France, Finland, 
India, Japan, the Netherlands, Sweden, Switzerland, and the United 
Kingdom. Cooperating with like-minded countries on developing QIS 
technologies is crucial because the expense, complexity, and scale 
required to innovate and manufacture necessary associated materials 
mean no single nation can go it alone. In the face of competition and 
challenges from China, allied cooperation is critical. 
However, the NQIAC has found that inadequate funding is hampering U.S. 
efforts to act on the agreements it has made.
69 The U.S. government will 
need to provide new dedicated research funding to ensure that 
international collaborations can be scientifically productive and place the 
negotiation and implementation of these agreements under the leadership 
of an appropriate agency if it wants to see these agreements bear fruit.
70 
Europe’s approach to implement international quantum collaboration is 
laudable and one the United States should seek to emulate. The EU has 
provided approximately €592,400 ($645,000) for a program called the 
Quantum Flagship International Cooperation on Quantum Technologies 
(InCoQFlag), which aims to identify win-win situations in terms of 
collaborations with countries investing heavily in QIS.71 Led by the French 
Atomic and Alternative Energies Commission, a public research 
organization, the project brings together leading European research 
organizations in “exploring types of collaboration that would help Europe 
structure the best framework for the development of quantum 
technologies, which would benefit economic value creation and the 
research community as a whole.”
72 The end goal is for the project to come 
up with a road map by the end of 2023 for international partnerships that 
the EU can use to set up advantageous partnerships. 
  
CENTER FOR DATA INNOVATION 24 
Export Controls 
Export controls are the rules governing the export of physical items, 
software, technology, and sometimes services to various destinations, 
uses, and users to accomplish certain national security and foreign policy 
(including human rights) objectives.73 In the United States, the Department 
of Commerce’s Bureau of Industry and Security (BIS) regulates the export 
of sensitive technologies, including those related to quantum technology, 
under the Export Administration Regulations.  
BIS is in the middle of developing new export controls to thwart the 
progress of China’s quantum computing ambitions.74 In 2021, the agency 
proposed adding a new Export Control Classification Number (ECCN) in 
order to control quantum computers and related electronic assemblies and 
components, including specified qubit devices and circuits and quantum 
control components and measurement devices.75 This rule also proposes 
controlling certain associated technology and software for the development 
and production of these items by updating the ECCN for “encryption 
commodities, software, and technology” and the ECCN for “software” for 
quantum.  
While BIS has been considering export controls for quantum technologies 
for years—discussions on the topic span both the Biden and Trump 
administrations—concrete progress or a timeline for these efforts remains 
unclear. It seems the government is trying to avoid an approach that 
moves fast and breaks things, which makes sense given the nascent 
nature of the industry and the stifling impact heavy-handed export controls 
could have on domestic growth. 
Recommendations 
 Congress should direct DOE to establish and lead a program that 
invites allied nations to co-invest in quantum moonshots. While the 
United States has made several bilateral quantum agreements to 
facilitate closer collaboration with like-minded partners, the U.S. 
government should specifically target cooperation by enrolling 
allied partners in quantum moonshots with resulting intellectual 
property or technical discoveries shared at levels proportionate to 
mutual investment. DOE can model this program on the EU’s 
InCoQFlag, which aims to identify win-win situations in terms of 
collaborations with countries investing heavily in QIS. 
 Congress should direct NIST to prioritize promoting U.S. 
participation, particularly from U.S. industry stakeholders, in 
international standards fora in the reauthorization of the NQIA. As 
outlined in the White House’s National Standards Strategy for 
Critical and Emerging Technologies published in 2023, it is a 
priority of the U.S. government to “catalyze U.S. attendance in 
standards development in high priority early-stage CET areas, such 
  
CENTER FOR DATA INNOVATION 25 
as quantum information technologies, where U.S. industry is 
nascent but standards work is ongoing.”76 
CONCLUSION 
Many nations, including China, are actively pursuing advancements in 
quantum. Several countries and regions such as the United Kingdom, 
Australia, and EU have launched extensive research initiatives and 
programs aimed at bolstering their positions in quantum—and some of 
these outstrip the United States in scale and scope, making the United 
States’ leadership in quantum far from assured. 
Quantum technologies are not only important for national security, but they 
also have the potential to exert a transformative influence on the economy 
and society. Being at the forefront of this technological frontier is 
strategically crucial for the United States in terms of both its economic and 
societal well-being. The U.S. government should take proactive measures 
immediately to maintain its leadership position. 
 
  
  
CENTER FOR DATA INNOVATION 26 
REFERENCES 
 
1. Patricia Moloney Figliola, “Quantum Information Science: Applications, 
Global Research and Development, and Policy Considerations” 
(Congressional Research Service, November 2018), 
https://crsreports.congress.gov/product/pdf/R/R45409/1. 
2. Ibid. 
3. National Science and Technology Council, National Quantum Initiative 
Supplement to the President’s FY 2022 Budget (Washington, D.C.: National 
Science and Technology Council Subcommittee on Quantum Information 
Science, December 2021), https://www.quantum.gov/wp-
content/uploads/2021/12/NQI-Annual-Report-FY2022.pdf. 
4. National Science and Technology Council, Advancing Quantum Information 
Science: National Challenges and Opportunities (Washington, D.C.: 
Interagency Working Group on Quantum Information Science of the 
Subcommittee on Physical Sciences, July 2016), 
https://obamawhitehouse.archives.gov/sites/default/files/quantum_info_s
ci_report_2016_07_22_final.pdf. 
5. Ibid. 
6. National Science and Technology Council, National Strategic Overview for 
Quantum Information Science (Washington, D.C.: National Science and 
Technology Council Subcommittee on Quantum Information Science, 
September 2018), https://www.quantum.gov/wp-
content/uploads/2020/10/2018_NSTC_National_Strategic_Overview_QIS.
pdf. 
7. Santanu Basu and Jacqueline A. Basu, “Breaking down the 2022 CHIPS and 
Science Act,” QED-C website, December 14, 2022, 
https://quantumconsortium.org/blog/breaking-down-the-2022-chips-and-
science-act/. 
8. “About TIP,” NSF, accessed March 2, 2023, https://beta.nsf.gov/tip/about-
tip. 
9. “About the National Quantum Initiative,” Quantum.gov, accessed March 4, 
2023, https://www.quantum.gov/about/. 
10. National Science and Technology Council, National Quantum Initiative 
Supplement to the President’s FY 2023 Budget (Washington, D.C.: National 
Science and Technology Council Subcommittee on Quantum Information 
Science, January 2023), https://www.quantum.gov/wp-
content/uploads/2023/01/NQI-Annual-Report-FY2023.pdf 
11. Ibid. 
12. Ibid. 
13. “National Quantum Initiative Advisory Committee: Notice of open meeting,” 
Federal Register, March 2, 2023 (comments were made by NQIAC members 
in the public meeting on March 24, 2023), 
https://www.federalregister.gov/documents/2023/03/06/2023-
04518/national-quantum-initiative-advisory-committee. 
14. Donald Stokes, Pasteur's Quadrant: Basic Science and Technological 
Innovation (Washington, D.C.: Brookings Institution Press, 1997). 
  
CENTER FOR DATA INNOVATION 27 
 
15. Mark Zachary Taylor, The Politics of Innovation: Why Some Countries Are 
Better Than Others at Science and Technology (New York: Oxford University 
Press, 2011), 83. 
16. UK Department for Science, Innovation & Technology, “National Quantum 
Strategy,” March 2023, 
https://assets.publishing.service.gov.uk/government/uploads/system/uplo
ads/attachment_data/file/1142942/national_quantum_strategy.pdf. 
17. Mateusz Masiowski et al., “Quantum computing funding remains strong, but 
talent gap raises concern,” McKinsey blog, June 15, 2022, 
https://www.mckinsey.com/capabilities/mckinsey-digital/our-
insights/quantum-computing-funding-remains-strong-but-talent-gap-raises-
concern. 
18. National Quantum Initiative Advisory Committee (NQIAC) Written Comments 
from Energy Sciences Coalition, March 14, 2023, 
https://www.quantum.gov/wp-content/uploads/2023/03/NQIAC-2023-03-
24-Written-Comments.pdf. 
19. Matt Hourihan, Mark Muro, and Melissa Roberts Chapman, “The bold vision 
of the CHIPS and Science Act isn’t getting the funding it needs” (Brookings, 
May 17, 2023), https://www.brookings.edu/articles/the-bold-vision-of-the-
chips-and-science-act-isnt-getting-the-funding-it-needs/. 
20. Ibid. 
21. Center for Data Innovation panel (17:00), “What Should Congress Include in 
The Next National Quantum Initiative Act?” May 2, 2023, 
https://datainnovation.org/2023/05/what-should-congress-include-in-the-
next-national-quantum-initiative-act/. 
22. Ibid. 
23. NSF Workshop, “Quantum Information Science: An Emerging Field of 
Interdisciplinary Research and Education in Science and Engineering” 
(report published as part of a workshop conducted October 28-29, 1999), 
https://www.nsf.gov/pubs/2000/nsf00101/nsf00101.htm. 
24. Patricia Moloney Figliola, “Federal Quantum Information Science: An 
Overview” (Congressional Research Service, July 2018), 
https://sgp.fas.org/crs/misc/IF10872.pdf. 
25. “The NSF Quantum Leap Challenge Institute for Robust Quantum 
Simulation,” University of Maryland, accessed February 2, 2023, 
https://rqs.umd.edu/. 
26. “Quantum Information Science and Engineering Research at NSF,” NSF, 
accessed March 2, 2023, 
https://www.nsf.gov/mps/quantum/quantum_research_at_nsf.jsp. 
27. National Science Board, National Science Foundation (NSF), Science and 
Engineering Indicators 2022: The State of U.S. Science and Engineering 
(Alexandria, VA: NSF, January 2022), 
https://ncses.nsf.gov/pubs/nsb20221. 
28. “National Quantum Initiative Advisory Committee: Notice of open meeting,” 
Federal Register (comments were made by NQIAC members in the public 
meeting on March 24, 2023). 
29. Ibid. 
  
CENTER FOR DATA INNOVATION 28 
 
30. “UC Santa Barbara Quantum Foundry,” accessed March 2, 2023, 
https://quantumfoundry.ucsb.edu/; “MonArk Quantum Foundry,” accessed 
March 2, 2023, https://www.monarkfoundry.org/. 
31. Department of Energy (DOE) Basic Energy Sciences Advisory Committee on 
International Benchmarking, Critical Research Frontiers and Strategies 
Critical Research Frontiers and Strategies (Washington, D.C.: DOE, 2021), 
https://science.osti.gov/-
/media/bes/pdf/reports/2021/International_Benchmarking-Report.pdf.   
32. Basu and Basu, “Breaking down the 2022 CHIPS and Science Act.” 
33. “National Q-12 Education Partnership: About,” accessed February 28, 2023, 
https://q12education.org/about. 
34. “About QISE Education,” accessed February 28, 2023, 
https://q12education.org/learning-materials-framework. 
35. “Incorporating QIS into High School Computer Science,” accessed February 
28, 2023, https://q12education.org/learning-materials-framework/cs. 
36. Katie Hendrickson et al., “2022 State of computer science education: 
Accelerating action through advocacy,” CODE website, 
https://advocacy.code.org/stateofcs. 
37. Sasha Jones, “STEM Instruction: How Much There Is and Who Gets It,” 
EdWeek, January 08, 2019, https://www.edweek.org/teaching-
learning/stem-instruction-how-much-there-is-and-who-gets-it/2019/01. 
38. Robert D. Atkinson, Mark Muro, and Jacob Whiton, “The Case for Growth 
Centers: How to spread tech innovation across America” (ITIF and 
Brookings, December 2019), https://www2.itif.org/2019-growth-
centers.pdf. 
39. Kirsten Stadermann, “Connecting Secondary School Quantum Physics and 
Nature of Science,” International Journal of Science Education (March 
2020), 42:6, 997-1016, DOI: 10.1080/09500693.2020.1745926. 
40. “Trends in the Netherlands 2017,” accessed March 1, 2023, 
https://longreads.cbs.nl/trends17-eng/society/figures/education/. 
41. Stadermann, “Connecting Secondary School Quantum Physics and Nature 
of Science.” 
42. “QubitbyQubit: About Us,” accessed March 1, 2023, 
https://www.qubitbyqubit.org/about. 
43. Ibid. 
44. “Colorado University Quantum Engineering – Minor,” University of Colorado, 
accessed March 1, 2023, 
https://catalog.colorado.edu/undergraduate/colleges-schools/engineering-
applied-science/programs-study/electrical-computer-energy-
engineering/quantum-engineering-minor/; “Quantum Undergraduate 
Research Experience at CHTM (QU-REACH),” University of New Mexico, 
accessed March 1, 2023, https://qureach.unm.edu/. 
45. “Harvard University: Quantum Science and Engineering,” Harvard University, 
accessed March 1, 2023, https://gsas.harvard.edu/program/quantum-
science-and-engineering. 
46. Nicholas Hillman, “Why Performance-Based College Funding Doesn’t Work,” 
The Century Foundation website, May 25, 2016, 
  
CENTER FOR DATA INNOVATION 29 
 
https://tcf.org/content/report/why-performance-based-college-funding-
doesnt-work/. 
47. Laurel M. Pritchard et al., “Implementing Curricular Change Across the 
University: Challenges and Successes,” International Journal of Teaching 
and Learning in Higher Education (2021), Volume 33, Number 1, 34–47, 
https://files.eric.ed.gov/fulltext/EJ1338432.pdf. 
48. Stephanie Saul, “College Enrollment Drops, Even as the Pandemic’s Effects 
Ebb,” The New York Times, May 26, 2022, 
https://www.nytimes.com/2022/05/26/us/college-enrollment.html. 
49. Ciaran Hughes et al., “Assessing the Needs of the Quantum Industry,” IEEE 
Transactions on Education (November 2022), 
https://www.researchgate.net/publication/359182844_Assessing_the_Ne
eds_of_the_Quantum_Industry. 
50. Ibid. 
51. National Science and Technology Council, Quantum Information Science 
and Technology Workforce Development National Strategic Plan 
(Washington, D.C.: National Science and Technology Council Subcommittee 
on Quantum Information Science, February 2022), 
https://www.quantum.gov/wp-content/uploads/2022/02/QIST-Natl-
Workforce-Plan.pdf. 
52. Ibid. 
53. “National Quantum Initiative Advisory Committee: Notice of open meeting,” 
Federal Register (comments were made by NQIAC members in the public 
meeting on March 24, 2023). 
54. Cade Metz, “The Next Tech Talent Shortage: Quantum Computing 
Researchers,” The New York Times, Oct. 21, 2018, 
https://www.nytimes.com/2018/10/21/technology/quantum-computing-
jobs-immigration-visas.html. 
55. John Feeley and Dick Burke, “Canada regularly poaches US immigrant tech 
talent: Mexico could be next,” The Hill, February 5, 2023, 
https://thehill.com/opinion/immigration/3843282-canada-regularly-
poaches-us-immigrant-tech-talent-mexico-could-be-next/. 
56. Bob Sorensen and Tom Sorensen, “Challenges and opportunities for 
securing a robust US quantum computing supply chain” (Hyperion Research, 
June 2022), https://quantumconsortium.org/quantum-computing-supply-
chain-issues/. 
57. Karen Howard, “The Quantum Leap Hinges on Worker Skills and Supply 
Chain Limits” (Government Accountability Office, October 2021), 
https://www.gao.gov/blog/quantum-leap-hinges-worker-skills-and-supply-
chain-limits. 
58. Lara Seligman, “China Dominates the Rare Earths Market. This U.S. Mine Is 
Trying to Change That,” Politico, December 14, 2022, 
https://www.politico.com/news/magazine/2022/12/14/rare-earth-mines-
00071102. 
59.  Sorensen and Sorensen, “Challenges and opportunities for securing a 
robust US quantum computing supply chain.” 
60. Ibid. 
  
CENTER FOR DATA INNOVATION 30 
 
61. National Academies of Sciences, Engineering, and Medicine (NASEM) 2019, 
Quantum Computing: Progress and Prospects (Washington, D.C.: The 
National Academies Press), 32, https://doi.org/10.17226/25196. 
62.  Quantum Economic Development Consortium (QED-C), “Public Private 
Partnerships in Quantum Computing: The Potential for Accelerating Near-
Term Quantum Applications,” September 2022, 
https://quantumconsortium.org/ppp22/. 
63. Hodan Omaar, “Why the United States Needs to Support Near-Term 
Quantum Computing Applications” (Center for Data Innovation, April 2021), 
https://www2.datainnovation.org/2021-quantum-computing.pdf. 
64. UK Research and Innovation (UKRI), UK Quantum Technologies Challenge 
(London: UKRI, 2023), https://www.ukri.org/wp-
content/uploads/2023/01/UKRI-03012023-
Quantum_projects_brochure2022.pdf. 
65. Government of Canada, “Quantum Computing-as-a-Service: Tender Notice 
RFP,” Public Works and Government Services Canada website, September 
2020, https://buyandsell.gc.ca/procurement-data/tender-notice/PW-20-
00931408. 
66. Government of Canada, “Quantum Computing As-A-Service – Questions and 
Answers,” Public Works and Government Services Canada website, 
accessed March 25, 2023, 
https://buyandsell.gc.ca/cds/public/2020/12/18/b9c8af6a8a1d7d435f9
b2c93007c8367/amendment_2_-_quantum_computing_as-a-service_-
_questions_and_answers.pdf. 
67. “Government of Canada: Quantum computing,” last modified July 12, 2022, 
https://ised-isde.canada.ca/site/innovative-solutions-canada/en/quantum-
computing. 
68. Ibid. 
69. National Quantum Initiative Advisory Committee (NQIAC) Written Comments 
from Energy Sciences Coalition, March 14, 2023, 
https://www.quantum.gov/wp-content/uploads/2023/03/NQIAC-2023-03-
24-Written-Comments.pdf. 
70. “National Quantum Initiative Advisory Committee: Notice of open meeting,” 
Federal Register (comments were made by NQIAC members in the public 
meeting on March 24, 2023). 
71. “InCoQFlag - International Cooperation on Quantum Technologies,” 
accessed March 3, 2023, https://qt.eu/about-quantum-
flagship/projects/incoqflag/. 
72. Ibid. 
73. Stephen Ezell, “An Allied Approach to Semiconductor Leadership” (ITIF, 
September 2020), 
https://itif.org/publications/2020/09/17/alliedapproach-semiconductor-
leadership. 
74. Joe Williams and Max A. Cherney, “Biden’s push for new quantum controls 
has one big problem: Nobody knows where to draw the line,” Protocol, 
November 2, 2022, https://www.protocol.com/enterprise/quantum-
computing-export-controls. 
75. Ibid. 
  
CENTER FOR DATA INNOVATION 31 
 
75. White House, United States Government National Standards for Critical and 
Emerging Technologies, May 2023, https://www.whitehouse.gov/wp-
content/uploads/2023/05/US-Gov-National-Standards-Strategy-2023.pdf. 
 
  
CENTER FOR DATA INNOVATION 32 
ABOUT THE AUTHOR 
Hodan Omaar is a senior policy analyst at the Center for Data 
Innovation. Previously, she worked as a senior consultant on 
technology and risk management in London and as an economist at a 
blockchain start-up in Berlin. She has an MA in Economics and 
Mathematics from the University of Edinburgh. 
ABOUT THE CENTER FOR DATA INNOVATION 
The Center for Data Innovation studies the intersection of data, 
technology, and public policy. With staff in Washington, London, and 
Brussels, the Center formulates and promotes pragmatic public 
policies designed to maximize the benefits of data-driven innovation in 
the public and private sectors. It educates policymakers and the public 
about the opportunities and challenges associated with data, as well 
as technology trends such as open data, artificial intelligence, and the 
Internet of Things. The Center is part of the Information Technology 
and Innovation Foundation (ITIF), a nonprofit, nonpartisan think tank. 
  
Contact: info@datainnovation.org 
datainnovation.org",0.00,2025-06-13 17:48:38.717065,2025-06-14 02:23:18.757749,Framework,https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2022/05/04/national-security-memorandum-on-promoting-united-states-leadership-in-quantum-computing-while-mitigating-risks-to-vulnerable-cryptographic-systems/,"{""author"": ""Hodan Omaar"", ""verified"": true, ""extraction_method"": ""URL_VERIFIED""}",White House,2023-10-10,This framework establishes a framework for emerging technology governance governance and implementation. It provides national guidance for National Institute of Standards and Technology and related organizations.,,,,,Unknown,0.0,,Quantum,True,valid,2025-06-14 03:57:35.618983,
48,National Security Memorandum on Promoting United States Leadership in Quantum Computing While Mitigating Risks to Vulnerable Cryptographic Systems | The White House,"# National Security Memorandum on Promoting United States Leadership in Quantum Computing While Mitigating Risks to Vulnerable Cryptographic Systems

NATIONAL SECURITY MEMORANDUM/NSM-10

MEMORANDUM FOR THE VICE PRESIDENT

THE SECRETARY OF STATE

THE SECRETARY OF THE TREASURY

THE SECRETARY OF DEFENSE

THE ATTORNEY GENERAL

THE SECRETARY OF COMMERCE

THE SECRETARY OF ENERGY

THE SECRETARY OF HOMELAND SECURITY

Â Â Â Â Â Â Â Â Â Â Â Â Â THE ASSISTANT TO THE PRESIDENT AND CHIEF OF STAFF

Â Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE OFFICE OF MANAGEMENT BUDGET

THE DIRECTOR OF NATIONAL INTELLIGENCE

THE DIRECTOR OF THE CENTRAL INTELLIGENCE AGENCY

THE ASSISTANT TO THE PRESIDENT FOR NATIONAL

SECURITY AFFAIRS

THE COUNSEL TO THE PRESIDENT

THE ASSISTANT TO THE PRESIDENT FOR ECONOMIC

POLICY AND DIRECTOR OF THE NATIONAL ECONOMIC

COUNCIL

THE DIRECTOR OF THE OFFICE OF SCIENCE AND

TECHNOLOGY POLICY

THE NATIONAL CYBER DIRECTOR

THE CHAIRMAN OF THE JOINT CHIEFS OF STAFF

THE DIRECTOR OF THE FEDERAL BUREAU OF

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â INVESTIGATION

Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE NATIONAL SECURITY AGENCY

Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE NATIONAL INSTITUTE OF

Â Â Â Â Â Â Â Â Â Â Â Â STANDARDS AND TECHNOLOGY

Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE CYBERSECURITY AND

Â Â Â Â Â Â Â Â Â Â Â Â INFRASTRUCTURE SECURITY AGENCY

SUBJECT: Promoting United States Leadership in Quantum

Computing While Mitigating Risks to Vulnerable

Cryptographic Systems

This memorandum outlines my Administration’s policies and initiatives related to quantum computing. It identifies key steps needed to maintain the Nation’s competitive advantage in quantum information science (QIS), while mitigating the risks of quantum computers to the Nation’s cyber, economic, and national security. It directs specific actions for agencies to take as the United States begins the multi-year process of migrating vulnerable computer systems to quantum-resistant cryptography. A classified annex to this memorandum addresses sensitive national security issues.

Section 1. Policy. (a) Quantum computers hold the potential to drive innovations across the American economy, from fields as diverse as materials science and pharmaceuticals to finance and energy. While the full range of applications of quantum computers is still unknown, it is nevertheless clear that America’s continued technological and scientific leadership will depend, at least in part, on the Nation’s ability to maintain a competitive advantage in quantum computing and QIS.

(b) Yet alongside its potential benefits, quantum computing also poses significant risks to the economic and national security of the United States. Most notably, a quantum computer of sufficient size and sophistication — also known as a cryptanalytically relevant quantum computer (CRQC) — will be capable of breaking much of the public-key cryptography used on digital systems across the United States and around the world. When it becomes available, a CRQC could jeopardize civilian and military communications, undermine supervisory and control systems for critical infrastructure, and defeat security protocols for most Internet-based financial transactions.

(c) In order to balance the competing opportunities and risks of quantum computers, it is the policy of my Administration: (1) to maintain United States leadership in QIS, through continued investment, partnerships, and a balanced approach to technology promotion and protection; and (2) to mitigate the threat of CRQCs through a timely and equitable transition of the Nation’s cryptographic systems to interoperable quantumâresistant cryptography.

(d) Additional guidance and directives may be required in the future as quantum computing technologies and their associated risks mature.

Sec. 2. Promoting United States Leadership. (a) The United States must pursue a whole-of-government and wholeâofâsociety strategy to harness the economic and scientific benefits of QIS, and the security enhancements provided by quantum-resistant cryptography. This strategy will require a coordinated, proactive approach to QIS research and development (R&D), an expansion of education and workforce programs, and a focus on developing and strengthening partnerships with industry, academic institutions, allies, and like-minded nations.

(b) The United States must seek to encourage transformative and fundamental scientific discoveries through investments in core QIS research programs. Investments should target the discovery of new quantum applications, new approaches to quantum-component manufacturing, and advances in quantumâenabling technologies, such as photonics, nanofabrication, and cryogenic and semiconductor systems.

(c) The United States must seek to foster the next generation of scientists and engineers with quantum-relevant skill sets, including those relevant to quantum-resistant cryptography. Education in QIS and related cybersecurity principles should be incorporated into academic curricula at all levels of schooling to support the growth of a diverse domestic workforce. Furthermore, it is vital that we attract and retain talent and encourage career opportunities that keep quantum experts employed domestically.

(d) To promote the development of quantum technology and the effective deployment of quantum-resistant cryptography, the United States must establish partnerships with industry; academia; and State, local, Tribal, and territorial (SLTT) governments. These partnerships should advance joint R&D initiatives and streamline mechanisms for technology transfer between industry and government.

(e) The United States must promote professional and academic collaborations with overseas allies and partners. This international engagement is essential for identifying and following global QIS trends and for harmonizing quantum security and protection programs.

(f) In support of these goals, within 90 days of the date of this memorandum, agencies that fund research in, develop, or acquire quantum computers shall coordinate with the Director of the Office of Science and Technology Policy to ensure a coherent national strategy for QIS promotion and technology protection, including for workforce issues. To facilitate this coordination, all such agencies shall identify a liaison to the National Quantum Coordination Office to share information and best practices, consistent with section 102(b)(3) of the National Quantum Initiative Act (Public Law 115-368) and section 6606 of the National Defense Authorization Act for Fiscal Year 2022 (Public Law 117-81). All coordination efforts shall be undertaken with appropriate protections for sensitive and classified information and intelligence sources and methods.

Sec. 3. Mitigating the Risks to Encryption. (a) Any digital system that uses existing public standards for publicâkey cryptography, or that is planning to transition to such cryptography, could be vulnerable to an attack by a CRQC. To mitigate this risk, the United States must prioritize the timely and equitable transition of cryptographic systems to quantum-resistant cryptography, with the goal of mitigating as much of the quantum risk as is feasible by 2035. Currently, the Director of the National Institute of Standards and Technology (NIST) and the Director of the National Security Agency (NSA), in their capacity as the National Manager for National Security Systems (National Manager), are each developing technical standards for quantumâresistant cryptography for their respective jurisdictions. The first sets of these standards are expected to be released publicly by 2024.

(b) Central to this migration effort will be an emphasis on cryptographic agility, both to reduce the time required to transition and to allow for seamless updates for future cryptographic standards. This effort is an imperative across all sectors of the United States economy, from government to critical infrastructure, commercial services to cloud providers, and everywhere else that vulnerable public-key cryptography is used.

(c) Consistent with these goals:

(i) Within 90 days of the date of this memorandum, the Secretary of Commerce, through the Director of NIST, shall initiate an open working group with industry, including critical infrastructure owners and operators, and other stakeholders, as determined by the Director of NIST, to further advance adoption of quantum-resistant cryptography. This working group shall identify needed tools and data sets, and other considerations to inform the development by NIST of guidance and best practices to assist with quantumâresistant cryptography planning and prioritization. Findings of this working group shall be provided, on an ongoing basis, to the Director of the Office of Management and Budget (OMB), the Assistant to the President for National Security Affairs (APNSA), and the National Cyber Director to incorporate into planning efforts.

(ii) Within 90 days of the date of this memorandum, the Secretary of Commerce, through the Director of NIST, shall establish a “Migration to Post-Quantum Cryptography Project” at the National Cybersecurity Center of Excellence to work with the private sector to address cybersecurity challenges posed by the transition to quantum-resistant cryptography. This project shall develop programs for discovery and remediation of any system that does not use quantum-resistant cryptography or that remains dependent on vulnerable systems.

(iii) Within 180 days of the date of this memorandum, and annually thereafter, the Secretary of Homeland Security, through the Director of the Cybersecurity and Infrastructure Security Agency (CISA), and in coordination with Sector Risk Management Agencies, shall engage with critical infrastructure and SLTT partners regarding the risks posed by quantum computers, and shall provide an annual report to the Director of OMB, the APNSA, and the National Cyber Director that includes recommendations for accelerating those entities’ migration to quantum-resistant cryptography.

(iv) Within 180 days of the date of this memorandum, and on an ongoing basis, the Director of OMB, in consultation with the Director of CISA, the Director of NIST, the National Cyber Director, and the Director of NSA, shall establish requirements for inventorying all currently deployed cryptographic systems, excluding National Security Systems (NSS). These requirements shall include a list of key information technology (IT) assets to prioritize, interim benchmarks, and a common (and preferably automated) assessment process for evaluating progress on quantum-resistant cryptographic migration in IT systems.

(v) Within 1 year of the date of this memorandum, and on an annual basis thereafter, the heads of all Federal Civilian Executive Branch (FCEB) Agencies shall deliver to the Director of CISA and the National Cyber Director an inventory of their IT systems that remain vulnerable to CRQCs, with a particular focus on High Value Assets and High Impact Systems. Inventories should include current cryptographic methods used on IT systems, including system administrator protocols, non-security software and firmware that require upgraded digital signatures, and information on other key assets.

(vi) By October 18, 2023, and on an annual basis thereafter, the National Cyber Director shall, based on the inventories described in subsection 3(c)(v) of this memorandum and in coordination with the Director of CISA and the Director of NIST, deliver a status report to the APNSA and the Director of OMB on progress made by FCEB Agencies on their migration of non-NSS IT systems to quantum-resistant cryptography. This status report shall include an assessment of the funding necessary to secure vulnerable IT systems from the threat posed by adversarial access to quantum computers, a description and analysis of ongoing coordination efforts, and a strategy and timeline for meeting proposed milestones.

(vii) Within 90 days of the release of the first set of NIST standards for quantum-resistant cryptography referenced in subsection 3(a) of this memorandum, and on an annual basis thereafter, as needed, the Secretary of Commerce, through the Director of NIST, shall release a proposed timeline for the deprecation of quantum-vulnerable cryptography in standards, with the goal of moving the maximum number of systems off quantum-vulnerable cryptography within a decade of the publication of the initial set of standards. The Director of NIST shall work with the appropriate technical standards bodies to encourage interoperability of commercial cryptographic approaches.

(viii) Within 1 year of the release of the first set of NIST standards for quantum-resistant cryptography referenced in subsection 3(a) of this memorandum, the Director of OMB, in coordination with the Director of CISA and the Director of NIST, shall issue a policy memorandum requiring FCEB Agencies to develop a plan to upgrade their non-NSS IT systems to quantum-resistant cryptography. These plans shall be expeditiously developed and be designed to address the most significant risks first. The Director of OMB shall work with the head of each FCEB Agency to estimate the costs to upgrade vulnerable systems beyond already planned expenditures, ensure that each plan is coordinated and shared among relevant agencies to assess interoperability between solutions, and coordinate with the National Cyber Director to ensure plans are updated accordingly.

(ix) Until the release of the first set of NIST standards for quantum-resistant cryptography referenced in subsection 3(a) of this memorandum, the heads of FCEB Agencies shall not procure any commercial quantum-resistant cryptographic solutions for use in IT systems supporting enterprise and mission operations. However, to assist with anticipating potential compatibility issues, the heads of such FCEB Agencies should conduct tests of commercial solutions that have implemented pre-standardized quantum-resistant cryptographic algorithms. These tests will help identify interoperability or performance issues that may occur in Federal environments at an early stage and will contribute to the mitigation of those issues. The heads of such FCEB Agencies should continue to implement and, where needed, upgrade existing cryptographic implementations, but should transition to quantum-resistant cryptography only once the first set of NIST standards for quantum-resistant cryptography is complete and implemented in commercial products. Conformance with international standards should be encouraged, and may be required for interoperability.

(x) Within 1 year of the date of this memorandum, and annually thereafter, the Director of NSA, serving in its capacity as the National Manager, in consultation with the Secretary of Defense and the Director of National Intelligence, shall provide guidance on quantum-resistant cryptography migration, implementation, and oversight for NSS. This guidance shall be consistent with National Security Memorandum/NSM-8 (Improving the Cybersecurity of National Security, Department of Defense, and Intelligence Community Systems). The National Manager shall share best practices and lessons learned with the Director of OMB and the National Cyber Director, as appropriate.

(xi) Within 1 year of the date of this memorandum, and on an ongoing basis, and consistent with section 1 of NSM-8, the heads of agencies operating NSS shall identify and document all instances where quantum-vulnerable cryptography is used by NSS and shall provide this information to the National Manager.

(xii) Within 180 days of issuance by the National Manager of its standards on quantum-resistant cryptography referenced in section 3(a) of this memorandum, and annually thereafter, the National Manager shall release an official timeline for the deprecation of vulnerable cryptography in NSS, until the migration to quantum-resistant cryptography is completed.

(xiii) Within 1 year of issuance by the National Manager of its standards on quantum-resistant cryptography for referenced in subsection 3(a) of this memorandum, and annually thereafter, the heads of agencies operating or maintaining NSS shall submit to the National Manager, and, as appropriate, the Department of Defense Chief Information Officer or the Intelligence Community Chief Information Officer, depending on their respective jurisdictions, an initial plan to transition to quantumâresistant cryptography in all NSS. These plans shall be updated annually and shall include relevant milestones, schedules, authorities, impediments, funding requirements, and exceptions authorized by the head of the agency in accordance with section 3 of NSM-8 and guidance from the National Manager.

(xiv) By December 31, 2023, agencies maintaining NSS shall implement symmetric-key protections (e.g., High Assurance Internet Protocol Encryptor (HAIPE) exclusion keys or VPN symmetric key solutions) to provide additional protection for quantum-vulnerable key exchanges, where appropriate and in consultation with the National Manager. Implementation should seek to avoid interference with interoperability or other cryptographic modernization efforts.

(xv) By December 31, 2023, the Secretary of Defense shall deliver to the APNSA and the Director of OMB an assessment of the risks of quantum computing to the defense industrial base and to defense supply chains, along with a plan to engage with key commercial entities to upgrade their IT systems to achieve quantum resistance.

Sec. 4. Protecting United States Technology. (a) In addition to promoting quantum leadership and mitigating the risks of CRQCs, the United States Government must work to safeguard relevant quantum R&D and intellectual property (IP) and to protect relevant enabling technologies and materials. Protection mechanisms will vary, but may include counterintelligence measures, well-targeted export controls, and campaigns to educate industry and academia on the threat of cybercrime and IP theft.

(b) All agencies responsible for either promoting or protecting QIS and related technologies should understand the security implications of adversarial use and consider those security implications when implementing new policies, programs, and projects.

(c) The United States should ensure the protection of U.S.âdeveloped quantum technologies from theft by our adversaries. This will require campaigns to educate industry, academia, and SLTT partners on the threat of IP theft and on the importance of strong compliance, insider threat detection, and cybersecurity programs for quantum technologies. As appropriate, Federal law enforcement agencies and other relevant agencies should investigate and prosecute actors who engage in the theft of quantum trade secrets or who violate United States export control laws. To support efforts to safeguard sensitive information, Federal law enforcement agencies should exchange relevant threat information with agencies responsible for developing and promoting quantum technologies.

(d) Consistent with these goals, by December 31, 2022, the heads of agencies that fund research in, develop, or acquire quantum computers or related QIS technologies shall develop comprehensive technology protection plans to safeguard QIS R&D, acquisition, and user access. Plans shall be coordinated across agencies, including with Federal law enforcement, to safeguard quantum computing R&D and IP, acquisition, and user access. These plans shall be updated annually and provided to the APNSA, the Director of OMB, and the Co-Chairs of the National Science and Technology Council Subcommittee on Economic and Security Implications of Quantum Science.

Sec. 5. Definitions. For purposes of this memorandum:

(a) the term “agency” has the meaning ascribed to it under 44 U.S.C. 3502;

(b) the term “critical infrastructure” means systems and assets, whether physical or virtual, so vital to the United States that their incapacitation or destruction would have a debilitating effect on the Nation’s security, economy, public health and safety, or any combination thereof;

(c) the term “cryptographic agility” means a design feature that enables future updates to cryptographic algorithms and standards without the need to modify or replace the surrounding infrastructure;

(d) the term “cryptanalytically relevant quantum computer” or “CRQC” means a quantum computer capable of undermining current public-key cryptographic algorithms;

(e) the term “Federal Civilian Executive Branch Agency” or “FCEB Agency” means any agency except the Department of Defense or agencies in the Intelligence Community;

(f) the term “high value asset” means information or an information system that is so critical to an organization that the loss or corruption of this information, or loss of access to the system, would have serious impacts on the organization’s ability to perform its mission or conduct business;

(g) the term “high impact system” means an information system in which at least one security objective (i.e., confidentiality, integrity, or availability) is assigned a Federal Information Processing Standards (FIPS) 199 potential impact value of “high”;

(h) the term “information technology” or “IT” has the meaning ascribed to it under 44 U.S.C. 3502;

(i) the term “National Security Systems” or “NSS” has the meaning ascribed to it in 44 U.S.C 3552(b)(6) and shall also include other Department of Defense and Intelligence Community systems, as described in 44 U.S.C. 3553(e)(2) and 44 U.S.C. 3553(e)(3);

(j) the term “quantum computer” means a computer utilizing the collective properties of quantum states, such as superposition, interference and entanglement, to perform calculations. The foundations in quantum physics give a quantum computer the ability to solve a subset of hard mathematical problems at a much faster rate than a classical (i.e., nonâquantum) computer;

(k) the term “quantum information sciences” or “QIS” has the meaning ascribed to it under 15 U.S.C. 8801(6) and means the study and application of the laws of quantum physics for the storage, transmission, manipulation, computing, or measurement of information; and

(l) the term “quantum-resistant cryptography” means those cryptographic algorithms or methods that are assessed not to be specifically vulnerable to attack by either a CRQC or classical computer. This is also referred to as post-quantum cryptography.

Sec. 6. General Provisions. (a) Nothing in this memorandum shall be construed to impair or otherwise affect:

(i) the authority granted by law to an executive department or agency, or the head thereof, to include the protection of intelligence sources and methods; or

(ii) the functions of the Director of OMB relating to budgetary, administrative, or legislative proposals.

(b) This memorandum shall be implemented consistent with applicable law and subject to the availability of appropriations.

(c) This memorandum shall also be implemented without impeding the conduct or support of intelligence activities, and all implementation measures shall be designed to be consistent with appropriate protections for sensitive information and intelligence sources and methods.

(d) This memorandum is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person.

JOSEPH R. BIDEN JR.","# National Security Memorandum on Promoting United States Leadership in Quantum Computing While Mitigating Risks to Vulnerable Cryptographic Systems

NATIONAL SECURITY MEMORANDUM/NSM-10

MEMORANDUM FOR THE VICE PRESIDENT

THE SECRETARY OF STATE

THE SECRETARY OF THE TREASURY

THE SECRETARY OF DEFENSE

THE ATTORNEY GENERAL

THE SECRETARY OF COMMERCE

THE SECRETARY OF ENERGY

THE SECRETARY OF HOMELAND SECURITY

Â Â Â Â Â Â Â Â Â Â Â Â Â THE ASSISTANT TO THE PRESIDENT AND CHIEF OF STAFF

Â Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE OFFICE OF MANAGEMENT BUDGET

THE DIRECTOR OF NATIONAL INTELLIGENCE

THE DIRECTOR OF THE CENTRAL INTELLIGENCE AGENCY

THE ASSISTANT TO THE PRESIDENT FOR NATIONAL

SECURITY AFFAIRS

THE COUNSEL TO THE PRESIDENT

THE ASSISTANT TO THE PRESIDENT FOR ECONOMIC

POLICY AND DIRECTOR OF THE NATIONAL ECONOMIC

COUNCIL

THE DIRECTOR OF THE OFFICE OF SCIENCE AND

TECHNOLOGY POLICY

THE NATIONAL CYBER DIRECTOR

THE CHAIRMAN OF THE JOINT CHIEFS OF STAFF

THE DIRECTOR OF THE FEDERAL BUREAU OF

Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â INVESTIGATION

Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE NATIONAL SECURITY AGENCY

Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE NATIONAL INSTITUTE OF

Â Â Â Â Â Â Â Â Â Â Â Â STANDARDS AND TECHNOLOGY

Â Â Â Â Â Â Â Â Â Â Â Â THE DIRECTOR OF THE CYBERSECURITY AND

Â Â Â Â Â Â Â Â Â Â Â Â INFRASTRUCTURE SECURITY AGENCY

SUBJECT: Promoting United States Leadership in Quantum

Computing While Mitigating Risks to Vulnerable

Cryptographic Systems

This memorandum outlines my Administration’s policies and initiatives related to quantum computing. It identifies key steps needed to maintain the Nation’s competitive advantage in quantum information science (QIS), while mitigating the risks of quantum computers to the Nation’s cyber, economic, and national security. It directs specific actions for agencies to take as the United States begins the multi-year process of migrating vulnerable computer systems to quantum-resistant cryptography. A classified annex to this memorandum addresses sensitive national security issues.

Section 1. Policy. (a) Quantum computers hold the potential to drive innovations across the American economy, from fields as diverse as materials science and pharmaceuticals to finance and energy. While the full range of applications of quantum computers is still unknown, it is nevertheless clear that America’s continued technological and scientific leadership will depend, at least in part, on the Nation’s ability to maintain a competitive advantage in quantum computing and QIS.

(b) Yet alongside its potential benefits, quantum computing also poses significant risks to the economic and national security of the United States. Most notably, a quantum computer of sufficient size and sophistication — also known as a cryptanalytically relevant quantum computer (CRQC) — will be capable of breaking much of the public-key cryptography used on digital systems across the United States and around the world. When it becomes available, a CRQC could jeopardize civilian and military communications, undermine supervisory and control systems for critical infrastructure, and defeat security protocols for most Internet-based financial transactions.

(c) In order to balance the competing opportunities and risks of quantum computers, it is the policy of my Administration: (1) to maintain United States leadership in QIS, through continued investment, partnerships, and a balanced approach to technology promotion and protection; and (2) to mitigate the threat of CRQCs through a timely and equitable transition of the Nation’s cryptographic systems to interoperable quantumâresistant cryptography.

(d) Additional guidance and directives may be required in the future as quantum computing technologies and their associated risks mature.

Sec. 2. Promoting United States Leadership. (a) The United States must pursue a whole-of-government and wholeâofâsociety strategy to harness the economic and scientific benefits of QIS, and the security enhancements provided by quantum-resistant cryptography. This strategy will require a coordinated, proactive approach to QIS research and development (R&D), an expansion of education and workforce programs, and a focus on developing and strengthening partnerships with industry, academic institutions, allies, and like-minded nations.

(b) The United States must seek to encourage transformative and fundamental scientific discoveries through investments in core QIS research programs. Investments should target the discovery of new quantum applications, new approaches to quantum-component manufacturing, and advances in quantumâenabling technologies, such as photonics, nanofabrication, and cryogenic and semiconductor systems.

(c) The United States must seek to foster the next generation of scientists and engineers with quantum-relevant skill sets, including those relevant to quantum-resistant cryptography. Education in QIS and related cybersecurity principles should be incorporated into academic curricula at all levels of schooling to support the growth of a diverse domestic workforce. Furthermore, it is vital that we attract and retain talent and encourage career opportunities that keep quantum experts employed domestically.

(d) To promote the development of quantum technology and the effective deployment of quantum-resistant cryptography, the United States must establish partnerships with industry; academia; and State, local, Tribal, and territorial (SLTT) governments. These partnerships should advance joint R&D initiatives and streamline mechanisms for technology transfer between industry and government.

(e) The United States must promote professional and academic collaborations with overseas allies and partners. This international engagement is essential for identifying and following global QIS trends and for harmonizing quantum security and protection programs.

(f) In support of these goals, within 90 days of the date of this memorandum, agencies that fund research in, develop, or acquire quantum computers shall coordinate with the Director of the Office of Science and Technology Policy to ensure a coherent national strategy for QIS promotion and technology protection, including for workforce issues. To facilitate this coordination, all such agencies shall identify a liaison to the National Quantum Coordination Office to share information and best practices, consistent with section 102(b)(3) of the National Quantum Initiative Act (Public Law 115-368) and section 6606 of the National Defense Authorization Act for Fiscal Year 2022 (Public Law 117-81). All coordination efforts shall be undertaken with appropriate protections for sensitive and classified information and intelligence sources and methods.

Sec. 3. Mitigating the Risks to Encryption. (a) Any digital system that uses existing public standards for publicâkey cryptography, or that is planning to transition to such cryptography, could be vulnerable to an attack by a CRQC. To mitigate this risk, the United States must prioritize the timely and equitable transition of cryptographic systems to quantum-resistant cryptography, with the goal of mitigating as much of the quantum risk as is feasible by 2035. Currently, the Director of the National Institute of Standards and Technology (NIST) and the Director of the National Security Agency (NSA), in their capacity as the National Manager for National Security Systems (National Manager), are each developing technical standards for quantumâresistant cryptography for their respective jurisdictions. The first sets of these standards are expected to be released publicly by 2024.

(b) Central to this migration effort will be an emphasis on cryptographic agility, both to reduce the time required to transition and to allow for seamless updates for future cryptographic standards. This effort is an imperative across all sectors of the United States economy, from government to critical infrastructure, commercial services to cloud providers, and everywhere else that vulnerable public-key cryptography is used.

(c) Consistent with these goals:

(i) Within 90 days of the date of this memorandum, the Secretary of Commerce, through the Director of NIST, shall initiate an open working group with industry, including critical infrastructure owners and operators, and other stakeholders, as determined by the Director of NIST, to further advance adoption of quantum-resistant cryptography. This working group shall identify needed tools and data sets, and other considerations to inform the development by NIST of guidance and best practices to assist with quantumâresistant cryptography planning and prioritization. Findings of this working group shall be provided, on an ongoing basis, to the Director of the Office of Management and Budget (OMB), the Assistant to the President for National Security Affairs (APNSA), and the National Cyber Director to incorporate into planning efforts.

(ii) Within 90 days of the date of this memorandum, the Secretary of Commerce, through the Director of NIST, shall establish a “Migration to Post-Quantum Cryptography Project” at the National Cybersecurity Center of Excellence to work with the private sector to address cybersecurity challenges posed by the transition to quantum-resistant cryptography. This project shall develop programs for discovery and remediation of any system that does not use quantum-resistant cryptography or that remains dependent on vulnerable systems.

(iii) Within 180 days of the date of this memorandum, and annually thereafter, the Secretary of Homeland Security, through the Director of the Cybersecurity and Infrastructure Security Agency (CISA), and in coordination with Sector Risk Management Agencies, shall engage with critical infrastructure and SLTT partners regarding the risks posed by quantum computers, and shall provide an annual report to the Director of OMB, the APNSA, and the National Cyber Director that includes recommendations for accelerating those entities’ migration to quantum-resistant cryptography.

(iv) Within 180 days of the date of this memorandum, and on an ongoing basis, the Director of OMB, in consultation with the Director of CISA, the Director of NIST, the National Cyber Director, and the Director of NSA, shall establish requirements for inventorying all currently deployed cryptographic systems, excluding National Security Systems (NSS). These requirements shall include a list of key information technology (IT) assets to prioritize, interim benchmarks, and a common (and preferably automated) assessment process for evaluating progress on quantum-resistant cryptographic migration in IT systems.

(v) Within 1 year of the date of this memorandum, and on an annual basis thereafter, the heads of all Federal Civilian Executive Branch (FCEB) Agencies shall deliver to the Director of CISA and the National Cyber Director an inventory of their IT systems that remain vulnerable to CRQCs, with a particular focus on High Value Assets and High Impact Systems. Inventories should include current cryptographic methods used on IT systems, including system administrator protocols, non-security software and firmware that require upgraded digital signatures, and information on other key assets.

(vi) By October 18, 2023, and on an annual basis thereafter, the National Cyber Director shall, based on the inventories described in subsection 3(c)(v) of this memorandum and in coordination with the Director of CISA and the Director of NIST, deliver a status report to the APNSA and the Director of OMB on progress made by FCEB Agencies on their migration of non-NSS IT systems to quantum-resistant cryptography. This status report shall include an assessment of the funding necessary to secure vulnerable IT systems from the threat posed by adversarial access to quantum computers, a description and analysis of ongoing coordination efforts, and a strategy and timeline for meeting proposed milestones.

(vii) Within 90 days of the release of the first set of NIST standards for quantum-resistant cryptography referenced in subsection 3(a) of this memorandum, and on an annual basis thereafter, as needed, the Secretary of Commerce, through the Director of NIST, shall release a proposed timeline for the deprecation of quantum-vulnerable cryptography in standards, with the goal of moving the maximum number of systems off quantum-vulnerable cryptography within a decade of the publication of the initial set of standards. The Director of NIST shall work with the appropriate technical standards bodies to encourage interoperability of commercial cryptographic approaches.

(viii) Within 1 year of the release of the first set of NIST standards for quantum-resistant cryptography referenced in subsection 3(a) of this memorandum, the Director of OMB, in coordination with the Director of CISA and the Director of NIST, shall issue a policy memorandum requiring FCEB Agencies to develop a plan to upgrade their non-NSS IT systems to quantum-resistant cryptography. These plans shall be expeditiously developed and be designed to address the most significant risks first. The Director of OMB shall work with the head of each FCEB Agency to estimate the costs to upgrade vulnerable systems beyond already planned expenditures, ensure that each plan is coordinated and shared among relevant agencies to assess interoperability between solutions, and coordinate with the National Cyber Director to ensure plans are updated accordingly.

(ix) Until the release of the first set of NIST standards for quantum-resistant cryptography referenced in subsection 3(a) of this memorandum, the heads of FCEB Agencies shall not procure any commercial quantum-resistant cryptographic solutions for use in IT systems supporting enterprise and mission operations. However, to assist with anticipating potential compatibility issues, the heads of such FCEB Agencies should conduct tests of commercial solutions that have implemented pre-standardized quantum-resistant cryptographic algorithms. These tests will help identify interoperability or performance issues that may occur in Federal environments at an early stage and will contribute to the mitigation of those issues. The heads of such FCEB Agencies should continue to implement and, where needed, upgrade existing cryptographic implementations, but should transition to quantum-resistant cryptography only once the first set of NIST standards for quantum-resistant cryptography is complete and implemented in commercial products. Conformance with international standards should be encouraged, and may be required for interoperability.

(x) Within 1 year of the date of this memorandum, and annually thereafter, the Director of NSA, serving in its capacity as the National Manager, in consultation with the Secretary of Defense and the Director of National Intelligence, shall provide guidance on quantum-resistant cryptography migration, implementation, and oversight for NSS. This guidance shall be consistent with National Security Memorandum/NSM-8 (Improving the Cybersecurity of National Security, Department of Defense, and Intelligence Community Systems). The National Manager shall share best practices and lessons learned with the Director of OMB and the National Cyber Director, as appropriate.

(xi) Within 1 year of the date of this memorandum, and on an ongoing basis, and consistent with section 1 of NSM-8, the heads of agencies operating NSS shall identify and document all instances where quantum-vulnerable cryptography is used by NSS and shall provide this information to the National Manager.

(xii) Within 180 days of issuance by the National Manager of its standards on quantum-resistant cryptography referenced in section 3(a) of this memorandum, and annually thereafter, the National Manager shall release an official timeline for the deprecation of vulnerable cryptography in NSS, until the migration to quantum-resistant cryptography is completed.

(xiii) Within 1 year of issuance by the National Manager of its standards on quantum-resistant cryptography for referenced in subsection 3(a) of this memorandum, and annually thereafter, the heads of agencies operating or maintaining NSS shall submit to the National Manager, and, as appropriate, the Department of Defense Chief Information Officer or the Intelligence Community Chief Information Officer, depending on their respective jurisdictions, an initial plan to transition to quantumâresistant cryptography in all NSS. These plans shall be updated annually and shall include relevant milestones, schedules, authorities, impediments, funding requirements, and exceptions authorized by the head of the agency in accordance with section 3 of NSM-8 and guidance from the National Manager.

(xiv) By December 31, 2023, agencies maintaining NSS shall implement symmetric-key protections (e.g., High Assurance Internet Protocol Encryptor (HAIPE) exclusion keys or VPN symmetric key solutions) to provide additional protection for quantum-vulnerable key exchanges, where appropriate and in consultation with the National Manager. Implementation should seek to avoid interference with interoperability or other cryptographic modernization efforts.

(xv) By December 31, 2023, the Secretary of Defense shall deliver to the APNSA and the Director of OMB an assessment of the risks of quantum computing to the defense industrial base and to defense supply chains, along with a plan to engage with key commercial entities to upgrade their IT systems to achieve quantum resistance.

Sec. 4. Protecting United States Technology. (a) In addition to promoting quantum leadership and mitigating the risks of CRQCs, the United States Government must work to safeguard relevant quantum R&D and intellectual property (IP) and to protect relevant enabling technologies and materials. Protection mechanisms will vary, but may include counterintelligence measures, well-targeted export controls, and campaigns to educate industry and academia on the threat of cybercrime and IP theft.

(b) All agencies responsible for either promoting or protecting QIS and related technologies should understand the security implications of adversarial use and consider those security implications when implementing new policies, programs, and projects.

(c) The United States should ensure the protection of U.S.âdeveloped quantum technologies from theft by our adversaries. This will require campaigns to educate industry, academia, and SLTT partners on the threat of IP theft and on the importance of strong compliance, insider threat detection, and cybersecurity programs for quantum technologies. As appropriate, Federal law enforcement agencies and other relevant agencies should investigate and prosecute actors who engage in the theft of quantum trade secrets or who violate United States export control laws. To support efforts to safeguard sensitive information, Federal law enforcement agencies should exchange relevant threat information with agencies responsible for developing and promoting quantum technologies.

(d) Consistent with these goals, by December 31, 2022, the heads of agencies that fund research in, develop, or acquire quantum computers or related QIS technologies shall develop comprehensive technology protection plans to safeguard QIS R&D, acquisition, and user access. Plans shall be coordinated across agencies, including with Federal law enforcement, to safeguard quantum computing R&D and IP, acquisition, and user access. These plans shall be updated annually and provided to the APNSA, the Director of OMB, and the Co-Chairs of the National Science and Technology Council Subcommittee on Economic and Security Implications of Quantum Science.

Sec. 5. Definitions. For purposes of this memorandum:

(a) the term “agency” has the meaning ascribed to it under 44 U.S.C. 3502;

(b) the term “critical infrastructure” means systems and assets, whether physical or virtual, so vital to the United States that their incapacitation or destruction would have a debilitating effect on the Nation’s security, economy, public health and safety, or any combination thereof;

(c) the term “cryptographic agility” means a design feature that enables future updates to cryptographic algorithms and standards without the need to modify or replace the surrounding infrastructure;

(d) the term “cryptanalytically relevant quantum computer” or “CRQC” means a quantum computer capable of undermining current public-key cryptographic algorithms;

(e) the term “Federal Civilian Executive Branch Agency” or “FCEB Agency” means any agency except the Department of Defense or agencies in the Intelligence Community;

(f) the term “high value asset” means information or an information system that is so critical to an organization that the loss or corruption of this information, or loss of access to the system, would have serious impacts on the organization’s ability to perform its mission or conduct business;

(g) the term “high impact system” means an information system in which at least one security objective (i.e., confidentiality, integrity, or availability) is assigned a Federal Information Processing Standards (FIPS) 199 potential impact value of “high”;

(h) the term “information technology” or “IT” has the meaning ascribed to it under 44 U.S.C. 3502;

(i) the term “National Security Systems” or “NSS” has the meaning ascribed to it in 44 U.S.C 3552(b)(6) and shall also include other Department of Defense and Intelligence Community systems, as described in 44 U.S.C. 3553(e)(2) and 44 U.S.C. 3553(e)(3);

(j) the term “quantum computer” means a computer utilizing the collective properties of quantum states, such as superposition, interference and entanglement, to perform calculations. The foundations in quantum physics give a quantum computer the ability to solve a subset of hard mathematical problems at a much faster rate than a classical (i.e., nonâquantum) computer;

(k) the term “quantum information sciences” or “QIS” has the meaning ascribed to it under 15 U.S.C. 8801(6) and means the study and application of the laws of quantum physics for the storage, transmission, manipulation, computing, or measurement of information; and

(l) the term “quantum-resistant cryptography” means those cryptographic algorithms or methods that are assessed not to be specifically vulnerable to attack by either a CRQC or classical computer. This is also referred to as post-quantum cryptography.

Sec. 6. General Provisions. (a) Nothing in this memorandum shall be construed to impair or otherwise affect:

(i) the authority granted by law to an executive department or agency, or the head thereof, to include the protection of intelligence sources and methods; or

(ii) the functions of the Director of OMB relating to budgetary, administrative, or legislative proposals.

(b) This memorandum shall be implemented consistent with applicable law and subject to the availability of appropriations.

(c) This memorandum shall also be implemented without impeding the conduct or support of intelligence activities, and all implementation measures shall be designed to be consistent with appropriate protections for sensitive information and intelligence sources and methods.

(d) This memorandum is not intended to, and does not, create any right or benefit, substantive or procedural, enforceable at law or in equity by any party against the United States, its departments, agencies, or entities, its officers, employees, or agents, or any other person.

JOSEPH R. BIDEN JR.",0.00,2025-06-14 00:24:59.633632,2025-06-14 03:11:28.682084,Document,https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2022/05/04/national-security-memorandum-on-promoting-united-states-leadership-in-quantum-computing-while-mitigating-risks-to-vulnerable-cryptographic-systems/,"{""author"": ""The White House"", ""verified"": true, ""source_url"": ""https://bidenwhitehouse.archives.gov/briefing-room/statements-releases/2022/05/04/national-security-memorandum-on-promoting-united-states-leadership-in-quantum-computing-while-mitigating-risks-to-vulnerable-cryptographic-systems/"", ""document_id"": ""2a47b6bb-581b-4d15-b143-a6092a22c9bb"", ""extraction_method"": ""URL_VERIFIED"", ""verification_timestamp"": ""2025-06-14T00:24:59.528873""}",White House,2022-05-04,This document addresses quantum and risk by outlines my administration’s policies and initiatives related to quantum computi. It provides national guidance for National Institute of Standards and Technology and related organizations.,,85,,80,Unknown,0.0,,Quantum,True,valid,2025-06-14 03:57:35.618983,
49,Quantum Ethics: There’s No Time Like the Present to Plan for the Human Future with Quantum Technology - The Quantum Record,"By James Myers and Mariana Meneses

#### Human history provides many examples of how we underestimate the rapid development and future uses of our technological innovations.

A prime case of this is the internet which, since it became widely available only 20 years ago, has transformed our world in ways that no one predicted at the outset.

In quantum computing, a powerful new technology is now under development. The machine’s speed and accuracy are already proving, in some limited instances, more powerful than the best supercomputers now in use, although it’s widely thought that the scientific and financial challenges the quantum computer faces might not be resolved for another decade.

**All powerful technologies come with ethical issues. Can we afford to defer the ethical discussions around quantum computing? When the machine’s power is unleashed, will we have enough time to address its human consequences?**

The ways that the speed and power of the quantum computer might affect the world are, for the present, unknowable, because we can’t predict the timing and consequences of our own ingenuity. But what if, like the internet, quantum computing becomes viable and widespread much more quickly than many think? Who will have access to it, what applications will it be used for, what are its potential benefits and what harms might it cause, and what kinds of regulations and controls might be necessary?

The internet is a case in point: before its widespread use, no one predicted the rise of social media and the host of serious ethical issues that it causes, including increased rates of suicide and other mental health issues, and its destabilizing effects on democracies – particularly in a year like 2024 when half the world will be casting votes in national and regional elections.

Had early planning for the advent of social media occurred, we might not have found ourselves in the current predicament.



#### For someone born in the last 20 years during the era of the internet, it may be difficult to imagine what the world without the internet was like for those born only a few decades earlier who are now middle-aged.

Fortunately, the internet provides evidence of its own consequences. Above is a brief clip of a 1995 interview of Microsoft founder Bill Gates by David Letterman. At that time, the internet was not widely available to the public and broadband connections were rare. The few public users there were could access the internet only over phone lines using modems that were extremely slow by today’s standards, achieving a maximum speed of 56k bps. Many internet users today have access at speeds sometimes 1,000 times faster.

### How far off-target are we with our predictions now about the future of quantum computing – maybe as much or more than Bill Gates and David Letterman were in 1995 about the internet?

One of the most difficult issues with quantum computing might not be the technical problems it presently faces, but its future ethical use. With so many differing opinions, ethics presents thorny issues that require a great deal of time to resolve, while history demonstrates that human ingenuity is capable of overcoming the mightiest scientific hurdles very quickly. The speed of scientific resolution is especially quick with a spark of genius like that of Albert Einstein, whose Theory of Special Relativity in 1905 triggered a massive transformation that continues to shape our world.


#### The rapidly accelerating rate of investment in quantum computing is an indicator that a breakthrough might be close at hand.

There are now hundreds of thousands of people working around the world, either directly or indirectly, in the heavily male dominated industry. Some of the world’s biggest companies, including IBM, Google, Microsoft, and Amazon, are investing heavily in the technology. This April, McKinsey estimated the industry could be worth $1.3 trillion in little over 10 years, noting that governments have already pledged $34 billion of investments in quantum computing.

In spite of the dramatic increase of investment in the technology, McKinsey reports that its discussions with quantum computing technology executives, academics, and investors reveal 72% believe there won’t be an error-free machine within the next 10 years, and the remaining 28% don’t foresee this happening until after 2040. But what if these individuals are underestimating their combined potential?

### The future often arrives faster than we think.

Are we on the precipice of a transformative moment now, with quantum computing? If a breakthrough were to occur next week that solves the error correction problem which has so far constrained the technology, how would the future unfold from that point? How will the power of quantum computers be used by the people or companies or governments that control them, and how will they deal with the rest of us who either don’t understand the technology or don’t have access to it, if we’re stuck with the sluggish binary computers we now use?

#### The quantum computer could be very powerful indeed, possibly with unlimited potential in its ability to calculate and measure probabilities and combinations with speed and accuracy that far exceeds even the most powerful supercomputers of today.

The power of the quantum computer derives from the physics of the quantum, which is the smallest amount of energy in the universe that can either cause change or be changed. The quantum computer transmits its signals from quantum to quantum and it does so, not one at a time like the “on” and “off” (or 1 and 0) electrical signals in the wiring of today’s computers, but with the phenomenon known as “superposition.”

When one quantum is in superposition with another quantum, it means the two are connected in a way that what happens to one quantum *instantly* happens to its partner.

### Instantly: as in no time at all.

There is literally no way to imagine what no time at all is like, because it’s not like any thing in time. Even the fastest, most sophisticated binary computers of today take time, and however quick we think that time might be, it’s still time. Time is irrelevant, however, for two quanta that are in superposition, and disposing with time provides the machine with an incredible advantage over today’s time-bound computers.



#### The computers and laptops and tablets and “smart”phones that we rely on today could seem like antique museum pieces, at least to those few who might control the first truly effective quantum computers.

When that time will come is anyone’s guess, but what if it’s not a decade but a year away, or a month, or even a day? Is it worth the gamble how the future will turn out, and what does history say about past gambles on how time unfolds? Even if the irresponsible use of power eventually undermines and defeats itself, history shows that much damage can be caused in the process.

### The Quantum Record examines the philosophy of quantum technology and quantum ethics.

The philosophy of technology is crucially important to understand at pivotal moments in time like now, in 2024, and like 30 years earlier when the internet was in its infancy. The “why” question is key: why are we investing time and money in this, why do we want to develop it, why and who will it benefit, and many other “why’s,” – which is inevitably the question after the scientific “how” is resolved. It might require only one further discovery to perfect the logic that resolves the problem of quantum error correction, and that could happen tomorrow or ten years from now – it’s unknowable, and one that’s particularly big and risky in the context of ethics.

Others are beginning to think ahead to the ethical consequences of quantum computing, and by introducing this new series on quantum ethics The Quantum Record wants to spread news of their good and important work, and engage as many as we possibly can in a crucially important discussion about the type of technological future that we want to shape. We hold the power of the quantum, that tiniest cause or effect of change, practically in our hands and we can’t afford to fumble it.


#### We’ll set out here some of the broad categories that will be examined in greater depth in forthcoming articles.

Our goals are to broaden and strengthen the conversation on the questions of ethics that brings together the scientists working so hard on the “how” questions of quantum technology with its future users, for whom the “why” questions will be paramount.

The bridging can only be done with a common language, which is especially tricky with quantum computing because of its incredible complexities in mathematics, physics, geometry, engineering, and computer science. With the unresolved question of the quantum observer effect, there remain deep, unsolved mysteries about the nature of the quantum itself and its connection with us, the conscious observers. Our aim is to build on the work of those pioneers already beginning to consider the ethical implications of the technology, and to add our voices to their ranks.

Here, in no particular order and with a brief synopsis, are some of the principal areas we’ll focus on in coming articles, and we’ll be covering other issues as they develop.

### Quantum encryption crisis

Private and sensitive data, like e-mails, bank accounts, credit cards, and trade secrets, are now most commonly encrypted using prime number factoring which is extremely difficult for today’s computers to crack. Quantum computers can, however, factor prime numbers far more quickly, a realization that has triggered concern among many about a looming quantum cryptography crisis in which already-encrypted data can easily be decrypted in the future. It’s thought that unethical actors are already storing data they can’t currently access, with the expectation that they will be able to break the code in the future with the help of quantum computers.

Prime number factoring is a mathematical process that uses the fundamental theorem of arithmetic to break a number into the product of two or more prime numbers. A prime number is divisible only by one and itself, and every number greater than one can be derived by multiplying two or more prime numbers. Although a number like 1200 is relatively quickly broken down into its prime factors 2, 3, and 5 (as in 1200 = 24 ∙ 31 ∙ 52), finding the prime factors of large numbers is a time-consuming task with today’s computers. Operating in a quantum computer, Shor’s algorithm uses polynomial time – the vastly decreased time scale that applies to quantum signals in superposition – to factor the prime basis of numbers far more quickly.

Unless a method to factor prime numbers in polynomial time can be developed for classical computers, existing cryptographic methods will be rendered obsolete in the post-quantum era.

### Fair and equitable access to the technology


As we noted, some of the world’s largest companies like IBM, Google, Microsoft, and Amazon are investing heavily in quantum technology. If one of them, or another company, government, or person were to perfect the technology, would they share it with the rest of the world? If so, at what cost, and would it be subject to regulation?

As a result of the quantum computer’s potential speed and accuracy, the “quantum advantage” (a term that was formerly called “quantum supremacy”) could be incredibly significant for the first developers. Will there be inequities in the way such an advantage is used? Will large profits be extracted from users?

History shows that companies like Google, which has a near-monopoly on internet search and effectively controls the world’s index of data, can make record profits from their dominant positions – and the law of accounting is that every dollar of profit that someone makes comes at the expense of someone else.

Even with potentially widespread access to the powerful technology, there remains the issue of its complexity and the extent of training required to enable its use. Quantum computing is heavily reliant on knowledge of physics, mathematics, geometry, engineering and computing science, and we could ask now whether these skills will be equally widespread when the technology becomes available. If only a select few understand how it works, the few could become very powerful.

### Military use of the technology

As The Quantum Record has previously noted, the U.S. and other military forces are already investing heavily in technology and educating students in STEM fields.

Military automation is now a driving force to which quantum computing could add significantly. Take, for instance, the U.S. military’s new “Replicator Initiative,” which is funding the development of autonomous systems and is intended, as U.S. Deputy Defense Secretary Kathleen Hicks stated, to counter China’s “biggest advantage, which is mass. More ships. More missiles. More people.” Noting that, “This is about mastering the technology of tomorrow,” Hicks stated, “After all, we don’t use our people as cannon fodder like some competitors do.”

It is difficult to know to what extent powerful militaries, like those of the U.S., China, and Russia, are investing in quantum technologies, since current activities and future plans aren’t publicized for strategic reasons. The technological weapons in their arsenals are, however, now so fearsome that the question is one of concern to the safety of every human on the planet and is in the forefront of ethical issues for quantum computing.



### The technology’s power to create visual simulations

As the saying goes, “Seeing is believing,” and when we see something almost perfectly life-like we often don’t stop – or don’t have the time to stop – to consider whether it’s real or fake. “Deepfake” images of people doing or saying things that never occurred are already a plague on the internet, placed there by malicious actors who either benefit from distorting our perception of reality or taking revenge against others.

Holographic images produced by a quantum computer could be powerfully life-like, because the computer can handle combinations and permutations of data pixels far more quickly than any present machine. When that happens, how will we know whether to believe the images we see on our screens or to question their reality? Will we know who put the images there, why they did it, and for whose benefit?



**Today’s “classical” binary computers can produce already very convincing holographic images**, as in this revival of the legendary 1970s pop rock supergroup ABBA that fills stadiums with fans who feel that they are experiencing the real event even though many of them were born long after ABBA disbanded in 1982. The quantum computer could take such experiences up many times closer to reality.

### The technology’s power to manipulate genetics

DNA, which is the basis of our genetic information, is an incredibly complex structure that operates as an instruction set for the folding of proteins in our bodies. Protein-folding is not yet fully understood and challenges today’s fastest supercomputers because it involves an incredible number of combinations and permutations requiring vast computing power.

In 2022, Google’s AlphaFold technology predicted the structures of over 200 million proteins, a 2000% increase in the number it had predicted only a year earlier. Significant advances in CRISPR technology, which was the subject of the 2020 Nobel Prize in Chemistry, are making it easier to manipulate DNA by removing, adding, or changing its sequences. While there will likely be significant medical benefits from the technology, for example in combatting diseases like cancer that override the proper sequencing of cellular replication, there can be abuses. The power of the quantum computer could amplify the potential for abuse.



**We don’t have to look far for examples of genetic abuse with existing technology.**

In 2018, researchers in China went to prison for a widely-condemned and failed CRISPR experiment in which they attempted to create human babies immune to HIV. While regulations exist to prevent human experimentation like this, we can see they are not always effective and the power of quantum computing technology might outrun the regulators.

Particular care is required to prevent experiments in eugenics, a discredited practice of more than a century ago based on the false notion that the human genetic pool could be “improved” by sterilizing people with genetic traits deemed undesirable. Genocide, such as Hitler’s extermination of the lives of six million Jews as recently as 79 years ago, is the next step on the path of eugenics.

### The technology’s power to forecast and influence human behaviour

Forecasting the future involves probabilities. With 7.8 billion humans who can choose either action or inaction at any moment in the present, there are exponentially that number of future probabilities in our behaviour that defy any supercomputer.

There is potentially great profit to be had from correct predictions of human behaviour. For example, finance firm Goldman Sachs is developing quantum computing algorithms that would give it incredible speed and accuracy in predicting the trading values of derivatives. Bought and sold in financial markets, derivatives are contracts on the expected future value of an underlying asset such as stocks or commodities. The trading prices of derivatives are established based on perceptions of the relative present risks and variability of the asset in relation to other market alternatives. The accuracy of derivatives pricing requires mathematical analysis of statistical probabilities over time in a process known as “monte carlo simulation,” a task for which the speed of the quantum computer will provide a significant financial advantage.

If only one company, or a handful of them, is able to outrun other traders on financial markets, what will happen to the majority who lack the technology, and where will the profits go?


##### Watched globally over 100 million times, The Social Dilemma received two Emmy Awards and raised awareness of the ways that human behaviour can be manipulated over the internet.


And what about other types of human behaviour, particularly in politics where actions have long-lasting consequences? In a January 2024 Phys.org article, Dorje C. Brody asks, “Could quantum physics be the key that unlocks the secrets of human behavior?” In a February 2021 article, Scott Fulton III asks, “Could quantum computers fix political polls?” Quantum computing could deliver a new understanding of cognitive and emotional biases. As argued by researchers Thomas Holtfort and Andreas Horsh, quantum social science, a new research area, is challenging the basic assumption that social life, decision-making behaviour, and consciousness are classical physical phenomena.

**If this is true that certain aspects of human irrationality can be explained by mathematical models borrowed from quantum physics, this could potentially allow for the exploitation of these biases.**

Famously, Daniel Kahneman, Amos Tversky, and others observed that, in behavioral economics, systematic errors in judgment and decision-making arise from the use of mental shortcuts or heuristics, such as confirmation bias, where people tend to seek information that confirms their existing beliefs. (A brief overview of the idea is in this pdf from Rutgers University).

As shown in a 2022 paper by Alvaro Huerga, from the University of Deusto, in Spain, and co-authors, one of the main challenges in human behaviour modelling is predicting users’ next actions. Algorithms like Quantum Kernel Alignment and Quantum Support Vector Machines propose new methods for anticipating human actions and decision-making processes. By harnessing the principles of quantum mechanics, researchers aim to overcome the limitations of classical computing in understanding the complexities of human behaviour, potentially offering insights into voting patterns and political affiliations across diverse populations.

### The future is now.

Obviously, the application of quantum computing in predicting and potentially manipulating political behaviour raises significant ethical concerns. The ability to simulate vast datasets and model intricate social dynamics, in a fraction of the time it would take classical supercomputers to accomplish, presents the risk of exploiting cognitive biases and vulnerabilities in individuals.

Moreover, the accuracy and scale of such predictions could empower entities to influence elections and public opinion in countries and regions around the globe.



#### The advent of quantum social science poses fundamental questions about the ethical boundaries of manipulating human behaviour and the potential for democratic processes to be undermined by technological advancements.

There are also, as we have outlined, a host of other issues which could ultimately and severely affect human behaviour.

History demonstrates that we often underestimate human ingenuity, and the potential power of the quantum computer is not something that we should risk underestimating. Many will seek to use it for their advantage at the expense of others, from a profit motive or many other motivations driven by a desire for personal benefit.

There’s no time like the present to plan for the human future in the era of quantum computing. As we have witnessed with the European Union’s recently-enacted A.I. Act, the ethical discussions will require time and effort, but there is a great deal at stake with the quantum computer.

Future generations will be grateful for our attention to the ethical issues, and it’s a duty we owe to them as well as to ourselves as we stand on the precipice of the power of quantum technology.","By James Myers and Mariana Meneses

#### Human history provides many examples of how we underestimate the rapid development and future uses of our technological innovations.

A prime case of this is the internet which, since it became widely available only 20 years ago, has transformed our world in ways that no one predicted at the outset.

In quantum computing, a powerful new technology is now under development. The machine’s speed and accuracy are already proving, in some limited instances, more powerful than the best supercomputers now in use, although it’s widely thought that the scientific and financial challenges the quantum computer faces might not be resolved for another decade.

**All powerful technologies come with ethical issues. Can we afford to defer the ethical discussions around quantum computing? When the machine’s power is unleashed, will we have enough time to address its human consequences?**

The ways that the speed and power of the quantum computer might affect the world are, for the present, unknowable, because we can’t predict the timing and consequences of our own ingenuity. But what if, like the internet, quantum computing becomes viable and widespread much more quickly than many think? Who will have access to it, what applications will it be used for, what are its potential benefits and what harms might it cause, and what kinds of regulations and controls might be necessary?

The internet is a case in point: before its widespread use, no one predicted the rise of social media and the host of serious ethical issues that it causes, including increased rates of suicide and other mental health issues, and its destabilizing effects on democracies – particularly in a year like 2024 when half the world will be casting votes in national and regional elections.

Had early planning for the advent of social media occurred, we might not have found ourselves in the current predicament.



#### For someone born in the last 20 years during the era of the internet, it may be difficult to imagine what the world without the internet was like for those born only a few decades earlier who are now middle-aged.

Fortunately, the internet provides evidence of its own consequences. Above is a brief clip of a 1995 interview of Microsoft founder Bill Gates by David Letterman. At that time, the internet was not widely available to the public and broadband connections were rare. The few public users there were could access the internet only over phone lines using modems that were extremely slow by today’s standards, achieving a maximum speed of 56k bps. Many internet users today have access at speeds sometimes 1,000 times faster.

### How far off-target are we with our predictions now about the future of quantum computing – maybe as much or more than Bill Gates and David Letterman were in 1995 about the internet?

One of the most difficult issues with quantum computing might not be the technical problems it presently faces, but its future ethical use. With so many differing opinions, ethics presents thorny issues that require a great deal of time to resolve, while history demonstrates that human ingenuity is capable of overcoming the mightiest scientific hurdles very quickly. The speed of scientific resolution is especially quick with a spark of genius like that of Albert Einstein, whose Theory of Special Relativity in 1905 triggered a massive transformation that continues to shape our world.


#### The rapidly accelerating rate of investment in quantum computing is an indicator that a breakthrough might be close at hand.

There are now hundreds of thousands of people working around the world, either directly or indirectly, in the heavily male dominated industry. Some of the world’s biggest companies, including IBM, Google, Microsoft, and Amazon, are investing heavily in the technology. This April, McKinsey estimated the industry could be worth $1.3 trillion in little over 10 years, noting that governments have already pledged $34 billion of investments in quantum computing.

In spite of the dramatic increase of investment in the technology, McKinsey reports that its discussions with quantum computing technology executives, academics, and investors reveal 72% believe there won’t be an error-free machine within the next 10 years, and the remaining 28% don’t foresee this happening until after 2040. But what if these individuals are underestimating their combined potential?

### The future often arrives faster than we think.

Are we on the precipice of a transformative moment now, with quantum computing? If a breakthrough were to occur next week that solves the error correction problem which has so far constrained the technology, how would the future unfold from that point? How will the power of quantum computers be used by the people or companies or governments that control them, and how will they deal with the rest of us who either don’t understand the technology or don’t have access to it, if we’re stuck with the sluggish binary computers we now use?

#### The quantum computer could be very powerful indeed, possibly with unlimited potential in its ability to calculate and measure probabilities and combinations with speed and accuracy that far exceeds even the most powerful supercomputers of today.

The power of the quantum computer derives from the physics of the quantum, which is the smallest amount of energy in the universe that can either cause change or be changed. The quantum computer transmits its signals from quantum to quantum and it does so, not one at a time like the “on” and “off” (or 1 and 0) electrical signals in the wiring of today’s computers, but with the phenomenon known as “superposition.”

When one quantum is in superposition with another quantum, it means the two are connected in a way that what happens to one quantum *instantly* happens to its partner.

### Instantly: as in no time at all.

There is literally no way to imagine what no time at all is like, because it’s not like any thing in time. Even the fastest, most sophisticated binary computers of today take time, and however quick we think that time might be, it’s still time. Time is irrelevant, however, for two quanta that are in superposition, and disposing with time provides the machine with an incredible advantage over today’s time-bound computers.



#### The computers and laptops and tablets and “smart”phones that we rely on today could seem like antique museum pieces, at least to those few who might control the first truly effective quantum computers.

When that time will come is anyone’s guess, but what if it’s not a decade but a year away, or a month, or even a day? Is it worth the gamble how the future will turn out, and what does history say about past gambles on how time unfolds? Even if the irresponsible use of power eventually undermines and defeats itself, history shows that much damage can be caused in the process.

### The Quantum Record examines the philosophy of quantum technology and quantum ethics.

The philosophy of technology is crucially important to understand at pivotal moments in time like now, in 2024, and like 30 years earlier when the internet was in its infancy. The “why” question is key: why are we investing time and money in this, why do we want to develop it, why and who will it benefit, and many other “why’s,” – which is inevitably the question after the scientific “how” is resolved. It might require only one further discovery to perfect the logic that resolves the problem of quantum error correction, and that could happen tomorrow or ten years from now – it’s unknowable, and one that’s particularly big and risky in the context of ethics.

Others are beginning to think ahead to the ethical consequences of quantum computing, and by introducing this new series on quantum ethics The Quantum Record wants to spread news of their good and important work, and engage as many as we possibly can in a crucially important discussion about the type of technological future that we want to shape. We hold the power of the quantum, that tiniest cause or effect of change, practically in our hands and we can’t afford to fumble it.


#### We’ll set out here some of the broad categories that will be examined in greater depth in forthcoming articles.

Our goals are to broaden and strengthen the conversation on the questions of ethics that brings together the scientists working so hard on the “how” questions of quantum technology with its future users, for whom the “why” questions will be paramount.

The bridging can only be done with a common language, which is especially tricky with quantum computing because of its incredible complexities in mathematics, physics, geometry, engineering, and computer science. With the unresolved question of the quantum observer effect, there remain deep, unsolved mysteries about the nature of the quantum itself and its connection with us, the conscious observers. Our aim is to build on the work of those pioneers already beginning to consider the ethical implications of the technology, and to add our voices to their ranks.

Here, in no particular order and with a brief synopsis, are some of the principal areas we’ll focus on in coming articles, and we’ll be covering other issues as they develop.

### Quantum encryption crisis

Private and sensitive data, like e-mails, bank accounts, credit cards, and trade secrets, are now most commonly encrypted using prime number factoring which is extremely difficult for today’s computers to crack. Quantum computers can, however, factor prime numbers far more quickly, a realization that has triggered concern among many about a looming quantum cryptography crisis in which already-encrypted data can easily be decrypted in the future. It’s thought that unethical actors are already storing data they can’t currently access, with the expectation that they will be able to break the code in the future with the help of quantum computers.

Prime number factoring is a mathematical process that uses the fundamental theorem of arithmetic to break a number into the product of two or more prime numbers. A prime number is divisible only by one and itself, and every number greater than one can be derived by multiplying two or more prime numbers. Although a number like 1200 is relatively quickly broken down into its prime factors 2, 3, and 5 (as in 1200 = 24 ∙ 31 ∙ 52), finding the prime factors of large numbers is a time-consuming task with today’s computers. Operating in a quantum computer, Shor’s algorithm uses polynomial time – the vastly decreased time scale that applies to quantum signals in superposition – to factor the prime basis of numbers far more quickly.

Unless a method to factor prime numbers in polynomial time can be developed for classical computers, existing cryptographic methods will be rendered obsolete in the post-quantum era.

### Fair and equitable access to the technology


As we noted, some of the world’s largest companies like IBM, Google, Microsoft, and Amazon are investing heavily in quantum technology. If one of them, or another company, government, or person were to perfect the technology, would they share it with the rest of the world? If so, at what cost, and would it be subject to regulation?

As a result of the quantum computer’s potential speed and accuracy, the “quantum advantage” (a term that was formerly called “quantum supremacy”) could be incredibly significant for the first developers. Will there be inequities in the way such an advantage is used? Will large profits be extracted from users?

History shows that companies like Google, which has a near-monopoly on internet search and effectively controls the world’s index of data, can make record profits from their dominant positions – and the law of accounting is that every dollar of profit that someone makes comes at the expense of someone else.

Even with potentially widespread access to the powerful technology, there remains the issue of its complexity and the extent of training required to enable its use. Quantum computing is heavily reliant on knowledge of physics, mathematics, geometry, engineering and computing science, and we could ask now whether these skills will be equally widespread when the technology becomes available. If only a select few understand how it works, the few could become very powerful.

### Military use of the technology

As The Quantum Record has previously noted, the U.S. and other military forces are already investing heavily in technology and educating students in STEM fields.

Military automation is now a driving force to which quantum computing could add significantly. Take, for instance, the U.S. military’s new “Replicator Initiative,” which is funding the development of autonomous systems and is intended, as U.S. Deputy Defense Secretary Kathleen Hicks stated, to counter China’s “biggest advantage, which is mass. More ships. More missiles. More people.” Noting that, “This is about mastering the technology of tomorrow,” Hicks stated, “After all, we don’t use our people as cannon fodder like some competitors do.”

It is difficult to know to what extent powerful militaries, like those of the U.S., China, and Russia, are investing in quantum technologies, since current activities and future plans aren’t publicized for strategic reasons. The technological weapons in their arsenals are, however, now so fearsome that the question is one of concern to the safety of every human on the planet and is in the forefront of ethical issues for quantum computing.



### The technology’s power to create visual simulations

As the saying goes, “Seeing is believing,” and when we see something almost perfectly life-like we often don’t stop – or don’t have the time to stop – to consider whether it’s real or fake. “Deepfake” images of people doing or saying things that never occurred are already a plague on the internet, placed there by malicious actors who either benefit from distorting our perception of reality or taking revenge against others.

Holographic images produced by a quantum computer could be powerfully life-like, because the computer can handle combinations and permutations of data pixels far more quickly than any present machine. When that happens, how will we know whether to believe the images we see on our screens or to question their reality? Will we know who put the images there, why they did it, and for whose benefit?



**Today’s “classical” binary computers can produce already very convincing holographic images**, as in this revival of the legendary 1970s pop rock supergroup ABBA that fills stadiums with fans who feel that they are experiencing the real event even though many of them were born long after ABBA disbanded in 1982. The quantum computer could take such experiences up many times closer to reality.

### The technology’s power to manipulate genetics

DNA, which is the basis of our genetic information, is an incredibly complex structure that operates as an instruction set for the folding of proteins in our bodies. Protein-folding is not yet fully understood and challenges today’s fastest supercomputers because it involves an incredible number of combinations and permutations requiring vast computing power.

In 2022, Google’s AlphaFold technology predicted the structures of over 200 million proteins, a 2000% increase in the number it had predicted only a year earlier. Significant advances in CRISPR technology, which was the subject of the 2020 Nobel Prize in Chemistry, are making it easier to manipulate DNA by removing, adding, or changing its sequences. While there will likely be significant medical benefits from the technology, for example in combatting diseases like cancer that override the proper sequencing of cellular replication, there can be abuses. The power of the quantum computer could amplify the potential for abuse.



**We don’t have to look far for examples of genetic abuse with existing technology.**

In 2018, researchers in China went to prison for a widely-condemned and failed CRISPR experiment in which they attempted to create human babies immune to HIV. While regulations exist to prevent human experimentation like this, we can see they are not always effective and the power of quantum computing technology might outrun the regulators.

Particular care is required to prevent experiments in eugenics, a discredited practice of more than a century ago based on the false notion that the human genetic pool could be “improved” by sterilizing people with genetic traits deemed undesirable. Genocide, such as Hitler’s extermination of the lives of six million Jews as recently as 79 years ago, is the next step on the path of eugenics.

### The technology’s power to forecast and influence human behaviour

Forecasting the future involves probabilities. With 7.8 billion humans who can choose either action or inaction at any moment in the present, there are exponentially that number of future probabilities in our behaviour that defy any supercomputer.

There is potentially great profit to be had from correct predictions of human behaviour. For example, finance firm Goldman Sachs is developing quantum computing algorithms that would give it incredible speed and accuracy in predicting the trading values of derivatives. Bought and sold in financial markets, derivatives are contracts on the expected future value of an underlying asset such as stocks or commodities. The trading prices of derivatives are established based on perceptions of the relative present risks and variability of the asset in relation to other market alternatives. The accuracy of derivatives pricing requires mathematical analysis of statistical probabilities over time in a process known as “monte carlo simulation,” a task for which the speed of the quantum computer will provide a significant financial advantage.

If only one company, or a handful of them, is able to outrun other traders on financial markets, what will happen to the majority who lack the technology, and where will the profits go?


##### Watched globally over 100 million times, The Social Dilemma received two Emmy Awards and raised awareness of the ways that human behaviour can be manipulated over the internet.


And what about other types of human behaviour, particularly in politics where actions have long-lasting consequences? In a January 2024 Phys.org article, Dorje C. Brody asks, “Could quantum physics be the key that unlocks the secrets of human behavior?” In a February 2021 article, Scott Fulton III asks, “Could quantum computers fix political polls?” Quantum computing could deliver a new understanding of cognitive and emotional biases. As argued by researchers Thomas Holtfort and Andreas Horsh, quantum social science, a new research area, is challenging the basic assumption that social life, decision-making behaviour, and consciousness are classical physical phenomena.

**If this is true that certain aspects of human irrationality can be explained by mathematical models borrowed from quantum physics, this could potentially allow for the exploitation of these biases.**

Famously, Daniel Kahneman, Amos Tversky, and others observed that, in behavioral economics, systematic errors in judgment and decision-making arise from the use of mental shortcuts or heuristics, such as confirmation bias, where people tend to seek information that confirms their existing beliefs. (A brief overview of the idea is in this pdf from Rutgers University).

As shown in a 2022 paper by Alvaro Huerga, from the University of Deusto, in Spain, and co-authors, one of the main challenges in human behaviour modelling is predicting users’ next actions. Algorithms like Quantum Kernel Alignment and Quantum Support Vector Machines propose new methods for anticipating human actions and decision-making processes. By harnessing the principles of quantum mechanics, researchers aim to overcome the limitations of classical computing in understanding the complexities of human behaviour, potentially offering insights into voting patterns and political affiliations across diverse populations.

### The future is now.

Obviously, the application of quantum computing in predicting and potentially manipulating political behaviour raises significant ethical concerns. The ability to simulate vast datasets and model intricate social dynamics, in a fraction of the time it would take classical supercomputers to accomplish, presents the risk of exploiting cognitive biases and vulnerabilities in individuals.

Moreover, the accuracy and scale of such predictions could empower entities to influence elections and public opinion in countries and regions around the globe.



#### The advent of quantum social science poses fundamental questions about the ethical boundaries of manipulating human behaviour and the potential for democratic processes to be undermined by technological advancements.

There are also, as we have outlined, a host of other issues which could ultimately and severely affect human behaviour.

History demonstrates that we often underestimate human ingenuity, and the potential power of the quantum computer is not something that we should risk underestimating. Many will seek to use it for their advantage at the expense of others, from a profit motive or many other motivations driven by a desire for personal benefit.

There’s no time like the present to plan for the human future in the era of quantum computing. As we have witnessed with the European Union’s recently-enacted A.I. Act, the ethical discussions will require time and effort, but there is a great deal at stake with the quantum computer.

Future generations will be grateful for our attention to the ethical issues, and it’s a duty we owe to them as well as to ourselves as we stand on the precipice of the power of quantum technology.",0.00,2025-06-14 05:04:35.165943,2025-06-14 05:04:35.165943,Research,https://thequantumrecord.com/philosophy-of-technology/quantum-ethics-plan-for-human-future/,"{""author"": ""James Myers"", ""verified"": true, ""source_url"": ""https://thequantumrecord.com/philosophy-of-technology/quantum-ethics-plan-for-human-future/"", ""document_id"": ""f6a12f71-4cfe-4231-80bf-cf23f23dddc3"", ""extraction_method"": ""URL_VERIFIED"", ""verification_timestamp"": ""2025-06-14T05:04:34.689246""}",The Quantum Record,2024-05-08,,0,75,0,85,Unknown,0.0,,Quantum,True,verified,2025-06-14 05:16:20.601758,
